(master_env_v178_py36) 21:43  [cms] $ 2to3 . -w -n
RefactoringTool: Skipping optional fixer: buffer
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: set_literal
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: No changes to ./cms/FileConverter.py
RefactoringTool: No changes to ./cms/cms_exceptions.py
RefactoringTool: Refactored ./cms/cms_modeller.py
--- ./cms/cms_modeller.py	(original)
+++ ./cms/cms_modeller.py	(refactored)
@@ -107,7 +107,7 @@
 	inputtpedstring = ''.join(args.inputTpeds)
 	inputtpeds = inputtpedstring.split(',')
 	npops = len(inputtpeds)
-	print("calculating summary statistics for " +  str(npops) + " populations...")
+	print(("calculating summary statistics for " +  str(npops) + " populations..."))
 	allCmds = []
 	for ipop in range(npops):
 		inputtped = inputtpeds[ipop]
@@ -135,7 +135,7 @@
 def execute_bootstrap(args):
 	'''pulls all per-snp/per-snp-pair values to get genome-wide bootstrap estimates.'''
 	nbootstraprep = args.nBootstrapReps
-	print("running " + str(nbootstraprep) + " bootstrap estimates of summary statistics...")
+	print(("running " + str(nbootstraprep) + " bootstrap estimates of summary statistics..."))
 	targetstats_filename = args.out + "_bootstrap_n" + str(nbootstraprep) + ".txt"
 	writefile = open(targetstats_filename, 'w')
 
@@ -151,7 +151,7 @@
 			allRegionDER, allRegionANC, allRegionPI, allseqlens = [], [], [], []
 			nsnps, totalregions, totallen = 0, 0, 0
 			inputfilename = inputfilenames[ipop]
-			print("reading allele frequency statistics from: " + inputfilename)
+			print(("reading allele frequency statistics from: " + inputfilename))
 			writefile.write(str(ipop) + '\n')
 			if checkFileExists(inputfilename):
 				allpi, allnderiv, allnanc, nregions, seqlens = readFreqsFile(inputfilename)
@@ -163,7 +163,7 @@
 				totallen += sum(seqlens)
 				for i in range(len(allpi)):
 					nsnps += len(allpi[i])
-			print("TOTAL: logged frequency values for " + str(nsnps) + " SNPS across " + str(totalregions) + ".")
+			print(("TOTAL: logged frequency values for " + str(nsnps) + " SNPS across " + str(totalregions) + "."))
 			
 			####################################
 			#### PI: MEAN & BOOTSTRAP STDERR ###
@@ -230,7 +230,7 @@
 		#print('npops ' + str(npops)) #debug
 		for ipop in range(npops):
 			inputfilename = inputfilenames[ipop]
-			print("reading linkage disequilibrium statistics from: " + inputfilename)
+			print(("reading linkage disequilibrium statistics from: " + inputfilename))
 			writefile.write(str(ipop) + '\n')
 			N_r2regs, N_dprimeregs = 0, 0
 			N_r2snps, N_dprimesnps = 0, 0
@@ -239,8 +239,8 @@
 			N_r2snps += sum([len(x) for x in allRegionr2])
 			N_dprimeregs += ndprimeregions
 			N_dprimesnps += sum([len(x) for x in allRegionDprime])
-			print("\tlogged r2 values for " + str(N_r2snps) + " SNP pairs across " + str(N_r2regs) + " regions.")
-			print("\tlogged D' values for " + str(N_dprimesnps) + " SNP pairs across " + str(N_dprimeregs) + " regions.")
+			print(("\tlogged r2 values for " + str(N_r2snps) + " SNP pairs across " + str(N_r2regs) + " regions."))
+			print(("\tlogged D' values for " + str(N_dprimesnps) + " SNP pairs across " + str(N_dprimeregs) + " regions."))
 
 
 			###################################
@@ -322,25 +322,25 @@
 		npopcomp = len(inputfilenames)
 		for icomp in range(npopcomp):
 			fstfilename	= inputfilenames[icomp]
-			print("reading Fst values from: " + fstfilename)
+			print(("reading Fst values from: " + fstfilename))
 			if checkFileExists(fstfilename):
 				allfst, nregions = readFstFile(fstfilename)
 			else:
-				print('missing ' + fstfilename)
+				print(('missing ' + fstfilename))
 			target_mean, target_se = estimateFstByBootstrap_bysnp(allfst, nrep = nbootstraprep)
 			writeline =  str(icomp) + "\t" + str(target_mean) + "\t" + str(target_se) + '\n'
 			writefile.write(writeline)
-			print("TOTAL: logged Fst values for " + str(len(allfst)) + " SNPs.\n")
+			print(("TOTAL: logged Fst values for " + str(len(allfst)) + " SNPs.\n"))
 
 	writefile.close()
-	print("wrote to file: " + targetstats_filename)
+	print(("wrote to file: " + targetstats_filename))
 	return
 def execute_point(args):
 	'''runs simulates of a point in parameter-space, comparing to specified target'''
 	################
 	## FILE PREP ###
 	################
-	print("generating " + str(args.nCoalescentReps) + " simulations from model: " + args.inputParamFile)
+	print(("generating " + str(args.nCoalescentReps) + " simulations from model: " + args.inputParamFile))
 	statfilename = args.outputDir
 	if args.outputDir[-1] != "/":
 		statfilename += "/"
@@ -372,7 +372,7 @@
 		else:
 			stats, pops = read_error_dimensionsfile(args.calcError) 
 			error = calc_error(statfilename, stats, pops)
-		print(" error: " + str(error)) #record?
+		print((" error: " + str(error))) #record?
 
 	################
 	## VISUALIZE ###
@@ -384,7 +384,7 @@
 	return
 def execute_grid(args):
 	'''run points in parameter-space according to specified grid'''
-	print("loading dimensions of grid to search from: " + args.grid_inputdimensionsfile)
+	print(("loading dimensions of grid to search from: " + args.grid_inputdimensionsfile))
 	gridname, keys, indices, values = read_dimensionsfile(args.grid_inputdimensionsfile, 'grid')
 	assert len(keys) == len(indices) 
 	combos =  [' '.join(str(y) for y in x) for x in product(*values)]
@@ -395,11 +395,11 @@
 		error = sample_point(args.nCoalescentReps, keys, indices, theseValues)
 		errors.append(error)
 	for icombo in range(len(combos)):
-		print(combo[icombo] + "\t" + errors[icombo] + "\n")
+		print((combo[icombo] + "\t" + errors[icombo] + "\n"))
 	return
 def execute_optimize(args):
 	'''run scipy.optimize module according to specified parameters'''
-	print("loading dimensions to search from: " + args.optimize_inputdimensionsfile)
+	print(("loading dimensions to search from: " + args.optimize_inputdimensionsfile))
 	runname, keys, indices = read_dimensionsfile(args.optimize_inputdimensionsfile, runType='optimize')
 
 	rangeDict = get_ranges()
@@ -432,7 +432,7 @@
 		low, high = float(interval[0]), float(interval[1])
 		realVal = get_real_value(result.x[i], low, high)
 		bestparams.append(result.x[i])
-		print("best " + str(key) + "|" + str(index) + "|" + str(realVal))
+		print(("best " + str(key) + "|" + str(index) + "|" + str(realVal)))
 	return
 
 ##########
RefactoringTool: Refactored ./cms/composite.py
--- ./cms/composite.py	(original)
+++ ./cms/composite.py	(refactored)
@@ -226,7 +226,7 @@
 	else:
 		suffix = ""
 	paramfilename = write_run_paramfile(paramfilename, ihs_master, nsl_master, delihh_master, xpehh_master, fst_master, deldaf_master, cutoffline, includeline)
-	print("wrote CMS run parameters to: " + paramfilename)
+	print(("wrote CMS run parameters to: " + paramfilename))
 	altpops = [1, 2, 3, 4]
 	selpop = int(selpop)
 	altpops.remove(selpop)
@@ -288,7 +288,7 @@
 					altpairs.append(pairfilename)
 				else:
 					print('missing')
-					print(in_ihs_file, in_nsl_file, in_delihh_file, in_xp_file, in_fst_deldaf_file)
+					print((in_ihs_file, in_nsl_file, in_delihh_file, in_xp_file, in_fst_deldaf_file))
 			if len(altpairs) !=0:
 				outfile = compositedir + "rep" + str(irep) + "_" + str(selpop) + file_ending + suffix
 				alreadyExists = False
@@ -305,11 +305,11 @@
 					print(fullcmd)
 					execute(fullcmd)
 				else:
-					print(outfile + " already exists") 
+					print((outfile + " already exists")) 
 			else:
 				print("no altpairs")
 	
-	print('calculated CMS scores for ' + str(numPerBin_neut) + ' neutral replicates and ' + str(numPerBin_sel) + " selection replicates per bin.")
+	print(('calculated CMS scores for ' + str(numPerBin_neut) + ' neutral replicates and ' + str(numPerBin_sel) + " selection replicates per bin."))
 	return
 def execute_composite_emp(args):
 	''' given component scores from empirical data (e.g. from scans.py) together with likelihood tables, generate CMS scores '''
@@ -326,7 +326,7 @@
 
 	if args.regional_cms_chrom is None:
 		cmd += "combine/combine_scores_gw" #genome-wide
-		chroms = range(1,23)
+		chroms = list(range(1,23))
 		file_ending = ".cms.gw.out"
 	else:
 		cmd += "combine/combine_scores_local" #within-region
@@ -378,7 +378,7 @@
 		suffix = ""
 	paramfilename += "_" + str(model_selpop)
 	paramfilename = write_run_paramfile(paramfilename, ihs_master, nsl_master, delihh_master, xpehh_master, fst_master, deldaf_master, cutoffline, includeline)
-	print("wrote CMS run parameters to: " + paramfilename)
+	print(("wrote CMS run parameters to: " + paramfilename))
 
 	#############################################
 	## CALCULATE CMS: ITERATE OVER CHROMOSOMES ##
@@ -420,7 +420,7 @@
 				fullcmd = cmd + " " + argstring
 				print(fullcmd)
 				execute(fullcmd)	
-	print('calculated CMS scores for ' + str(len(chroms)) + ' chromosomes.')
+	print(('calculated CMS scores for ' + str(len(chroms)) + ' chromosomes.'))
 
 	return
 def execute_normsims_genomewide(args): 
@@ -448,9 +448,9 @@
 				values.append(rawscore)
 			openfile.close()
 		else:
-			print('missing: ' + outfile)
-
-	print('loaded ' + str(len(values)) + ' values from neutral sims...')
+			print(('missing: ' + outfile))
+
+	print(('loaded ' + str(len(values)) + ' values from neutral sims...'))
 
 	#check for nans
 	values = np.array(values)
@@ -466,10 +466,10 @@
 	var = np.var(values)
 	sd = np.sqrt(var)
 
-	print("max: " + str(max(values)))
-	print("min: " + str(min(values)))
-	print("mean: " + str(np.mean(values)))
-	print("var: " + str(np.var(values)))
+	print(("max: " + str(max(values))))
+	print(("min: " + str(min(values))))
+	print(("mean: " + str(np.mean(values))))
+	print(("var: " + str(np.var(values))))
 
 	############################
 	## NORMALIZE NEUTRAL SIMS ##
@@ -493,7 +493,7 @@
 					writefile.write(writeline)
 				openfile.close()
 				writefile.close()
-	print("wrote to eg: " + normedfile)	
+	print(("wrote to eg: " + normedfile))	
 	
 	########################
 	## NORMALIZE SEL SIMS ##
@@ -518,7 +518,7 @@
 						writefile.write(writeline)
 					openfile.close()
 					writefile.close()
-	print("wrote to eg: " + normedfile)	
+	print(("wrote to eg: " + normedfile))	
 	return
 def execute_normemp_genomewide(args):
 	""" given output from composite_emp, normalize CMS scores genome-wide """  #could also introduce a feature to normalize to explicitly neutral regions. 
@@ -544,11 +544,11 @@
 	scores = scores[~np.isinf(scores)]
 	scores = list(scores)
 	
-	print('loaded ' + str(len(scores)) + " scores")
-	print("max: " + str(max(scores)))
-	print("min: " + str(min(scores)))
-	print("mean: " + str(np.mean(scores)))
-	print("var: " + str(np.var(scores)))
+	print(('loaded ' + str(len(scores)) + " scores"))
+	print(("max: " + str(max(scores))))
+	print(("min: " + str(min(scores))))
+	print(("mean: " + str(np.mean(scores))))
+	print(("var: " + str(np.var(scores))))
 
 	mean = np.mean(scores)
 	var = np.var(scores)
@@ -557,7 +557,7 @@
 	##############
 	## NORMALIZE #
 	############## 
-	chroms = range(1,23)
+	chroms = list(range(1,23))
 	for thischrom in chroms:
 		unnormedfile = get_emp_cms_file(selpop, thischrom, normed=False, basedir=score_basedir, suffix=suffix,) #model #selpop, chrom, normed=False, suffix=suffix, basedir = score_basedir)
 		assert os.path.isfile(unnormedfile)
@@ -575,7 +575,7 @@
 			writefile.write(writeline)
 		readfile.close()
 		writefile.close
-		print('wrote to '  + normedfile)
+		print(('wrote to '  + normedfile))
 	return
 
 ### Visualize and hone in 
@@ -596,7 +596,7 @@
 			startpos = int(args.startpos)
 			endpos = int(args.endpos)
 			haplotypes, coreindex, physpositions = pullRegion(inputfilename, startpos, endpos, args.maf, corePos = args.corepos)
-	print("loaded genotypes for " + str(len(haplotypes[0])) + " sites... ")
+	print(("loaded genotypes for " + str(len(haplotypes[0])) + " sites... "))
 
 	########################
 	## SORT BY SIMILARITY ##
@@ -627,14 +627,14 @@
 				dif = [item - int(snppos) for item in physpositions]
 				minDif = min(dif)
 				minDifIndex = dif.index(minDif)
-				print(str(minDifIndex))
+				print((str(minDifIndex)))
 				ax.axvline(minDifIndex, color="orange")
-				print('found nearest proxy variant based on physical distance ' + str(physpositions[minDifIndex]))
+				print(('found nearest proxy variant based on physical distance ' + str(physpositions[minDifIndex])))
 	if args.title is not None:
 		plt.title(args.title, fontsize=5)
 	plt.tight_layout()
 	plt.savefig(args.out, dpi=float(args.dpi))
-	print("plotted to: " + args.out)
+	print(("plotted to: " + args.out))
 	plt.close()
 	return	
 def execute_ml_region(args):
@@ -662,7 +662,7 @@
 			outfile.write(writestring)
 		infile.close()
 	outfile.close()
-	print("wrote to: " + outfilename)
+	print(("wrote to: " + outfilename))
 	#convertBedGraphtoBigWig:
 	print("for large datasets, convert to BigWig format, e.g.: http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/bedGraphToBigWig\n")
 	return
RefactoringTool: Refactored ./cms/likes_from_model.py
--- ./cms/likes_from_model.py	(original)
+++ ./cms/likes_from_model.py	(refactored)
@@ -128,7 +128,7 @@
 		bounds = bin_starts[ibin], bin_ends[ibin]
 		paramfilename = populateDir + "/params"
 		write_bin_paramfile(neutParamfile, paramfilename, bounds)
-		print('wrote to: ' + paramfilename)	
+		print(('wrote to: ' + paramfilename))	
 	return
 def execute_get_sel_traj(args):
 	'''generate forward trajectories of simulated allele frequencies for demographic scenarios with selection'''
@@ -162,7 +162,7 @@
 			renamed = outbase +"_" + str(ipop) + ".tped"
 			renamecmd = "mv "  + torenamefile + " " + renamed
 			execute(renamecmd)
-	print("wrote simulates to e.g. " + renamed)
+	print(("wrote simulates to e.g. " + renamed))
 	return
 def execute_run_sel_sim(args):
 	''' generates tped data for one replicate from a demographic model parameter file and selection trajectory file using coalescent simulator cosi '''
@@ -188,7 +188,7 @@
 			renamed = outbase +"_" + str(ipop) + ".tped"
 			renamecmd = "mv "  + torenamefile + " " + renamed
 			execute(renamecmd)
-	print("wrote simulates to e.g. " + renamed)
+	print(("wrote simulates to e.g. " + renamed))
 	return
 
 ### Calculate component scores from 
@@ -309,7 +309,7 @@
 				if os.path.isfile(unnormedfile):
 					repfiles.append(unnormedfile)
 				else:
-					print('missing: ' + unnormedfile)
+					print(('missing: ' + unnormedfile))
 			concatfile = open(concatfilename, 'w')
 			for irepfile in range(len(repfiles)):
 				repfile = repfiles[irepfile]
@@ -327,7 +327,7 @@
 						concatfile.write(line)
 				readfile.close()
 			concatfile.close()
-			print('wrote to: ' + concatfilename)
+			print(('wrote to: ' + concatfilename))
 
 		#already have concatfile
 		infilename = concatfilename
@@ -355,7 +355,7 @@
 			with open(binfilename, 'w') as outfile:
 				subprocess.check_output( fullcmd.split(), stderr=outfile)
 			outfile.close()
-			print('wrote to: ' + binfilename)
+			print(('wrote to: ' + binfilename))
 	return
 def execute_norm_from_neut_params(args): 
 	''' using parameters from neutral simulates, normalizes component scores '''
@@ -373,7 +373,7 @@
 	#else:
 	#	scenario_dir = "sel" + str(pop) + "/sel_" + str(args.selbin) + "/"
 	concatfilename, binfilename = get_concat_files(pop, score, altpop, basedir=basedir) #I NEED TO MAKE THIS HANDLE SEL SITUATION
-	print('loading normalization parameters from ' + binfilename + "...")
+	print(('loading normalization parameters from ' + binfilename + "..."))
 	###############
 	## NORMALIZE ##
 	###############
@@ -394,7 +394,7 @@
 			print('currently handling this manually: rewrite_fst_bins.py')
 			pass
 		if irep%100 == 0:
-			print("currently rep: " + str(irep))
+			print(("currently rep: " + str(irep)))
 	return	
 
 ### Define component score likelihood 
@@ -429,7 +429,7 @@
 			if (all_present.count(True)) == 9: #replicate done
 				completed_neut.append(loaded_neut_files)
 		all_completed_neut.append(completed_neut)
-	print("loaded " + str(sum([len(item) for item in all_completed_neut])) + " neutral replicates with complete component scores from " + neutdir)
+	print(("loaded " + str(sum([len(item) for item in all_completed_neut])) + " neutral replicates with complete component scores from " + neutdir))
 	####################
 	## LOAD SEL FILES ## 
 	####################
@@ -474,7 +474,7 @@
 				completed_sel.append(completed_bin)
 			all_completed_sel.append(completed_sel) #[ipop][ibin][irep][iscore]
 			nChunks, chunk_labels = len(binlabels), bin_medians_str
-	print("loaded " + str(selcounter) + " selection replicates with complete component scores")
+	print(("loaded " + str(selcounter) + " selection replicates with complete component scores"))
 	#################################
 	## SORT SCORES INTO HISTOGRAMS ## 
 	################################# 
@@ -543,7 +543,7 @@
 			#f1.ylabel("p(score)")
 			plt.savefig(savefilename)
 			plt.close()
-			print('saved to ' + savefilename)
+			print(('saved to ' + savefilename))
 
 		##################
 		## PER POP COMP ##
@@ -609,7 +609,7 @@
 			#f1.subplots_adjust(hspace=0)
 			plt.savefig(savefilename)
 			plt.close()
-			print('saved to ' + savefilename)
+			print(('saved to ' + savefilename))
 	return
 def execute_plot_likes_vs_scores(args): ##REVISIT
 	''' useful QC step; visualize the function that converts score values into likelihood contributions towards composite scores '''
@@ -629,14 +629,14 @@
 	fig, ax = plt.subplots()
 	ax.scatter(np.arange(len(comp_like_ratios)), comp_like_ratios) #I want to make this use the actual score ranges.
 	plt.savefig(savefilename)
-	print('saved to: ' + savefilename)
+	print(('saved to: ' + savefilename))
 	plt.close()
 
 	savefilename = inputprefix + "_cl_likes_vs_likesscores.png"
 	fig, ax = plt.subplots()
 	ax.scatter(np.arange(len(comp_likes)), comp_likes)
 	plt.savefig(savefilename)
-	print('saved to: ' + savefilename)
+	print(('saved to: ' + savefilename))
 	plt.close()
 	return
 def execute_write_master_likes(args):
RefactoringTool: No changes to ./cms/main.py
RefactoringTool: Refactored ./cms/power.py
--- ./cms/power.py	(original)
+++ ./cms/power.py	(refactored)
@@ -103,7 +103,7 @@
 	savefilename = args.savefilename
 	cmsfilename = args.cmsInfile
 	if os.path.isfile(cmsfilename):
-		print('loading from... ' + cmsfilename)
+		print(('loading from... ' + cmsfilename))
 		physpos, genpos, daf, ihs_normed, delihh_normed, nsl_normed, xpehh_normed, fst, deldaf, cms_unnormed, cms_normed = read_cms_repfile(cmsfilename) #need to make this flexible to regional input vs gw. (vs. likes)
 		causal_index = -1
 		if args.hilitePos is not None:
@@ -118,7 +118,7 @@
 		quick_plot(ax6, physpos, deldaf, "deldaf", causal_index)
 		quick_plot(ax7, physpos, cms_unnormed, "cms", causal_index)
 		plt.savefig(savefilename)
-		print("plotted to " + savefilename)
+		print(("plotted to " + savefilename))
 		plt.close()
 	return
 def execute_distviz(args):
@@ -139,7 +139,7 @@
 		print('must supply input .cms files')
 		sys.exit(0)
 
-	print('loading cms values from ' + str(len(allfiles)) + " files...")
+	print(('loading cms values from ' + str(len(allfiles)) + " files..."))
 
 	#pass index, expectedlen?
 	savefilename = args.savefilename
@@ -190,9 +190,9 @@
 	all_emp_pos, all_emp_scores = [], []
 	for chrom in range(1,numChr +1):
 		emp_cms_filename = get_emp_cms_file(selpop, chrom, normed=True, suffix=suffix, basedir=basedir)
-		print('loading chr ' + str(chrom) + ": " + emp_cms_filename)
+		print(('loading chr ' + str(chrom) + ": " + emp_cms_filename))
 		if not os.path.isfile(emp_cms_filename):
-			print("missing: " + emp_cms_filename)
+			print(("missing: " + emp_cms_filename))
 			break
 		physpos, genpos, seldaf, ihs_normed, delihh_normed, nsl_normed, xpehh_normed, fst, deldaf, cms_unnormed, cms_normed = read_cms_repfile(emp_cms_filename)
 
@@ -210,13 +210,13 @@
 
 	if args.regionsfile is not None:
 		regionchrs, regionstarts, regionends = load_regions(args.regionsfile)
-		print('loaded ' + str(len(regionchrs)) + ' significant regions from ' + args.regionsfile)
+		print(('loaded ' + str(len(regionchrs)) + ' significant regions from ' + args.regionsfile))
 		for iregion in range(len(regionchrs)):
 			regionchr, regionstart, regionend = regionchrs[iregion], regionstarts[iregion], regionends[iregion]
 			this_chrom = int(regionchr.strip('chr'))
 			ichrom = this_chrom-1
 			chrompos, chromscores = all_emp_pos[ichrom], all_emp_scores[ichrom]
-			zipped = zip(chrompos, chromscores)
+			zipped = list(zip(chrompos, chromscores))
 			plotpos, plotvals = [], []
 			for locus in zipped:
 				if locus[0] >= regionstart:
@@ -228,10 +228,10 @@
 
 	if args.percentile is not None:
 		percentile = float(args.percentile)
-		print('plotting data with heuristic cutoff for ' + str(percentile) + " percentile...")
+		print(('plotting data with heuristic cutoff for ' + str(percentile) + " percentile..."))
 		flat_emp_scores = [item for sublist in all_emp_scores for item in sublist if not np.isnan(item)]
 		score_cutoff = float(np.percentile(flat_emp_scores, percentile))
-		print("score cutoff: " + str(score_cutoff))
+		print(("score cutoff: " + str(score_cutoff)))
 		for chrom in range(1,numChr +1):
 			iax = chrom-1
 			ax = axarr[iax]
@@ -242,14 +242,14 @@
 
 			#get empirical scores and positions for pass threshhold and plot them as above with color
 			these_scores, these_pos = all_emp_scores[iax], all_emp_pos[iax]
-			zipped =  zip(these_scores, these_pos)
+			zipped =  list(zip(these_scores, these_pos))
 			significant = [item for item in zipped if item[0] >= score_cutoff]
 			signif_vals = [item[0] for item in significant]
 			signif_pos = [item[1] for item in significant]
 			ax.plot(signif_pos, signif_vals, color=colorDict[pop], linestyle='None', marker=".", markersize=.3)#, markersize=1)
 
 	plt.savefig(savename)
-	print('saved to: ' + savename)
+	print(('saved to: ' + savename))
 	return
 
 ########	Quantify and visualize power
@@ -283,11 +283,11 @@
 						if not np.isnan(causal_rank):
 							this_array.append(causal_rank)
 				else:
-					print("missing; " + cmsfilename)
-	print("for pop 1, loaded " + str(len(causal_ranks_1)) + " replicates.")
-	print("for pop 2, loaded " + str(len(causal_ranks_2)) + " replicates.")
-	print("for pop 3, loaded " + str(len(causal_ranks_3)) + " replicates.")
-	print("for pop 4, loaded " + str(len(causal_ranks_4)) + " replicates.")
+					print(("missing; " + cmsfilename))
+	print(("for pop 1, loaded " + str(len(causal_ranks_1)) + " replicates."))
+	print(("for pop 2, loaded " + str(len(causal_ranks_2)) + " replicates."))
+	print(("for pop 3, loaded " + str(len(causal_ranks_3)) + " replicates."))
+	print(("for pop 4, loaded " + str(len(causal_ranks_4)) + " replicates."))
 
 	cdf_fig, cdf_ax = plt.subplots()
 	if len(causal_ranks_1) > 0:
@@ -308,7 +308,7 @@
 	plt.xlabel('significance thresshold (i.e., examining the top x variants)')
 	plt.savefig(savefilename)
 	plt.close()
-	print('plotted to ' + savefilename)
+	print(('plotted to ' + savefilename))
 	return
 def execute_fpr(args):
 	''' estimate false positive rate for region identification '''
@@ -341,11 +341,11 @@
 				#print(str(rep_percentages) + "\t" + repfilename)
 				if len(rep_percentages) > 0:
 					if max(rep_percentages) > thresshold:
-						print("false positive: " + repfilename)
-
-	print('loaded ' + str(len(all_scores)) + " replicates populations for model " + model + "...")
+						print(("false positive: " + repfilename))
+
+	print(('loaded ' + str(len(all_scores)) + " replicates populations for model " + model + "..."))
 	fpr = calc_pr(all_percentages, thresshold)
-	print('false positive rate: ' + str(fpr) + "\n")
+	print(('false positive rate: ' + str(fpr) + "\n"))
 
 	if args.saveLog	is not None:
 		writefilename = args.saveLog 
@@ -354,7 +354,7 @@
 
 		writefile.write(model + "\t" + str(regionlen) + "\t" + str(thresshold) + '\t' + str(cutoff) + '\n')
 		writefile.close()
-		print('wrote to :  ' + str(writefilename))
+		print(('wrote to :  ' + str(writefilename)))
 	return
 def execute_tpr(args):
 	''' estimate true positive rate for region detection '''
@@ -390,7 +390,7 @@
 					print(repfilename)
 				if os.path.isfile(repfilename):
 					allrepfilenames.append(repfilename)
-		print('loaded ' + str(len(allrepfilenames)) + " replicates...")
+		print(('loaded ' + str(len(allrepfilenames)) + " replicates..."))
 		#numToTake = min(500, len(allrepfilenames))
 		#chosen = np.random.choice(allrepfilenames, numToTake, replace=False) #take random sample	
 		chosen = allrepfilenames #this was just to expedite, no?
@@ -403,9 +403,9 @@
 				rep_percentages = check_rep_windows(physpos, these_scores, regionlen, cutoff = cutoff)
 				all_percentages.append(rep_percentages)		
 
-		print('loaded ' + str(len(all_scores)) + " replicates populations for model " + model + "...")
+		print(('loaded ' + str(len(all_scores)) + " replicates populations for model " + model + "..."))
 		tpr = calc_pr(all_percentages, thresshold)
-		print('true positive rate: ' + str(tpr) + "\n")
+		print(('true positive rate: ' + str(tpr) + "\n"))
 
 		if args.saveLog	is not None:
 			writefilename = args.saveLog +"_" + thislabel
@@ -414,7 +414,7 @@
 
 			writefile.write(model + "\t" + str(regionlen) + "\t" + str(thresshold) + '\t' + str(cutoff) + '\n')
 			writefile.close()
-			print('wrote to :  ' + str(writefilename))
+			print(('wrote to :  ' + str(writefilename)))
 	return	
 def execute_roc(args):
 	''' plot receiver operating characteristic curve -- false positive rate vs. true positive rate '''
@@ -426,8 +426,8 @@
 	savefilename = args.savefilename
 
 	allfpr, alltpr = load_power_dict(modeldir, likes_dir_suffix)
-	fpr_keys = allfpr.keys()
-	tpr_keys = alltpr.keys()
+	fpr_keys = list(allfpr.keys())
+	tpr_keys = list(alltpr.keys())
 	
 	regionlens = list(set([item[0] for item in fpr_keys]))
 	thressholds =list(set([item[1] for item in fpr_keys]))
@@ -453,7 +453,7 @@
 						pass
 
 		if (len(plotfpr)) > 0:
-			plotfpr, plottpr = zip(*sorted(zip(plotfpr, plottpr)))
+			plotfpr, plottpr = list(zip(*sorted(zip(plotfpr, plottpr))))
 			ax.scatter(plotfpr, plottpr, label=str(plot_set), color=colorDict[plot_set], s=.5)
 			
 	plt.suptitle('ROC for ' + model + " " + likes_dir_suffix)
@@ -464,7 +464,7 @@
 	plt.legend()
 	plt.savefig(savefilename)
 	plt.close()
-	print("plotted to " + savefilename)
+	print(("plotted to " + savefilename))
 	return
 def execute_find_cutoff(args): #MUST ADD TRACK OF SUFFIX
 	''' given FPR and TPR calculations, select an optimal significance cutoff subject to a specified criterion '''
@@ -481,9 +481,9 @@
 	for pop in [1, 2, 3, 4, "ave"]:
 		best_tpr, best_fpr = 0, 0
 		best_cutoff = 0
-		print("Now finding optimal with a maximum FPR of " + str(maxFPR) + " for pop " + str(pop) + " using demographic model: " + model)
-		fpr_keys = all_fpr.keys()
-		tpr_keys = all_tpr.keys()
+		print(("Now finding optimal with a maximum FPR of " + str(maxFPR) + " for pop " + str(pop) + " using demographic model: " + model))
+		fpr_keys = list(all_fpr.keys())
+		tpr_keys = list(all_tpr.keys())
 		thesekeys_fpr = [key for key in fpr_keys if pop in key]
 		thesekeys_tpr = [key for key in tpr_keys if pop in key]
 		for key in thesekeys_fpr:
@@ -497,8 +497,8 @@
 						best_cutoff = tprkey
 						best_fpr = all_fpr[key]
 		print(best_cutoff)
-		print("FPR: " + str(best_fpr))
-		print("TPR: " + str(best_tpr) + "\n")
+		print(("FPR: " + str(best_fpr)))
+		print(("TPR: " + str(best_tpr) + "\n"))
 	return
 
 ########	Apply significance cutoffs
@@ -512,7 +512,7 @@
 	windowlen = args.regionlen
 	suffix = args.suffix
 
-	chroms = range(1,23)
+	chroms = list(range(1,23))
 	signif_windows = []
 	####################
 	## LOOP OVER CHRS ##
@@ -521,7 +521,7 @@
 		chrom_signif = []
 		normedempfilename = get_emp_cms_file(pop, chrom, normed=True, suffix=suffix, basedir=basedir)
 		if not os.path.isfile(normedempfilename):
-			print("missing: " + normedempfilename)
+			print(("missing: " + normedempfilename))
 		else:
 			physpos, genpos, seldaf, ihs_normed, delihh_normed, nsl_normed, xpehh_normed, fst, deldaf, cms_unnormed, cms_normed = read_cms_repfile(normedempfilename)
 			for iPos in range(len(physpos)):
@@ -559,7 +559,7 @@
 				writeline = "chr" + str(chromnum) + "\t" + str(starts[iregion]) + "\t" + str(ends[iregion]) + '\n'
 				writefile.write(writeline)
 		writefile.close()
-		print('wrote to ' + writefilename)	
+		print(('wrote to ' + writefilename))	
 	return
 def execute_regionlog(args):
 	input_filelist = args.input_filelist
@@ -588,7 +588,7 @@
 		return
 	else:
 		totalselregions = 0
-		print('loaded regions from ' + str(len(regionfiles)) + " files...")
+		print(('loaded regions from ' + str(len(regionfiles)) + " files..."))
 
 		header = ['chrom', 'start', 'end', 'len (kb)', 'pop', 'genes',]
 		####################
@@ -629,7 +629,7 @@
 				selregion_start, selregion_end = item[1], item[2]
 				key = (selregion_chr, selregion_start, selregion_end)
 				generegion_id = item[6]
-				if key not in geneDict.keys():
+				if key not in list(geneDict.keys()):
 					geneDict[key] = [generegion_id]
 				else:
 					geneDict[key].append(generegion_id)
@@ -641,7 +641,7 @@
 				regionlen = int(end) - int(start)
 				kb_regionlen=round(regionlen/1000)
 
-				if key in geneDict.keys():
+				if key in list(geneDict.keys()):
 					genelist = geneDict[key]
 					genes = set(genelist)
 					genestring = ""
@@ -675,7 +675,7 @@
 		book.save(TemporaryFile())
 	else:
 		writefile.close()
-	print('wrote ' + str(totalselregions) + ' significant regions to: ' + savefilename)
+	print(('wrote ' + str(totalselregions) + ' significant regions to: ' + savefilename))
 	return
 
 ##########
RefactoringTool: Refactored ./cms/run_traj.py
--- ./cms/run_traj.py	(original)
+++ ./cms/run_traj.py	(refactored)
@@ -17,4 +17,4 @@
 		continue
 	itWorked = True	
 
-print("found a trajectory in " + str(nAttempts) + " attempts.")
+print(("found a trajectory in " + str(nAttempts) + " attempts."))
RefactoringTool: No changes to ./cms/scans.py
RefactoringTool: No changes to ./cms/selection.py
RefactoringTool: Refactored ./cms/combine/input_func.py
--- ./cms/combine/input_func.py	(original)
+++ ./cms/combine/input_func.py	(refactored)
@@ -36,9 +36,9 @@
 				p1_ind, ihh1_ind, p2_ind, ihh2_ind = 4, 3, 6, 5
 			else:
 				print(line)
-				print(str(testXp))
-				print(str(testXp2))
-				print('check indices ' + infilename)
+				print((str(testXp)))
+				print((str(testXp2)))
+				print(('check indices ' + infilename))
 			break
 	infile.close()
 	if ".gz" in infilename:
@@ -64,7 +64,7 @@
 		outfile.write(writeline)
 	outfile.close()
 	infile.close()
-	print('wrote to: ' + outfilename)
+	print(('wrote to: ' + outfilename))
 	return outfilename
 def write_pair_sourcefile(writefilename, ihsfilename, delihhfilename, nslfilename, xpehhfilename, freqsfilename):
 	#if not os.path.isfile(writefilename):
RefactoringTool: Refactored ./cms/combine/viz_func.py
--- ./cms/combine/viz_func.py	(original)
+++ ./cms/combine/viz_func.py	(refactored)
@@ -112,7 +112,7 @@
 		haplotypes.append(haplotype)
 
 	if transpose:
-		haplotypes2 = zip(*haplotypes)
+		haplotypes2 = list(zip(*haplotypes))
 		haplotypes = list(haplotypes2)
 
 	if saveHapFile is not None:
@@ -137,8 +137,8 @@
 	
 	
 	for i in range(len(haplotypes)):
-		if i % 10 == 0: print("now haplotype " + str(i) + " out of " + str(len(haplotypes)))
-		hapsToCompare = range(i,len(haplotypes))
+		if i % 10 == 0: print(("now haplotype " + str(i) + " out of " + str(len(haplotypes))))
+		hapsToCompare = list(range(i,len(haplotypes)))
 		a = [difference(haplotypes[i],haplotypes[x]) for x in hapsToCompare]
 		for j in range(len(hapsToCompare)):
 			diffArray[i][hapsToCompare[j]] = a[j]
@@ -213,8 +213,8 @@
 	
 	
 	for i in range(len(haplotypes)):
-		if i % 10 == 0: print("now haplotype " + str(i) + " out of " + str(len(haplotypes)))
-		hapsToCompare = range(i,len(haplotypes))
+		if i % 10 == 0: print(("now haplotype " + str(i) + " out of " + str(len(haplotypes))))
+		hapsToCompare = list(range(i,len(haplotypes)))
 		a = [difference(haplotypes[i],haplotypes[x]) for x in hapsToCompare]
 		for j in range(len(hapsToCompare)):
 			diffArray[i][hapsToCompare[j]] = a[j]
@@ -305,7 +305,7 @@
 	return ax
 def readAnnotations(annotationfilename):
 	if not os.path.isfile(annotationfilename):
-		print("missing file: " + annotationfilename)
+		print(("missing file: " + annotationfilename))
 		return [], []
 	else:
 		positions, annotations = [], []
RefactoringTool: Refactored ./cms/dists/freqbins_func.py
--- ./cms/dists/freqbins_func.py	(original)
+++ ./cms/dists/freqbins_func.py	(refactored)
@@ -40,7 +40,7 @@
 		except:
 			continue
 		itWorked = True	
-	print("found a trajectory in " + str(nAttempts) + " attempts.")
+	print(("found a trajectory in " + str(nAttempts) + " attempts."))
 	assert os.path.isfile(output)
 	return
 def get_bin_strings(bin_medians):
RefactoringTool: Refactored ./cms/dists/likes_func.py
--- ./cms/dists/likes_func.py	(original)
+++ ./cms/dists/likes_func.py	(refactored)
@@ -166,7 +166,7 @@
 		neutfile.close()
 		causalfile.close()
 		linkedfile.close()
-		print('wrote to e.g.: ' + neutwritefilename)
+		print(('wrote to e.g.: ' + neutwritefilename))
 	return
 
 ##########################
@@ -240,13 +240,13 @@
 	#hit_allfreqs_filename = shared_prefix + "_allfreq_" + like_savestring + "_likes_causal.txt"
 	for likes_filename in [neut_filename, linked_filename, hit_hi_filename, hit_mid_filename, hit_low_filename, hit_allfreqs_filename]:
 		if not os.path.isfile(likes_filename):
-			print("missing: " + likes_filename)
+			print(("missing: " + likes_filename))
 	return neut_filename, linked_filename, hit_hi_filename, hit_mid_filename, hit_low_filename, hit_allfreqs_filename
 def write_master_likesfile(writefilename, neut_filename, hit_hi_filename, hit_mid_filename, hit_low_filename):
 	''' designate input frequency for SNP classes  '''
 	for filename in [neut_filename, hit_hi_filename, hit_mid_filename, hit_low_filename]:
 		if not os.path.isfile(filename):
-			print('MASTERFILE REQUIRES: ' + filename)
+			print(('MASTERFILE REQUIRES: ' + filename))
 			return
 	writefile = open(writefilename, 'w')
 	writefile.write(neut_filename + '\n')
@@ -254,7 +254,7 @@
 	writefile.write(hit_mid_filename + '\n')
 	writefile.write(hit_low_filename + '\n')
 	writefile.close()
-	print('wrote to: ' + writefilename)
+	print(('wrote to: ' + writefilename))
 	return
 def get_master_likefiles(likesdir, model, selpop, alt_likes = "vsNeut", likes_suffix = "allFreqs", checkExists = False):
 	''' can toggle checkExists to for creating vs. reading masterlikes files'''
@@ -267,7 +267,7 @@
 	if checkExists:
 		for masterlikesfile in [ihs_master_likesfile, nsl_master_likesfile, delihh_master_likesfile, xpehh_master_likesfile, fst_master_likesfile, deldaf_master_likesfile]:
 			if not os.path.isfile(masterlikesfile):
-				print("MISSING: " + masterlikesfile)
+				print(("MISSING: " + masterlikesfile))
 	return ihs_master_likesfile, nsl_master_likesfile, delihh_master_likesfile, xpehh_master_likesfile, fst_master_likesfile, deldaf_master_likesfile
 def get_likes_savestrings(score, basedir):
 	""" facilitates pulling combinations of likelihood tables for masters, depending e.g. on what has completed """
RefactoringTool: Refactored ./cms/dists/scores_func.py
--- ./cms/dists/scores_func.py	(original)
+++ ./cms/dists/scores_func.py	(refactored)
@@ -129,7 +129,7 @@
 	return
 def norm_sel_ihs(inputScoreFile, neutNormfilename):
 	''' normalize iHS component score for selection scenarios to neutral parameters ''' 
-	print("normalizing selection simulates to neutral: \n" + inputScoreFile + "\n" + neutNormfilename)
+	print(("normalizing selection simulates to neutral: \n" + inputScoreFile + "\n" + neutNormfilename))
 	bins, nums, means, variances = read_neut_normfile(neutNormfilename, 'ihs')
 	#print(str(means))
 	#print(str(variances))
@@ -155,7 +155,7 @@
 		normfile.write(writeline)		
 	openfile.close()
 	normfile.close()
-	print("wrote to: " + normfilename)
+	print(("wrote to: " + normfilename))
 	return
 def norm_neut_xpehh(inputScoreFile, outfileName, runProgram = "scans.py"):##JV: I found the way to properly write to file; should implement here
 	'''wraps call to scans.py'''
@@ -179,7 +179,7 @@
 		normfile.write(writeline)
 	openfile.close()
 	normfile.close()
-	print("wrote to: " + normfilename)
+	print(("wrote to: " + normfilename))
 	return
 
 #############################
@@ -210,10 +210,10 @@
 	neut_values2 = load_from_files(neut_files2,  startbound, endbound, absVal = foldDists)
 	neut_values3 = load_from_files(neut_files3,  startbound, endbound, absVal = foldDists)
 	neut_values4 = load_from_files(neut_files4,  startbound, endbound, absVal = foldDists)
-	print("loaded " + str(len(neut_values1)) + " neutral values for pop 1...")
-	print("loaded " + str(len(neut_values2)) + " neutral values for pop 2...")	
-	print("loaded " + str(len(neut_values3)) + " neutral values for pop 3...")			
-	print("loaded " + str(len(neut_values4)) + " neutral values for pop 4...")
+	print(("loaded " + str(len(neut_values1)) + " neutral values for pop 1..."))
+	print(("loaded " + str(len(neut_values2)) + " neutral values for pop 2..."))	
+	print(("loaded " + str(len(neut_values3)) + " neutral values for pop 3..."))			
+	print(("loaded " + str(len(neut_values4)) + " neutral values for pop 4..."))
 
 	sel_files1 = [all_completed_sel[0][sel_bin_index][irep][scoreindex] for irep in range(len(all_completed_sel[0][sel_bin_index]))]
 	sel_files2 = [all_completed_sel[1][sel_bin_index][irep][scoreindex] for irep in range(len(all_completed_sel[1][sel_bin_index]))]
@@ -223,10 +223,10 @@
 	causal_values2, linked_values2 = load_from_files_discriminate_causal(sel_files2,  startbound, endbound, absVal = foldDists)
 	causal_values3, linked_values3 = load_from_files_discriminate_causal(sel_files3,  startbound, endbound, absVal = foldDists)
 	causal_values4, linked_values4 = load_from_files_discriminate_causal(sel_files4,  startbound, endbound, absVal = foldDists)
-	print("loaded " + str(len(causal_values1)) + " causal SNP and " + str(len(linked_values1)) + " linked SNP iHS values for pop 1...")
-	print("loaded " + str(len(causal_values2)) + " causal SNP and " + str(len(linked_values2)) + " linked SNP iHS values for pop 2...")
-	print("loaded " + str(len(causal_values3)) + " causal SNP and " + str(len(linked_values3)) + " linked SNP iHS values for pop 3...")
-	print("loaded " + str(len(causal_values4)) + " causal SNP and " + str(len(linked_values4)) + " linked SNP iHS values for pop 4...")
+	print(("loaded " + str(len(causal_values1)) + " causal SNP and " + str(len(linked_values1)) + " linked SNP iHS values for pop 1..."))
+	print(("loaded " + str(len(causal_values2)) + " causal SNP and " + str(len(linked_values2)) + " linked SNP iHS values for pop 2..."))
+	print(("loaded " + str(len(causal_values3)) + " causal SNP and " + str(len(linked_values3)) + " linked SNP iHS values for pop 3..."))
+	print(("loaded " + str(len(causal_values4)) + " causal SNP and " + str(len(linked_values4)) + " linked SNP iHS values for pop 4..."))
 
 	all_score_values = [[neut_values1, causal_values1, linked_values1],
 						[neut_values2, causal_values2, linked_values2],
@@ -298,26 +298,26 @@
 		sel_files4c = [all_completed_sel[3][sel_bin_index][irep][7] for irep in range(len(all_completed_sel[3][sel_bin_index]))]
 
 	neut_values1 = load_from_files_flatten(neut_files1a, neut_files1b, neut_files1c, startbound, endbound, scorestring, stripHeader=True, physIndex=physIndex, absVal = foldDists, takeIndex =takeIndex, selDafIndex=selDafIndex)
-	print("loaded " + str(len(neut_values1)) + " neutral values for pop 1 ... (chosen from among three pop comps)")
+	print(("loaded " + str(len(neut_values1)) + " neutral values for pop 1 ... (chosen from among three pop comps)"))
 	neut_values2 = load_from_files_flatten(neut_files2a, neut_files2b, neut_files2c, startbound, endbound, scorestring, stripHeader=True, physIndex=physIndex, absVal = foldDists, takeIndex =takeIndex, selDafIndex=selDafIndex)
-	print("loaded " + str(len(neut_values2)) + " neutral values for pop 2 ... (chosen from among three pop comps)")
+	print(("loaded " + str(len(neut_values2)) + " neutral values for pop 2 ... (chosen from among three pop comps)"))
 	neut_values3 = load_from_files_flatten(neut_files3a, neut_files3b, neut_files3c, startbound, endbound, scorestring, stripHeader=True, physIndex=physIndex, absVal = foldDists, takeIndex =takeIndex, selDafIndex=selDafIndex)
-	print("loaded " + str(len(neut_values3)) + " neutral values for pop 3 ... (chosen from among three pop comps)")
+	print(("loaded " + str(len(neut_values3)) + " neutral values for pop 3 ... (chosen from among three pop comps)"))
 	neut_values4 = load_from_files_flatten(neut_files4a, neut_files4b, neut_files4c, startbound, endbound, scorestring, stripHeader=True, physIndex=physIndex, absVal = foldDists, takeIndex =takeIndex, selDafIndex=selDafIndex)
-	print("loaded " + str(len(neut_values4)) + " neutral values for pop 4 ... (chosen from among three pop comps)")
+	print(("loaded " + str(len(neut_values4)) + " neutral values for pop 4 ... (chosen from among three pop comps)"))
 
 	causal_values1, linked_values1 = load_from_files_discriminate_causal_flatten(sel_files1a, sel_files1b, sel_files1c, startbound, endbound, scorestring, stripHeader=True, physIndex=physIndex, absVal = foldDists, takeIndex =takeIndex, selDafIndex=selDafIndex)
-	print("loaded " + str(len(causal_values1)) + " causal values for pop 1 ... (chosen from among three pop comps)")
-	print("loaded " + str(len(linked_values1)) + " linked values for pop 1 ... (chosen from among three pop comps)")
+	print(("loaded " + str(len(causal_values1)) + " causal values for pop 1 ... (chosen from among three pop comps)"))
+	print(("loaded " + str(len(linked_values1)) + " linked values for pop 1 ... (chosen from among three pop comps)"))
 	causal_values2, linked_values2 = load_from_files_discriminate_causal_flatten(sel_files2a, sel_files2b, sel_files2c, startbound, endbound, scorestring, stripHeader=True, physIndex=physIndex, absVal = foldDists, takeIndex =takeIndex, selDafIndex=selDafIndex)
-	print("loaded " + str(len(causal_values2)) + " causal values for pop 2 ... (chosen from among three pop comps)")
-	print("loaded " + str(len(linked_values2)) + " linked values for pop 2 ... (chosen from among three pop comps)")
+	print(("loaded " + str(len(causal_values2)) + " causal values for pop 2 ... (chosen from among three pop comps)"))
+	print(("loaded " + str(len(linked_values2)) + " linked values for pop 2 ... (chosen from among three pop comps)"))
 	causal_values3, linked_values3 = load_from_files_discriminate_causal_flatten(sel_files3a, sel_files3b, sel_files3c, startbound, endbound, scorestring, stripHeader=True, physIndex=physIndex, absVal = foldDists, takeIndex =takeIndex, selDafIndex=selDafIndex)
-	print("loaded " + str(len(causal_values3)) + " causal values for pop 3 ... (chosen from among three pop comps)")
-	print("loaded " + str(len(linked_values3)) + " linked values for pop 3 ... (chosen from among three pop comps)")
+	print(("loaded " + str(len(causal_values3)) + " causal values for pop 3 ... (chosen from among three pop comps)"))
+	print(("loaded " + str(len(linked_values3)) + " linked values for pop 3 ... (chosen from among three pop comps)"))
 	causal_values4, linked_values4 = load_from_files_discriminate_causal_flatten(sel_files4a, sel_files4b, sel_files4c, startbound, endbound, scorestring, stripHeader=True, physIndex=physIndex, absVal = foldDists, takeIndex =takeIndex, selDafIndex=selDafIndex)	
-	print("loaded " + str(len(causal_values4)) + " causal values for pop 4 ... (chosen from among three pop comps)")
-	print("loaded " + str(len(linked_values4)) + " linked values for pop 4 ... (chosen from among three pop comps)")
+	print(("loaded " + str(len(causal_values4)) + " causal values for pop 4 ... (chosen from among three pop comps)"))
+	print(("loaded " + str(len(linked_values4)) + " linked values for pop 4 ... (chosen from among three pop comps)"))
 	
 	all_score_values = [[neut_values1, causal_values1, linked_values1],
 						[neut_values2, causal_values2, linked_values2],
RefactoringTool: No changes to ./cms/model/bootstrap_func.py
RefactoringTool: Refactored ./cms/model/error_func.py
--- ./cms/model/error_func.py	(original)
+++ ./cms/model/error_func.py	(refactored)
@@ -128,8 +128,8 @@
 		d = float(target_var[i])
 		partialsum = ((a-c)**2)/(math.sqrt(d))
 		if verbose == True:
-			print("(" + str(a) + "-" + str(c) + ")^2  / " + str(d))
-			print(str(partialsum))
+			print(("(" + str(a) + "-" + str(c) + ")^2  / " + str(d)))
+			print((str(partialsum)))
 		sum_bins += partialsum
 	return (sum_bins/n)**.5
 def calc_error(statfilename, stats = ['pi', 'sfs', 'anc', 'r2', 'dprime', 'fst'], pops = [1, 2, 3, 4], piScaleFactor = 100, verbose=False): 
@@ -138,7 +138,7 @@
 	targetStats = get_target_values() 
 	simStats = read_custom_statsfile(statfilename, len(pops))
 	if verbose:
-		print("getting statfile from " + statfilename)
+		print(("getting statfile from " + statfilename))
 	popPairs = []
 	for i in range(len(pops)):
 		for j in range(i, len(pops)):
@@ -162,7 +162,7 @@
 
 				RMS = root_mean_square_error(sim_val, target_val, target_var)
 				if verbose:
-					print("RMS, " + str(item) + ": " + str(RMS) + "\t")
+					print(("RMS, " + str(item) + ": " + str(RMS) + "\t"))
 				tot += (RMS ** 2)
 				counter +=1
 		else:
@@ -178,7 +178,7 @@
 				if stat == "pi": 
 					RMS *= piScaleFactor
 				if verbose:
-					print("RMS, " + str(item) + ": " + str(RMS) + "\t")
+					print(("RMS, " + str(item) + ": " + str(RMS) + "\t"))
 				tot += (RMS ** 2)
 				counter +=1
 
RefactoringTool: No changes to ./cms/model/params_func.py
RefactoringTool: No changes to ./cms/model/plot_func.py
RefactoringTool: Refactored ./cms/model/search_func.py
--- ./cms/model/search_func.py	(original)
+++ ./cms/model/search_func.py	(refactored)
@@ -42,7 +42,7 @@
 	ranges = get_ranges()
 	params = generate_params()
 	scaledParams = {}
-	for key in ranges.keys():
+	for key in list(ranges.keys()):
 		low, high = float(ranges[key][0]), float(ranges[key][1])
 		interval = high - low
 		value = params[key]
RefactoringTool: Refactored ./cms/power/parse_func.py
--- ./cms/power/parse_func.py	(original)
+++ ./cms/power/parse_func.py	(refactored)
@@ -50,7 +50,7 @@
 		in_fst_deldaf_file = basedir + "freqs/chr" + str(chrom) + "_strictMask_" + str(pop) + "_" + str(altpop)
 	for filename in [in_ihs_file, in_delihh_file, in_nsl_file, in_xp_file, in_fst_deldaf_file]:
 		if not os.path.isfile(filename):
-			print("MISSING: " + filename)
+			print(("MISSING: " + filename))
 	return in_ihs_file, in_delihh_file, in_nsl_file, in_xp_file, in_fst_deldaf_file
 def get_sim_component_score_files(model, irep, selpop, scenariopop, altpop, selbin = "neut", filebase = "/idi/sabeti-scratch/jvitti/clean/scores/", normed = False):
 	""" locates component score files for simulated data """
@@ -70,7 +70,7 @@
 		in_xp_file += ".norm"
 	for returnfile in [in_ihs_file, in_nsl_file, in_delihh_file, in_xp_file, in_fst_deldaf_file]:
 		if not os.path.isfile(returnfile):
-			print("Missing: " + returnfile)
+			print(("Missing: " + returnfile))
 	return in_ihs_file, in_nsl_file, in_delihh_file, in_xp_file, in_fst_deldaf_file 
 def get_concat_files(model, pop, score, altpop = '', basedir = "/idi/sabeti-scratch/jvitti/clean/scores/"):
 	""" locates concatenated component score files to facilitate normalization to neutral replicates """
@@ -131,7 +131,7 @@
 	#modeldir + "sel" + str(pop) + "/tpr" + likes_dir_suffix + "/tpr_" + str(regionlen) + "_" + str(percentage) + "_" + str(cutoff) + "_" + str(selFreq)
 	for filename in [tprfile, fprfile]:
 		if not os.path.isfile(filename):
-			print("missing: " + filename)
+			print(("missing: " + filename))
 		else:
 			pass
 	return fprfile, tprfile 
@@ -141,7 +141,7 @@
 	if normed:
 		filename += ".norm"
 	if not os.path.isfile(filename):
-		print("MISSING empirical file : " + filename)
+		print(("MISSING empirical file : " + filename))
 	return filename
 
 ##################
@@ -211,11 +211,11 @@
 	return all_simscores
 def load_empscores(selpop, normed = False, basedir =  "", suffix = '', takeIndex = -1):
 	""" for genome-wide empirical CMS data, loads a value according to takeIndex """	
-	chroms = range(1,23)
+	chroms = list(range(1,23))
 	scores = []
 	for chrom in chroms:
 		scorefile = get_emp_cms_file(selpop, chrom, normed=normed, basedir=basedir, suffix=suffix)
-		print('loading from ' + scorefile)
+		print(('loading from ' + scorefile))
 		assert os.path.isfile(scorefile)
 		openfile = open(scorefile, 'r')
 		openfile.readline() #header
@@ -265,7 +265,7 @@
 							allpops_fpr.append(fpr)
 							allpops_tpr.append(tpr)
 						else:
-							print("missing " + tprfile + "\t" + fprfile)				
+							print(("missing " + tprfile + "\t" + fprfile))				
 					ave_fpr = np.average(allpops_fpr)
 					ave_tpr = np.average(allpops_tpr)
 					popave_key = (regionlen, percentage, cutoff, "ave")
RefactoringTool: Refactored ./cms/power/power_func.py
--- ./cms/power/power_func.py	(original)
+++ ./cms/power/power_func.py	(refactored)
@@ -25,7 +25,7 @@
 		#assert(os.path.isfile(hitlikesfilename) and os.path.isfile(misslikesfilename))
 		writefile.write(hitlikesfilename + "\n" + misslikesfilename + "\n")
 	writefile.close()
-	print("wrote to: " + writefilename)
+	print(("wrote to: " + writefilename))
 	return
 
 ###############
@@ -181,13 +181,13 @@
 	allvals = allvals[~np.isinf(allvals)]
 	#allvals = list(allvals)
 	#print(allvals)
-	print("percentile for score = 10: " + str(percentileofscore(allvals, 10)))
-	print("percentile for score = 15: " + str(percentileofscore(allvals, 15)))
+	print(("percentile for score = 10: " + str(percentileofscore(allvals, 10))))
+	print(("percentile for score = 15: " + str(percentileofscore(allvals, 15))))
 	if len(allvals) > 0:
 		f, ax = plt.subplots(1)
 		ax.hist(allvals, bins=numBins)
 		plt.savefig(savefilename)
-		print('plotted to ' + savefilename)
+		print(('plotted to ' + savefilename))
 	return
 def plotManhattan(ax, neut_rep_scores, emp_scores, chrom_pos, nSnps, maxSkipVal = 0, zscores = True):
 	#neut_rep_scores.sort()
@@ -218,15 +218,15 @@
 				#	pval = get_pval(neut_rep_scores, item)
 				#pvalues.append(pval)
 
-			print("calculated pvalues for chrom " + str(chrom))
-			chrom_pos = range(lastpos, lastpos + len(pvalues))
+			print(("calculated pvalues for chrom " + str(chrom)))
+			chrom_pos = list(range(lastpos, lastpos + len(pvalues)))
 
 			logtenpvals = [(-1. * math.log10(pval)) for pval in pvalues]
 			ax.scatter(chrom_pos, logtenpvals, color =plotcolor, s=.5)
 			lastpos = chrom_pos[-1]
 		else:
 
-			chrom_pos = range(lastpos, lastpos + len(emp_scores[ichrom]))
+			chrom_pos = list(range(lastpos, lastpos + len(emp_scores[ichrom])))
 			ax.scatter(chrom_pos, emp_scores[ichrom], color=plotcolor, s=.5)
 			lastpos = chrom_pos[-1]
 	return ax
RefactoringTool: No changes to ./cms/test/__init__.py
RefactoringTool: No changes to ./cms/test/integration/test_main.py
RefactoringTool: No changes to ./cms/test/unit/test_scans.py
RefactoringTool: No changes to ./cms/test/unit/test_selection.py
RefactoringTool: Refactored ./cms/test/unit/test_tools.py
--- ./cms/test/unit/test_tools.py	(original)
+++ ./cms/test/unit/test_tools.py	(refactored)
@@ -38,7 +38,7 @@
 
 @pytest.fixture(params=all_tool_tests())
 def tool_class(request):
-    print(request.param)
+    print((request.param))
     return request.param
 
 
RefactoringTool: Refactored ./cms/tools/__init__.py
--- ./cms/tools/__init__.py	(original)
+++ ./cms/tools/__init__.py	(refactored)
@@ -21,8 +21,8 @@
     from urllib.parse import urlparse    # pylint: disable=E0611
 except ImportError:
     # Python 2.x
-    from urllib import urlretrieve # pylint: disable=E0611
-    from urlparse import urlparse # pylint: disable=import-error
+    from urllib.request import urlretrieve # pylint: disable=E0611
+    from urllib.parse import urlparse # pylint: disable=import-error
 
 # Put all tool files in __all__
 # allows "from tools import *" to import all tooles for testtools
@@ -555,7 +555,7 @@
             _log.warning("failed to decode JSON output from conda create: %s", result.stdout.decode("UTF-8"))
             return # return rather than raise so we can fall back to the next install method
 
-        if "error" in data.keys() and "prefix already exists" in data["error"]:
+        if "error" in list(data.keys()) and "prefix already exists" in data["error"]:
             # the environment already exists
             # the package may not be installed...
             _log.debug("Conda environment already exists. Installing package...")
@@ -583,7 +583,7 @@
             self.apply_patches()
 
         else:
-            if "success" in data.keys() and data["success"]:
+            if "success" in list(data.keys()) and data["success"]:
                 # we were able to create the environment and install the package
                 _log.debug("Conda environment created and package installed.")
 
RefactoringTool: Refactored ./cms/tools/selscan.py
--- ./cms/tools/selscan.py	(original)
+++ ./cms/tools/selscan.py	(refactored)
@@ -14,7 +14,7 @@
 import operator
 
 try:
-    from itertools import izip as zip # pylint:disable=redefined-builtin
+     # pylint:disable=redefined-builtin
 except ImportError: # py3 zip is izip
     pass
 
@@ -110,7 +110,7 @@
         if samples_to_include is not None and len(samples_to_include) > 0:
             indices_of_matching_samples = sorted([processor.sample_names.index(x) for x in samples_to_include])
         else:
-            indices_of_matching_samples = range(0,len(processor.sample_names))
+            indices_of_matching_samples = list(range(0,len(processor.sample_names)))
 
         indices_of_matching_genotypes = [(x*2, (x*2)+1) for x in indices_of_matching_samples]
         indices_of_matching_genotypes = list(np.ravel(np.array(indices_of_matching_genotypes)))
@@ -309,8 +309,8 @@
                                 if sec_remaining > 10:
                                     human_time_remaining = relative_time(datetime.utcnow()+time_left)
                                     print("")
-                                    print("Completed: {:.2%}".format(float(current_pos_bp)/float(end_pos)))
-                                    print("Estimated time of completion: {}".format(human_time_remaining))
+                                    print(("Completed: {:.2%}".format(float(current_pos_bp)/float(end_pos))))
+                                    print(("Estimated time of completion: {}".format(human_time_remaining)))
                                     #log.info("Genotype counts found: %s", str(list(recordLengths)))
 
 
RefactoringTool: No changes to ./cms/util/__init__.py
RefactoringTool: Refactored ./cms/util/call_sample_reader.py
--- ./cms/util/call_sample_reader.py	(original)
+++ ./cms/util/call_sample_reader.py	(refactored)
@@ -8,7 +8,7 @@
 import os
 from collections import defaultdict
 try:
-    from urllib2 import urlopen
+    from urllib.request import urlopen
 except ImportError: # for py3
     from urllib.request import urlopen
 
@@ -53,7 +53,7 @@
 
         reader = csv.DictReader(f, delimiter="\t")
         for row in  reader:
-            sample_membership[row["sample"]] = {k.replace(" ", "_"):v for k,v in row.items() if k not in ("sample", "")}
+            sample_membership[row["sample"]] = {k.replace(" ", "_"):v for k,v in list(row.items()) if k not in ("sample", "")}
         if isLocal:
             f.close()
         return sample_membership
@@ -107,7 +107,7 @@
         '''
             Add the metadata keys for a given row to the sample key of the sample_membership dict
         '''
-        self.sample_membership[row["sample"]] = {k.replace(" ", "_"):v for k,v in row.items() if k not in ("sample", "")}
+        self.sample_membership[row["sample"]] = {k.replace(" ", "_"):v for k,v in list(row.items()) if k not in ("sample", "")}
 
 
     def store_population_membership(self, row, rowFieldNames):
@@ -163,7 +163,7 @@
             return self.sample_names
 
         samples_to_include = set()
-        for key, value in kwargs.items():
+        for key, value in list(kwargs.items()):
             patterns = []
             # handle passing in either a single string or a list of strings
             if isinstance(value, list):
@@ -171,7 +171,7 @@
             else:
                 patterns.append(value)
 
-            samples_to_include |= set((k for k, v in self.sample_membership.items() if v[key] in patterns))
+            samples_to_include |= set((k for k, v in list(self.sample_membership.items()) if v[key] in patterns))
         return list(samples_to_include)
 
     def get_population_for_sample(self, sample_name):
RefactoringTool: Refactored ./cms/util/cmd.py
--- ./cms/util/cmd.py	(original)
+++ ./cms/util/cmd.py	(refactored)
@@ -58,7 +58,7 @@
         the values of the object on as parameters to the function call.
     '''
     def _main(args):
-        args2 = dict((k,v) for k,v in vars(args).items() if k not in ('loglevel','tmpDir','tmpDirKeep','version','func_main','command'))
+        args2 = dict((k,v) for k,v in list(vars(args).items()) if k not in ('loglevel','tmpDir','tmpDirKeep','version','func_main','command'))
         mainfunc(**args2)
     _main.__doc__ = mainfunc.__doc__
     return _main
@@ -112,7 +112,7 @@
     log.info("software version: %s, python version: %s", __version__, sys.version)
     log.info("command: %s %s %s", 
         sys.argv[0], sys.argv[1],
-        ' '.join(["%s=%s" % (k,v) for k,v in vars(args).items() if k not in ('command', 'func_main')]))
+        ' '.join(["%s=%s" % (k,v) for k,v in list(vars(args).items()) if k not in ('command', 'func_main')]))
     
     if hasattr(args, 'tmpDir'):
         """ 
RefactoringTool: Refactored ./cms/util/db_helper.py
--- ./cms/util/db_helper.py	(original)
+++ ./cms/util/db_helper.py	(refactored)
@@ -340,7 +340,7 @@
                               }
 
         matchingPopSets = []
-        for k,v in popSets.iteritems():
+        for k,v in popSets.items():
             if v["pops"]==set(pop_ids) and v["superpops"] == set(superpop_ids):
                 matchingPopSets.append(k)
 
RefactoringTool: No changes to ./cms/util/file.py
RefactoringTool: Refactored ./cms/util/json_helpers.py
--- ./cms/util/json_helpers.py	(original)
+++ ./cms/util/json_helpers.py	(refactored)
@@ -35,7 +35,7 @@
                     metaDataDict.setdefault(key_to_act_on, []).append(dict_to_set)
                 else:
             
-                    for k,v in dict_to_set.iteritems():
+                    for k,v in dict_to_set.items():
                         if not key_to_act_on:
                             metaDataDict[k] = v
                         else:
RefactoringTool: Refactored ./cms/util/misc.py
--- ./cms/util/misc.py	(original)
+++ ./cms/util/misc.py	(refactored)
@@ -1,5 +1,5 @@
 '''A few miscellaneous tools. '''
-from __future__ import print_function, division  # Division of integers with / should never round!
+  # Division of integers with / should never round!
 import collections
 import itertools
 import logging
@@ -53,7 +53,7 @@
         out.setdefault(i, 0)
         out[i] += 1
         tot += 1
-    for k, v in out.items():
+    for k, v in list(out.items()):
         yield (k, v, float(v) / tot)
     for i in zero_checks:
         if i not in out:
@@ -84,10 +84,10 @@
     next(b, None)
     if hasattr(itertools, 'izip'):
         # Python 2
-        return itertools.izip(a, b)
+        return zip(a, b)
     else:
         # Python 3
-        return zip(a, b)
+        return list(zip(a, b))
 
 
 def batch_iterator(iterator, batch_size):
RefactoringTool: No changes to ./cms/util/parallel.py
RefactoringTool: No changes to ./cms/util/recom_map.py
RefactoringTool: Refactored ./cms/util/stats.py
--- ./cms/util/stats.py	(original)
+++ ./cms/util/stats.py	(refactored)
@@ -1,5 +1,5 @@
 '''A few pure-python statistical tools to avoid the need to install scipy. '''
-from __future__ import division # Division of integers with / should never round!
+ # Division of integers with / should never round!
 from math import exp, log, sqrt, gamma, lgamma, erf
 import itertools
 
@@ -141,7 +141,7 @@
 
     p0 = prob_of_table(table[0])
     result = 0
-    for firstRowM1 in itertools.product(*[range(min(rowSums[0], colSums[i]) + 1)
+    for firstRowM1 in itertools.product(*[list(range(min(rowSums[0], colSums[i]) + 1))
                                          for i in range(n - 1)]) :
         lastElmt = rowSums[0] - sum(firstRowM1)
         if lastElmt < 0 or lastElmt > colSums[-1] :
RefactoringTool: Refactored ./cms/util/vcf_reader.py
--- ./cms/util/vcf_reader.py	(original)
+++ ./cms/util/vcf_reader.py	(refactored)
@@ -4,7 +4,7 @@
 import re
 
 try:
-    from itertools import izip as zip # pylint:disable=redefined-builtin
+     # pylint:disable=redefined-builtin
 except ImportError: # py3 zip is izip
     pass
 
@@ -49,7 +49,7 @@
         Consume an iterable not reading it into memory; return the number of items.
         """
         counter = count()
-        deque(zip(iterable, counter), maxlen=0)  # (consume at C speed)
+        deque(list(zip(iterable, counter)), maxlen=0)  # (consume at C speed)
         return next(counter)
 
     @staticmethod
RefactoringTool: Refactored ./cms/util/version.py
--- ./cms/util/version.py	(original)
+++ ./cms/util/version.py	(refactored)
@@ -127,4 +127,4 @@
 
 
 if __name__ == "__main__":
-    print(get_version())
+    print((get_version()))
RefactoringTool: Refactored ./cms/util/old/annot.py
--- ./cms/util/old/annot.py	(original)
+++ ./cms/util/old/annot.py	(refactored)
@@ -4,7 +4,7 @@
 __version__ = "PLACEHOLDER"
 __date__ = "PLACEHOLDER"
 
-import sqlite3, itertools, urllib, logging, re, os
+import sqlite3, itertools, urllib.request, urllib.parse, urllib.error, logging, re, os
 import util.file, util.misc
 
 log = logging.getLogger(__name__)
@@ -43,10 +43,10 @@
                 self.cur.executemany("""insert into annot (chr,pos,allele_ref,allele_alt,
                     effect,impact,gene_id,gene_name,protein_pos,residue_ref,residue_alt)
                     values (?,?,?,?,?,?,?,?,?,?,?)""",
-                    imap(lambda row:
+                    map(lambda row:
                         [row['CHROM'], int(row['POS']), row['REF'], row['ALT']] +
                         parse_eff(row['CHROM'], row['POS'], row['INFO']),
-                        ifilter(lambda r: r['ALT'] != '.', ffp)))
+                        filter(lambda r: r['ALT'] != '.', ffp)))
             except Exception:
                 log.exception("exception processing file %s line %s", snpEffVcf, ffp.line_num)
                 raise
@@ -116,7 +116,7 @@
                 gene_name = 'tRNA 3-trailer sequence RNase, putative'
             else:
                 try:
-                    gene_name = urllib.unquote_plus(other[5]).encode('ascii')
+                    gene_name = urllib.parse.unquote_plus(other[5]).encode('ascii')
                 except UnicodeDecodeError:
                     log.error("error at %s:%s decoding the string '%s'" % (chrom, pos, other[5]))
                     raise
RefactoringTool: Refactored ./cms/util/old/vcf.py
--- ./cms/util/old/vcf.py	(original)
+++ ./cms/util/old/vcf.py	(refactored)
@@ -149,12 +149,12 @@
     # count up
     out = {'n_tot':len(alleles)}
     acounts = util.misc.histogram(alleles)
-    alist = sorted([(n,a) for a,n in acounts.items()])
+    alist = sorted([(n,a) for a,n in list(acounts.items())])
 
     # daf
     if ancestral is not None:
         out['a_ancestral'] = ancestral
-        derived = list(sorted([a for a in acounts.keys() if a!=ancestral]))
+        derived = list(sorted([a for a in list(acounts.keys()) if a!=ancestral]))
         out['a_derived'] = ','.join(derived)
         out['dac'] = sum(acounts[a] for a in derived)
         out['daf'] = out['n_tot'] and float(out['dac'])/out['n_tot'] or None
RefactoringTool: Refactored ./docs/conf.py
--- ./docs/conf.py	(original)
+++ ./docs/conf.py	(refactored)
@@ -19,7 +19,7 @@
 # If extensions (or modules to document with autodoc) are in another directory,
 # add these directories to sys.path here. If the directory is relative to the
 # documentation root, use os.path.abspath to make it absolute, like shown here.
-print "PATH: {}".format(os.path.dirname(os.path.abspath('../cms/cms/')))
+print("PATH: {}".format(os.path.dirname(os.path.abspath('../cms/cms/'))))
 sys.path.insert(0, os.path.dirname(os.path.abspath('../cms/cms/')))
 
 # -- Mock out the heavyweight pip packages, esp those that require C ----
@@ -71,9 +71,9 @@
 master_doc = 'index'
 
 # General information about the project.
-project = u'CMS'
-copyright = u'2015, Broad Institute'
-author = u'Broad Institute'
+project = 'CMS'
+copyright = '2015, Broad Institute'
+author = 'Broad Institute'
 
 # The version info for the project you're documenting, acts as replacement for
 # |version| and |release|, also used in various other places throughout the
@@ -252,8 +252,8 @@
 # (source start file, target name, title,
 #  author, documentclass [howto, manual, or own class]).
 latex_documents = [
-  (master_doc, 'CMS.tex', u'CMS Documentation',
-   u'Broad Institute', 'manual'),
+  (master_doc, 'CMS.tex', 'CMS Documentation',
+   'Broad Institute', 'manual'),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
@@ -282,7 +282,7 @@
 # One entry per manual page. List of tuples
 # (source start file, name, description, authors, manual section).
 man_pages = [
-    (master_doc, 'cms', u'CMS Documentation',
+    (master_doc, 'cms', 'CMS Documentation',
      [author], 1)
 ]
 
@@ -296,7 +296,7 @@
 # (source start file, target name, title, author,
 #  dir menu entry, description, category)
 texinfo_documents = [
-  (master_doc, 'CMS', u'CMS Documentation',
+  (master_doc, 'CMS', 'CMS Documentation',
    author, 'CMS', 'One line description of project.',
    'Miscellaneous'),
 ]
RefactoringTool: Refactored ./old/fastcms.py
--- ./old/fastcms.py	(original)
+++ ./old/fastcms.py	(refactored)
@@ -8,13 +8,13 @@
 import numpy as np
 import pandas as pd
 #from into import into
-from Operations.MiscUtil import Dict, dbg, AddFileSfx, MakeSeq, StatKeeper
-from Operations.tsvutils import DefineRulesTo_computeMeanStd, DefineRulesTo_normalizeOneColumn, \
+from .Operations.MiscUtil import Dict, dbg, AddFileSfx, MakeSeq, StatKeeper
+from .Operations.tsvutils import DefineRulesTo_computeMeanStd, DefineRulesTo_normalizeOneColumn, \
     computeMeanStd_binned_tsvs, normalizeInBins_tsv
-from Operations.Shari_Operations.localize import subs
-from Operations.Shari_Operations.localize.fstBySNP_Npops import fst_onePopPair
-from Operations.Shari_Operations.localize.CMS import CMSBins
-from Operations.bioutil import genomeBuild2genMapSfx
+from .Operations.Shari_Operations.localize import subs
+from .Operations.Shari_Operations.localize.fstBySNP_Npops import fst_onePopPair
+from .Operations.Shari_Operations.localize.CMS import CMSBins
+from .Operations.bioutil import genomeBuild2genMapSfx
 
 def getFN_xpop_signif( sweepDir, chrom, pop1, pop2 ):
     """Return filename of xpop significance scores from Sweep output"""
RefactoringTool: Refactored ./old/Classes/DotData.py
--- ./old/Classes/DotData.py	(original)
+++ ./old/Classes/DotData.py	(refactored)
@@ -2,7 +2,7 @@
 Classes and functions pertaining to the DotData class, a structured tabular data object. 
 '''
 
-from __future__ import with_statement
+
 import numpy, types, copy, sys, itertools, operator, System.Utils
 from Operations.DotDataFunctions.SaveDotDataAsSV import *
 from Operations.DotDataFunctions.SaveDotData import *
@@ -14,6 +14,7 @@
 from Operations.DotDataFunctions.DictionaryOps import MergeDicts, RestrictDict
 from System.Colors import GrayScale
 from Operations.MiscUtil import dbg, TableIterInnerJoin, TblIterFromDotData, tabwriten
+from functools import reduce
 
 class DotData(numpy.core.records.recarray):
 	"""A numpy recarray (a table with named columns where each column is of a uniform Python type),
@@ -138,7 +139,7 @@
 
 		assert not ( Path and SVPath )
 
-		if isinstance( Array, types.StringTypes ):
+		if isinstance( Array, (str,) ):
 			if Array.endswith( '.data' ) or Array.endswith( '.data/' ):
 				Path = Array
 			else:
@@ -152,9 +153,8 @@
 			
 
 		# Ensure that any iterable can be used as arguments: e.g. tuples, not just lists
-		Records, Columns, names, ToLoad = map( lambda x: x if x == None or isinstance(x, types.ListType)
-						       else list(x),
-						       ( Records, Columns, names, ToLoad ) )
+		Records, Columns, names, ToLoad = [x if x == None or isinstance(x, list)
+						       else list(x) for x in ( Records, Columns, names, ToLoad )]
 
 		###
 		
@@ -195,7 +195,7 @@
 			if len(coloringsInNames) == 0:
 				self.coloring = coloring				
 			else:
-				print "Warning: the following coloring keys,", coloringsInNames, ", are also attribute (column) names in the DotData. This is not allowed, and so these coloring keys will be deleted. The corresponding columns of data will not be lost and will retain the same names."
+				print("Warning: the following coloring keys,", coloringsInNames, ", are also attribute (column) names in the DotData. This is not allowed, and so these coloring keys will be deleted. The corresponding columns of data will not be lost and will retain the same names.")
 				for c in coloringsInNames:
 					coloring.pop(c)
 				self.coloring = coloring
@@ -245,7 +245,7 @@
 
 		"""
 
-		if isinstance(expr, types.CodeType) or isinstance(expr, types.StringType):
+		if isinstance(expr, types.CodeType) or isinstance(expr, bytes):
 			exprCode = expr if isinstance(expr, types.CodeType) \
 			    else compile( expr, sys._getframe(0).f_code.co_filename, 'eval' )
 
@@ -273,8 +273,8 @@
 		elif callable(expr):
 			return self[ numpy.array( [ expr(r) for r in self.recordsAsDicts() ]  ) ]
 		elif isinstance(expr, dict):
-			exprItems = frozenset( [ (columnName, val) for columnName, val in expr.items() if val != None ] )
-			return self[ numpy.array( [ exprItems <= frozenset( r.items() ) for r in self.recordsAsDicts() ]  ) ]
+			exprItems = frozenset( [ (columnName, val) for columnName, val in list(expr.items()) if val != None ] )
+			return self[ numpy.array( [ exprItems <= frozenset( list(r.items()) ) for r in self.recordsAsDicts() ]  ) ]
 		else:
 			raise TypeError('selectRows() requires either an expression (string or code object) or a dictionary')
 
@@ -331,21 +331,21 @@
 
 		"""
 
-		if 'coloring' in dir(self) and attribute in self.coloring.keys():
-			restrictedcoloring = dict([(a,self.coloring[a]) for a in self.coloring.keys() if set(self.coloring[a]) < set(self.coloring[attribute])])
+		if 'coloring' in dir(self) and attribute in list(self.coloring.keys()):
+			restrictedcoloring = dict([(a,self.coloring[a]) for a in list(self.coloring.keys()) if set(self.coloring[a]) < set(self.coloring[attribute])])
 			return DotData(Columns = [numpy.core.records.recarray.__getitem__(self.view(),attrib) for attrib in self.coloring[attribute]],dtype = numpy.dtype([(a,self.dtype[a].str) for a in self.coloring[attribute]]),coloring = restrictedcoloring,rowdata = self.rowdata)
-		elif isinstance(attribute,(list,tuple)) and all( isinstance( x, types.StringType ) for x in attribute ):
+		elif isinstance(attribute,(list,tuple)) and all( isinstance( x, bytes ) for x in attribute ):
 			if not ( set(attribute) <= set(self.coloring.keys()).union(self.dtype.names) ):
-				print "Unknown column name(s):", set(attribute) - set(self.coloring.keys()).union(self.dtype.names)
+				print("Unknown column name(s):", set(attribute) - set(self.coloring.keys()).union(self.dtype.names))
 			assert set(attribute) <= set(self.coloring.keys()).union(self.dtype.names)
 			attribset = []
 			for att in attribute:
 				if att in self.dtype.names and att not in attribset:
 					attribset += [att]
-				elif att in self.coloring.keys():
+				elif att in list(self.coloring.keys()):
 					attribset += [j for j in self.coloring[att] if j not in attribset]
 			#return DotData(Columns = [numpy.core.records.recarray.__getitem__(self.view(),a) for a in attribset], names = attribset, coloring = dict([(a,self.coloring[a]) for a in attribute if a in self.coloring.keys()]))
-			return DotData(Columns = [numpy.core.records.recarray.__getitem__(self.view(),a) for a in attribset], names = attribset, coloring = dict([(a,list(set(self.coloring[a]).intersection(set(attribute)))) for a in self.coloring.keys() if len(set(self.coloring[a]).intersection(set(attribute))) > 0]),rowdata=self.rowdata)
+			return DotData(Columns = [numpy.core.records.recarray.__getitem__(self.view(),a) for a in attribset], names = attribset, coloring = dict([(a,list(set(self.coloring[a]).intersection(set(attribute)))) for a in list(self.coloring.keys()) if len(set(self.coloring[a]).intersection(set(attribute))) > 0]),rowdata=self.rowdata)
 		elif isinstance(attribute,numpy.ndarray) or (isinstance(attribute,list)):
 			if self.rowdata != None:
 				rowdata = self.rowdata[attribute]
@@ -462,7 +462,7 @@
 		#assert all( len( blank ) == dotData.numCols() for dotData, blank in zip( dotDatas, blanks ) )
 
 
-		return DotDataFromTblIter( TableIterInnerJoin( tableIters = map( TblIterFromDotData, dotDatas ),
+		return DotDataFromTblIter( TableIterInnerJoin( tableIters = list(map( TblIterFromDotData, dotDatas )),
 							       cols = primaryKeyCols,
 							       suffixes = suffixes,
 							       blanks = blanks ) )
@@ -531,7 +531,7 @@
 		newRecords = [ ]
 
 		# Create a flat list that has a triple (primaryKeyVal, dotDataId, row) for each occurrence of a primary key value in a dotData row.
-		vals = sorted( [ (primaryKeyVal if isinstance( primaryKeyCol, types.StringType ) or \
+		vals = sorted( [ (primaryKeyVal if isinstance( primaryKeyCol, bytes ) or \
 					  len( primaryKeyCol ) == 1 else tuple( primaryKeyVal ), dotDataId, row ) \
 				  for dotDataId, ( dotData, primaryKeyCol ) in enumerate( zip( dotDatas, primaryKeyCols ) ) \
 					 for row, primaryKeyVal in enumerate( dotData[ primaryKeyCol ] ) ] )
@@ -544,7 +544,7 @@
 			if verbose:
 				if ( iteration % 100 ) == 0:
 					iteration = 0
-					print "merging: primary key ", primaryKeyVal
+					print("merging: primary key ", primaryKeyVal)
 				iteration += 1
 					
 			# For the primary key value 'primaryKeyVal', make a map from dotDataId to the row in that dotData where this primary key value occurs.
@@ -556,10 +556,10 @@
 			if innerJoin and len( dotDataId2row ) < len( dotDatas ): continue
 			newRecord = reduce( operator.concat, \
 						   [ tuple( dotDatas[ dotDataId ][ dotDataId2row[ dotDataId ] ] \
-								    if dotDataId2row.has_key( dotDataId ) \
+								    if dotDataId in dotDataId2row \
 								    else blanks[ dotDataId ] ) \
 						     for dotDataId in range( len( dotDatas ) ) ] )
-			newRecordType = map( type, newRecord )
+			newRecordType = list(map( type, newRecord ))
 			if recordType == None: recordType = newRecordType
 			else: assert newRecordType == recordType
 			newRecords.append( newRecord )
@@ -627,7 +627,7 @@
 			new = numpy.array(new)
 			for a in cols:
 				if self.dtype[a] < new.dtype:
-					print 'WARNING: dtype of column', a, 'is inferior to dtype of ', new, 'which may cause problems.'
+					print('WARNING: dtype of column', a, 'is inferior to dtype of ', new, 'which may cause problems.')
 				self[a][(self[a] == old)[rows]] = new
 		else:
 			for a in cols:
@@ -643,9 +643,9 @@
 				else:
 					ok = set(range(65536)).difference(avoid)
 					if len(ok) > 0:
-						sep = unichr(list(ok)[0])
+						sep = chr(list(ok)[0])
 					else:
-						print 'All unicode characters represented in column', a ,', can\t replace quickly.'
+						print('All unicode characters represented in column', a ,', can\t replace quickly.')
 						QuickRep = False
 						
 				if QuickRep:
@@ -655,7 +655,7 @@
 				self[a][rows] = numpy.cast[self.dtype[a]](newrows)
 					
 				if newrows.dtype > self.dtype[a]:
-						print 'WARNING: dtype of column', a, 'is inferior to dtype of its replacement which may cause problems.'
+						print('WARNING: dtype of column', a, 'is inferior to dtype of its replacement which may cause problems.')
 
 	def isPrimaryKey(self, columnName):
 		"""Test whether the given column is a primary key, i.e. that each row has a
@@ -663,19 +663,19 @@
 		return len( frozenset( self[ columnName ] ) ) == self.numRows()
 
 	def dbg(self, msg = None):
-		print '=============================='
+		print('==============================')
 		if msg is not None:
-		    print '\n' + msg + '\n'
-		    print '=============================='
+		    print('\n' + msg + '\n')
+		    print('==============================')
 		tabwriten( sys.stdout, self.headings )
 		nlines = 0
 		for line in self:
-		    sys.stdout.write( '\n' + '\t'.join( map( str, line )
-							if ( hasattr(line,'__iter__') and not isinstance(line,types.StringTypes) )
+		    sys.stdout.write( '\n' + '\t'.join( list(map( str, line ))
+							if ( hasattr(line,'__iter__') and not isinstance(line,(str,)) )
 							else (str(line),) ) )
 		    nlines += 1
 		sys.stdout.write( '\n%d rows\n' % nlines )
-		print '=============================='
+		print('==============================')
 
 	def aggregate(self,On=None,AggFuncDict = None,AggFunc=None):
 		"""
@@ -771,7 +771,7 @@
 		
 		#s = len(ANLen) - ANLen.argsort() - 1
 		s = ANLen.argsort()
-		Aggregates = Aggregates[s[range(len(Aggregates)-1,-1,-1)]]
+		Aggregates = Aggregates[s[list(range(len(Aggregates)-1,-1,-1))]]
 		
 		if not interspersed or len(AggVars) == 0:
 			return dv.datavstack([X,Aggregates])
@@ -783,17 +783,17 @@
 			HH = {}
 			for l in uniqify(Aggregates.rowdata['Aggregates']):
 				Avars = l.split(',')
-				print Avars
+				print(Avars)
 				HH[l] = System.Utils.FastRecarrayEqualsPairs(X[Avars][Diffs[:-1]],Aggregates[Avars])
 			
 			Order = []
 			for i in range(len(Diffs)-1):
 				t = time.time()
-				Order.extend(range(Diffs[i],Diffs[i+1]))
+				Order.extend(list(range(Diffs[i],Diffs[i+1])))
 				
 				Get = []
-				for l in HH.keys():
-					Get += [len(X) + j  for j in HH[l][2][range(HH[l][0][i],HH[l][1][i])] if len(set(DiffAtts[i]).intersection(Aggregates.rowdata['Aggregates'][j].split(','))) > 0 and set(Aggregates.rowdata['Aggregates'][j].split(',')) == set(l.split(','))]
+				for l in list(HH.keys()):
+					Get += [len(X) + j  for j in HH[l][2][list(range(HH[l][0][i],HH[l][1][i]))] if len(set(DiffAtts[i]).intersection(Aggregates.rowdata['Aggregates'][j].split(','))) > 0 and set(Aggregates.rowdata['Aggregates'][j].split(',')) == set(l.split(','))]
 	
 				Order.extend(Get)
 			
@@ -876,11 +876,11 @@
 		AggFuncDict = {}
 		
 	if AggFunc != None:
-		AggFuncDict.update( dict([(o,AggFunc) for o in Off if o not in AggFuncDict.keys()]) )
-		
-	NotProvided = Off.difference(AggFuncDict.keys()) if AggFuncDict else Off
+		AggFuncDict.update( dict([(o,AggFunc) for o in Off if o not in list(AggFuncDict.keys())]) )
+		
+	NotProvided = Off.difference(list(AggFuncDict.keys())) if AggFuncDict else Off
 	if len(NotProvided) > 0:
-		print 'No aggregation function provided for axes: ' , NotProvided, 'so assuming "sum".'
+		print('No aggregation function provided for axes: ' , NotProvided, 'so assuming "sum".')
 		AggFuncDict.update(dict([(o,sum) for o in NotProvided]))
 
 	if len(On) > 0:	
@@ -989,7 +989,7 @@
 			[AA,BB] = System.Utils.fastequalspairs(cvals,X[c])
 			
 			for (i,cc) in enumerate(cvals):
-				print time.time() - t, i, cc
+				print(time.time() - t, i, cc)
 				blist = [str(bv).replace(' ','') for bv in Bvals if bv in X[b][AA[i]:BB[i]]]
 				D.coloring[str(cc)] = [a] + [bn + '_' + d for bn in blist for d in NonTrivials]
 				for d in NonTrivials:
@@ -1031,7 +1031,7 @@
 			
 def DotDataFromTblIter( tableIter ):
 	"""Construct a DotData from a TableIter"""
-	return DotData( names = tableIter.headings, Records = map( tuple, tableIter ) )
+	return DotData( names = tableIter.headings, Records = list(map( tuple, tableIter )) )
 
 def SaveToSV( dotData, outFN, getio = None ):
 	"""Save given DotData to a .tsv file"""
RefactoringTool: Refactored ./old/Operations/IDotData.py
--- ./old/Operations/IDotData.py	(original)
+++ ./old/Operations/IDotData.py	(refactored)
@@ -45,18 +45,19 @@
 
   """
 
-from __future__ import with_statement, division
+
 
 __all__ = ( 'IDotData', )
 
 import sys, os, logging, itertools, operator, copy, contextlib, time, numbers, types, glob, math, collections, \
-    heapq, inspect, abc, __builtin__
+    heapq, inspect, abc, builtins
 import traceback as tb
 from abc import ABCMeta, abstractmethod
 from Operations.MiscUtil import chomp, dbg, coerceVal, is_sorted, IsSeq, MakeSeq, MakeDir, DumpFile, SlurpFileLines, \
     WaitForFileToAppear, joinstr, Sfx, IsScalar, DictGet, flatten, SystemSucceed, ReplaceFileExt, BreakString, joinstr, \
     RestrictDict, MergeDicts, tabwriten, AtomicForIsSeq, DbgIter, Dict, OpenForRead, OpenForWrite, SumKeeper, tmap, \
     MapPath, DictGetNotNone, tabwrite, AddFileSfx, SplitStr, FirstVal, ExtractOpts, IsFileType, iter_ith
+from functools import reduce
 
 try:
     import numpy as np
@@ -115,7 +116,7 @@
         """
 
         if IsSeq( key ): return tuple( self[k] for k in key )
-        return  coerceVal( self.lineVals[ self.colName2num[ key ] if isinstance( key, types.StringTypes ) else key ] )
+        return  coerceVal( self.lineVals[ self.colName2num[ key ] if isinstance( key, (str,) ) else key ] )
 
     def GetStrItem(self, key):
         """Get value of one field of this named tuple, either by field name or by position (column number).
@@ -124,11 +125,11 @@
         """
 
         if IsSeq( key ): return tuple( self[k] for k in key )
-        return  self.lineVals[ self.colName2num[ key ] if isinstance( key, types.StringTypes ) else key ]
-
-    
-    def __iter__(self): return itertools.imap( coerceVal, self.lineVals )
-    def __repr__(self): return str( zip( self.headings, self.lineVals ) )
+        return  self.lineVals[ self.colName2num[ key ] if isinstance( key, (str,) ) else key ]
+
+    
+    def __iter__(self): return map( coerceVal, self.lineVals )
+    def __repr__(self): return str( list(zip( self.headings, self.lineVals )) )
     def __len__(self): return len(self.headings)
 
     def __add__(self,other):
@@ -139,13 +140,13 @@
     
     def __eq__(self,other):
         other = MakeSeq( other )
-        return len(self) == len(other) and all([ a == b for a,b in itertools.izip( self, other ) ])
+        return len(self) == len(other) and all([ a == b for a,b in zip( self, other ) ])
 
     def __ne__(self,other): return not self == other
     def __lt__(self,other):
         other = MakeSeq( other )
-        return any( a < b for a,b in itertools.izip( self, other ) ) \
-            or len(self) < len(other) and all([ a == b for a,b in itertools.izip( self, other ) ])
+        return any( a < b for a,b in zip( self, other ) ) \
+            or len(self) < len(other) and all([ a == b for a,b in zip( self, other ) ])
 
     def __le__(self,other): return self == other or self < other
     def __gt__(self,other): return not self <= other
@@ -165,9 +166,7 @@
         self.names = names
 
     
-class IDotDataRoot(collections.Iterable, AtomicForIsSeq):
-
-    __metaclass__ = ABCMeta
+class IDotDataRoot(collections.Iterable, AtomicForIsSeq, metaclass=ABCMeta):
 
     """Abstract base class for various IDotData implementations.
     The implementation (a sublcass of IDotDataRoot) provides the tuple of headings when it calls
@@ -236,11 +235,11 @@
            for which item is True.  
         """
 
-        if all([ isinstance( it, types.StringTypes ) for it in MakeSeq( item ) ]):
+        if all([ isinstance( it, (str,) ) for it in MakeSeq( item ) ]):
             for it in MakeSeq( item ):
                 if it not in self.headings: raise AttributeError( item )
             return IDotDataGetColumns( parent = self, item = item )
-        if isinstance( item, ( types.IntType, types.LongType ) ): return iter_ith( self, item )
+        if isinstance( item, int ): return iter_ith( self, item )
         if isIDotData( item ): return IDotDataFilterBool( parent = self, filter = item )
         if hasattr( type( item ), 'isDotData' ): return self[ IDotData.fromDotData( item ) ] 
         asDotData = self.toDotData()[ item ]
@@ -380,12 +379,12 @@
 
     def mapVals(self, f):
         """Return an IDotData with all values mapped by the given function"""
-        return self.mapRecords( ( lambda x: f(x) ) if self.numCols() == 1 else ( lambda r: map( f, r ) ) )
+        return self.mapRecords( ( lambda x: f(x) ) if self.numCols() == 1 else ( lambda r: list(map( f, r )) ) )
 
     def addComputedCols( self, newColNames, newColFn):
         """Add a column computed from the records according to specified function"""
 
-        if isinstance( newColNames, types.StringTypes ) and ( ' ' in newColNames or '\t' in newColNames ):
+        if isinstance( newColNames, (str,) ) and ( ' ' in newColNames or '\t' in newColNames ):
             newColNames = tuple( newColNames.split( '\t' if '\t' in newColNames else None ) )
         
         return self.mapRecords( func = lambda r: tuple( r ) + tuple( MakeSeq( newColFn( r ) ) ),
@@ -479,10 +478,10 @@
     def __len__(self):
         """Return the number of data rows in this IDotData.   Note that the first time this is called,
         it may take linear time to run."""
-        if self.len is None: self.len = sum( itertools.imap( lambda x: 1, self ) )
+        if self.len is None: self.len = sum( map( lambda x: 1, self ) )
         return self.len
 
-    def __nonzero__(self):
+    def __bool__(self):
         """Return True if self has at least one row, False otherwise"""
 
         try:
@@ -496,10 +495,10 @@
     #def __neg__(self): return self.
 
     def dbg(self, msg = None):
-        print '=============================='
+        print('==============================')
         if msg is not None:
-            print '\n' + msg + '\n'
-            print '=============================='
+            print('\n' + msg + '\n')
+            print('==============================')
         tabwrite( sys.stdout, *self.headings )
         nlines = 0
         for line in self:
@@ -509,7 +508,7 @@
 #                                                else (str(line),) ) )
             nlines += 1
         sys.stdout.write( '\n%d rows\n' % nlines )
-        print '=============================='
+        print('==============================')
 
     def dbgGraph(self, fname):
         """Write a graph representation of the current object"""
@@ -583,7 +582,7 @@
 
         multiPass = DictGet( kwargs, 'multiPass', True )
         if not isinstance( multiPass, bool ):
-            for v in itertools.izip( *[ self.groupby( *cols, multiPass = False ) for i in range( multiPass ) ] ):
+            for v in zip( *[ self.groupby( *cols, multiPass = False ) for i in range( multiPass ) ] ):
                 yield ( v[0][0], ) + tmap( operator.itemgetter( 1 ), v )
             return
 
@@ -614,7 +613,7 @@
 
         """
 
-        if isinstance(expr, types.CodeType) or isinstance(expr, types.StringTypes):
+        if isinstance(expr, types.CodeType) or isinstance(expr, (str,)):
             exprCode = expr if isinstance(expr, types.CodeType) \
                 else compile( expr, sys._getframe(0).f_code.co_filename, 'eval' )
 
@@ -647,8 +646,8 @@
         elif callable(expr):
             return self.filter( expr )
         elif isinstance(expr, dict):
-            exprItems = frozenset( [ (columnName, val) for columnName, val in expr.items() if val != None ] )
-            return self.filter( lambda r: exprItems <= frozenset( r.asDict().items() ) )
+            exprItems = frozenset( [ (columnName, val) for columnName, val in list(expr.items()) if val != None ] )
+            return self.filter( lambda r: exprItems <= frozenset( list(r.asDict().items()) ) )
         else:
             raise TypeError('selectRows() requires either an expression (string or code object) or a dictionary')
 
@@ -699,7 +698,7 @@
             meansStds = imeanstd( self[ cols ] )
             if len( cols ) == 1: meansStds = [ meansStds ]
         
-        col2meanStd = dict( zip( cols, meansStds ) )
+        col2meanStd = dict( list(zip( cols, meansStds )) )
 
         colMeanStds = [ DictGet( col2meanStd, c, ( None, None ) ) for c in self.headings ]
 
@@ -1067,11 +1066,11 @@
         self.headings = tuple( headings )
 
         if not len( set( headings ) ) == len( headings ):
-            print 'headings=', headings
+            print('headings=', headings)
             seen = set()
             for h in headings:
                 if h in seen:
-                    print 'duplicate heading: ', h
+                    print('duplicate heading: ', h)
                 seen.add( h )
             
         assert len( set( headings ) ) == len( headings )
@@ -1082,7 +1081,7 @@
         self.valueFixer = valueFixer
         self.lineNum = 0
         
-    def next(self):
+    def __next__(self):
         try:
             lineVals = self._nextLine()
 
@@ -1090,16 +1089,16 @@
 
             if self.lineNum % 200000  ==  0:
                 logging.info( joinstr(' ', self.lineNum, ' of ', \
-                                  self.fname, ': ', zip( self.headings, lineVals ) ) )
+                                  self.fname, ': ', list(zip( self.headings, lineVals )) ) )
 
             if len( lineVals ) != len( self.headings ):
                 logging.error( 'invalid table format: header has %d columns, line %d has %d items'
                                % ( len( self.headings ), self.lineNum, len( lineVals ) ) ) 
-                print 'lineVals=', lineVals, ' self.headings=', self.headings, ' \nzip=', zip( self.headings, lineVals )
+                print('lineVals=', lineVals, ' self.headings=', self.headings, ' \nzip=', list(zip( self.headings, lineVals )))
                 
             assert len( lineVals ) == len( self.headings )
 
-            if self.valueFixer: lineVals = map( self.valueFixer, lineVals )
+            if self.valueFixer: lineVals = list(map( self.valueFixer, lineVals ))
 
             return IDotDataRecord( lineVals = lineVals,
                                    colName2num = self.colName2num,
@@ -1132,7 +1131,7 @@
         self.item = item
 
     def _iter(self):
-        return itertools.imap( self.getter, self.parent.recordsIter() )
+        return map( self.getter, self.parent.recordsIter() )
 
     def __str__(self):
         return 'IDotDataGetColumns(' + ','.join( self.item ) + ')'
@@ -1150,7 +1149,7 @@
         self.filter = filter
 
     def _iter(self):
-        for p, f in itertools.izip( self.parent, self.filter.flatIter() ):
+        for p, f in zip( self.parent, self.filter.flatIter() ):
             assert isinstance( f, bool )
             if f: yield p
 
@@ -1196,7 +1195,7 @@
     def __init__(self, parent, renamings):
         renamings = dict( renamings )
         assert set( renamings.keys() ) <= set( parent.headings ), \
-            'keys are %s headings are %s' % ( renamings.keys(), parent.headings )
+            'keys are %s headings are %s' % ( list(renamings.keys()), parent.headings )
         
         super(type(self), self).__init__( headings =
                                           [ DictGet( renamings, h, h) for h in parent.headings ],
@@ -1269,10 +1268,10 @@
     def __init__(self, fname, **tableReadOpts ):
         self.fname = fname
         self.tableReadOpts = tableReadOpts
-        exec ExtractOpts( tableReadOpts,
+        exec(ExtractOpts( tableReadOpts,
                           'headings', None,
                           'headingSep', None,
-                          'commentPrefix', '##' )
+                          'commentPrefix', '##' ))
         self.comments = []
 
         if headings is None:
@@ -1315,13 +1314,13 @@
         super(type(self),self).__init__( headings = tuple( SlurpFileLines( headerFN ) ) )
 
         tsvFiles = [ f for f in reduce( operator.concat,
-                                        map( operator.itemgetter(2), os.walk( fname ) ) ) if f.endswith('.csv') ]
+                                        list(map( operator.itemgetter(2), os.walk( fname ) )) ) if f.endswith('.csv') ]
         colNames = [ os.path.splitext( os.path.splitext( tsvF )[0] )[0] for tsvF in tsvFiles ]
 
         assert len( set( colNames ) ) == len( colNames )
         assert sorted( colNames ) == sorted( self.headings )
         
-        col2file = dict( zip( colNames, tsvFiles ) )
+        col2file = dict( list(zip( colNames, tsvFiles )) )
         self.allFiles = [ fname + col2file[ col ] for col in self.headings ]
 
     def _iter(self):
@@ -1358,7 +1357,7 @@
 
     def _nextLine(self):
         """Get next line"""
-        rawLine = chomp( self.freader.next() )
+        rawLine = chomp( next(self.freader) )
         if not rawLine:
 
             # possible bug: if there is only one heading and the line is
@@ -1371,12 +1370,12 @@
 
         # skip any comment lines.  normally these only appear at the beginning.
         while rawLine.startswith( commentPrefix ):
-            rawLine = chomp( self.freader.next() )
+            rawLine = chomp( next(self.freader) )
             if not rawLine: raise StopIteration
 
         while self.skipFirstLines > 0:
             dbg( '"skipping" rawLine' )
-            rawLine = chomp( self.freader.next() )
+            rawLine = chomp( next(self.freader) )
             if not rawLine: raise StopIteration
             self.skipFirstLines -= 1
 
@@ -1402,14 +1401,14 @@
                                          valueFixer = valueFixer )
         
         self.fs = tuple( map( open, allFiles ) )
-        self.freaders = map( iter, self.fs )
+        self.freaders = list(map( iter, self.fs ))
 
     def _nextLine(self):
         """Get next line"""
-        result =  tuple( chomp( freader.next() ) for freader in self.freaders )
+        result =  tuple( chomp( next(freader) ) for freader in self.freaders )
         if not result: raise StopIteration
         if len(result) != len( self.fs ):
-            print 'result=', result
+            print('result=', result)
         assert len(result) == len( self.fs )
         return result
 
@@ -1463,7 +1462,7 @@
                                  'Columns multiPass ToLoad' ) ).oneRegion( **( iddRegionInfo[ CanonFN( fname ) ] ) )
     
     if not headings and names: headings = names
-    if isinstance( headings, types.StringTypes ) and ( ' ' in headings or '\t' in headings ):
+    if isinstance( headings, (str,) ) and ( ' ' in headings or '\t' in headings ):
         headings = tuple( headings.split( '\t' if '\t' in headings else None ) )
     if not skipFirstLines and SVSkipFirstLines: skipFirstLines = SVSkipFirstLines
     if SVDelimiter: sep = SVDelimiter
@@ -1501,12 +1500,12 @@
 
 def makeIdxPrepender( idx ):
     """Creates a function that prepends a given index to its argument, returning a tuple."""
-    return lambda( v ): ( idx, v )
+    return lambda v: ( idx, v )
 
 class attrDbg(object):
 
     def __getattribute__(self, attr):
-        print 'getting attr ', attr
+        print('getting attr ', attr)
         return attr
     
 def makeKeyGetter( k ):
@@ -1551,24 +1550,24 @@
 
     if keys is not None:
         keys = [ ( operator.itemgetter( k ) if isinstance( k, (int,tuple,list) ) else
-                   ( operator.attrgetter( k ) if isinstance( k, types.StringTypes ) else k ) ) for k in keys ]
+                   ( operator.attrgetter( k ) if isinstance( k, (str,) ) else k ) ) for k in keys ]
     
     if ids:
-        return itermerge( iters = [ itertools.imap( makeIdxPrepender( idx ), i ) for idx, i in
-                                    ( enumerate( iters ) if ids == True else zip( ids, iters ) ) ],
+        return itermerge( iters = [ map( makeIdxPrepender( idx ), i ) for idx, i in
+                                    ( enumerate( iters ) if ids == True else list(zip( ids, iters )) ) ],
                           keys = ( operator.itemgetter( 1 ), ) * len( iters ) if keys is None else
-                          map( makeKeyGetter, keys ),
+                          list(map( makeKeyGetter, keys )),
                           includeKeys = includeKeys )
     
     if keys is not None:
-        result = itermerge( iters = [ itertools.imap( makeKeyApplier( k ), i )
+        result = itermerge( iters = [ map( makeKeyApplier( k ), i )
                                       for k, i in zip( keys, iters ) ] )
-        if not includeKeys: result = itertools.imap( operator.itemgetter( 1 ), result )
+        if not includeKeys: result = map( operator.itemgetter( 1 ), result )
         return result
     
     def merge( i1, i2 ):
-        next1 = iter( i1 ).next
-        next2 = iter( i2 ).next
+        next1 = iter( i1 ).__next__
+        next2 = iter( i2 ).__next__
         try:        
             v1 = next1()
         except StopIteration:
@@ -1644,8 +1643,8 @@
                 positionFilled[ i ] = True
 
         if all( positionFilled ):
-            rec = map( tuple, recordsList )
-            assert map( len, rec ) == headingLens
+            rec = list(map( tuple, recordsList ))
+            assert list(map( len, rec )) == headingLens
             rec = reduce( operator.concat, rec )
             yield IDotDataRecord( lineVals = rec + ( k if keyHeadings else () ), colName2num = colName2num,
                                   fname = None, headings = headings + keyHeadings )
@@ -1667,7 +1666,7 @@
     
 
 def GetIDotDatas(iDotDatas):
-    return tuple( IDotData(iDotData) if isinstance(iDotData, types.StringTypes) else
+    return tuple( IDotData(iDotData) if isinstance(iDotData, (str,)) else
                   ( IDotData.fromDotData( iDotData ) if hasattr( type( iDotData ), 'isDotData' ) else iDotData )
                   for iDotData in iDotDatas  )
 
@@ -1702,7 +1701,7 @@
 
 def IDotDataAsTuples( iDotDatas, cols, blanks = None ):
 
-    return TableIterInnerJoinAuxAsTuples( tableIters = map( iter, iDotDatas ), cols = cols,
+    return TableIterInnerJoinAuxAsTuples( tableIters = list(map( iter, iDotDatas )), cols = cols,
                                           blanks = blanks if blanks is not None else [ (None,)*len(idd.headings)
                                                                                        for idd in iDotDatas ],
                                           headingLens = [ len( idd.headings ) for idd in iDotDatas ] )
@@ -1726,7 +1725,7 @@
 
         # check that the keys are sorted in strictly increasing order -- important for correct operation of join 
         if not( prevKey==None or k > prevKey ):
-            print 'prevKey=', prevKey, ' key=', k, ' g is ', tuple( g )
+            print('prevKey=', prevKey, ' key=', k, ' g is ', tuple( g ))
         assert prevKey==None or k > prevKey
         prevKey = k
 
@@ -1734,8 +1733,8 @@
 
         origins = [ r[1][0] for r in records ]
         if not is_sorted( origins, strict = True ):
-            print 'records are ', records
-            print 'origins are ', origins
+            print('records are ', records)
+            print('origins are ', origins)
         assert is_sorted( origins, strict = True )
 
         recordsList = [ None ] * len( tableIters )
@@ -1761,15 +1760,15 @@
 
     def __init__(self, *iDotDatas, **kwargs):
 
-        print 'befFlatten: ', iDotDatas
+        print('befFlatten: ', iDotDatas)
         iDotDatas = flatten( iDotDatas )
 
-        print 'aftFlatten: ', iDotDatas
+        print('aftFlatten: ', iDotDatas)
         assert len( iDotDatas ) > 0
 
-        iDotDatas = map( IDotData, iDotDatas )
+        iDotDatas = list(map( IDotData, iDotDatas ))
         if 'sourceCol' in kwargs:
-            ids = DictGetNotNone( kwargs, 'sourceIds', map( operator.attrgetter( 'fname' ), iDotDatas ) )
+            ids = DictGetNotNone( kwargs, 'sourceIds', list(map( operator.attrgetter( 'fname' ), iDotDatas )) )
             dbg( 'kwargs["sourceCol"] ids' )
             iDotDatas = [ iDotData.hstack( IDotData.repeat( kwargs[ 'sourceCol' ], idd_id ) )
                           for idd_id, iDotData in zip( ids, iDotDatas ) ]
@@ -1829,11 +1828,11 @@
 
         lineValsGetter = operator.attrgetter( 'lineVals' )
         def mergeRecs( *vals ):
-            return  IDotDataRecord( lineVals = reduce( operator.concat, map( lineValsGetter, vals ) ),
+            return  IDotDataRecord( lineVals = reduce( operator.concat, list(map( lineValsGetter, vals )) ),
                                     colName2num = self.colName2num,
                                     headings = self.headings )
 
-        return itertools.imap( mergeRecs, *[ idd.recordsIter() for idd in self.iDotDatas ] )
+        return map( mergeRecs, *[ idd.recordsIter() for idd in self.iDotDatas ] )
 
 
 class IDotDataFilter(IDotDataRoot):
@@ -1860,7 +1859,7 @@
         self.iDotDatas = iDotDatas
         self.func = func
 
-    def _iter(self): return itertools.imap( self.func, *self.iDotDatas )
+    def _iter(self): return map( self.func, *self.iDotDatas )
 
 class IDotDataStarMap(IDotDataRoot):
 
@@ -1934,7 +1933,7 @@
         self.ifFalse = ifFalse
 
     def _iter(self):
-        for whichVal, ifTrueVal, ifFalseVal in itertools.izip( self.which, self.ifTrue, self.ifFalse ):
+        for whichVal, ifTrueVal, ifFalseVal in zip( self.which, self.ifTrue, self.ifFalse ):
             yield ifTrueVal if whichVal else ifFalseVal
 
 IDotData.where = IDotDataWhere
@@ -1962,7 +1961,7 @@
         self.choices = choices
 
     def _iter(self):
-        for r in itertools.izip( self.which, *self.choices ):
+        for r in zip( self.which, *self.choices ):
             yield r[ r[0] + 1 ]
 
 IDotData.choose = IDotDataChoose
@@ -1989,7 +1988,7 @@
     """
 
     def __init__(self, headings, iterable, multiPass = False):
-        if isinstance( headings, types.StringTypes ) and ( ' ' in headings or '\t' in headings ):
+        if isinstance( headings, (str,) ) and ( ' ' in headings or '\t' in headings ):
             headings = tuple( headings.split( '\t' if '\t' in headings else None ) )
         super(type(self),self).__init__( headings = headings )
         if multiPass and isinstance( iterable, collections.Iterator ): iterable = tuple( iterable )
@@ -2019,15 +2018,13 @@
 
 IDotData.fromDotData = IDotDataFromDotData
 
-class IDotDataWriterRoot(object):
-    __metaclass__ = ABCMeta
-
+class IDotDataWriterRoot(object, metaclass=ABCMeta):
     """Abstract base class for classes that help you incrementally write out an IDotData file record-by-record."""
 
     def __init__(self, headings):
         #assert headings
 
-        if isinstance( headings, types.StringTypes ) and ( ' ' in headings or '\t' in headings ):
+        if isinstance( headings, (str,) ) and ( ' ' in headings or '\t' in headings ):
             headings = tuple( headings.split( '\t' if '\t' in headings else None ) )
         self.headings = headings
 
@@ -2096,10 +2093,10 @@
                   + '.header.txt', '\n'.join( self.headings ) )
         self.isFirstRecord = True
 
-    type2name = { float: 'float', int: 'int', str: 'str', bool: 'bool', long: 'long' }
+    type2name = { float: 'float', int: 'int', str: 'str', bool: 'bool', int: 'long' }
     if haveNumpy: type2name.update( ( ( np.int64, 'long' ), (np.bool_, 'bool' ), ( np.float64, 'float' ) ) )
     typeOrder = ( bool, ) + ( ( np.bool_, ) if haveNumpy else () ) + \
-        ( int, long ) + ( ( np.int64, np.float64 ) if haveNumpy else () ) +  ( float, str )
+        ( int, int ) + ( ( np.int64, np.float64 ) if haveNumpy else () ) +  ( float, str )
 
     def __initColTypes(self, colTypes):
 
@@ -2245,7 +2242,7 @@
     seqStds = [ math.sqrt( sumSq.getSum() / n -
                            seqMean * seqMean )
                 for sumSq, seqMean, n in zip( sumSqs, seqMeans, ns ) ]
-    seqMeansStds = zip( seqMeans, seqStds )
+    seqMeansStds = list(zip( seqMeans, seqStds ))
 
     return seqMeansStds if numCols > 1 else seqMeansStds[ 0 ]
 
@@ -2289,7 +2286,7 @@
     seqStds = [ math.sqrt( sumSq.getSum() / n -
                            seqMean * seqMean )
                 for sumSq, seqMean, n in zip( sumSqs, seqMeans, ns ) ]
-    seqMeansStds = zip( seqMeans, seqStds, ns, ntots )
+    seqMeansStds = list(zip( seqMeans, seqStds, ns, ntots ))
 
     return seqMeansStds if numCols > 1 else seqMeansStds[ 0 ]
 
@@ -2305,7 +2302,7 @@
         if any arguments are IDotDatas, otherwise calls a numpy function of the same name.
         """
         
-        orig_fn = eval( 'np.' + f.func_name )
+        orig_fn = eval( 'np.' + f.__name__ )
 
         def new_f( *args, **kwargs ):
             return f( *args, **kwargs ) if any( IDotData.isA( a ) for a in args ) \
@@ -2313,7 +2310,7 @@
 
         new_f.__doc__ = orig_fn.__doc__
 
-        setattr( np, f.func_name, new_f )
+        setattr( np, f.__name__, new_f )
         
         return f
         
@@ -2360,10 +2357,10 @@
     def mean(a): return imean( a )
 
     @extendNumpy
-    def min(a): return __builtin__.min( a )
+    def min(a): return builtins.min( a )
 
     @extendNumpy
-    def max(a): return __builtin__.max( a )
+    def max(a): return builtins.max( a )
 
     numpy_abs = np.abs
     @extendNumpy
@@ -2390,7 +2387,7 @@
             else:
                 return numpy_atleast_1d( *arys  )
         else:
-            return map( atleast_1d, arys )
+            return list(map( atleast_1d, arys ))
 
     np.atleast_1d = atleast_1d
 
RefactoringTool: Refactored ./old/Operations/MiscUtil.py
--- ./old/Operations/MiscUtil.py	(original)
+++ ./old/Operations/MiscUtil.py	(refactored)
@@ -3,8 +3,9 @@
 For general utilities related to our datasets, see Operations.Ilya_Operations.DataUtil.py .
 """
 
-from __future__ import with_statement, division
+
 import platform
+from functools import reduce
 assert platform.python_version_tuple() >= ( 2, 6 )
 
 import sys, os, subprocess, operator, itertools, functools, inspect, logging, traceback, shutil, math, copy, types, \
@@ -41,12 +42,12 @@
 
 def dump_args(func, fname = None):
     "This decorator dumps out the arguments passed to a function before calling it"
-    argnames = func.func_code.co_varnames[:func.func_code.co_argcount]
-    if not fname: fname = func.func_name
+    argnames = func.__code__.co_varnames[:func.__code__.co_argcount]
+    if not fname: fname = func.__name__
     def echo_func(*args,**kwargs):
         callStr = fname + ":" + ', '.join(
             '%s=%r' % entry
-            for entry in zip(argnames,args) + kwargs.items())
+            for entry in list(zip(argnames,args)) + list(kwargs.items()))
         logging.info( 'calling ' + str(callStr) )
         retVal = func(*args, **kwargs)
         logging.info( 'from ' + str(callStr) + ' returned ' + str(retVal) )
@@ -187,7 +188,7 @@
 
     @functools.wraps( orig_func )
     def newFunc( *args, **kwargs ):
-        newPaths = map( MapPath, args[ :n_args ] )
+        newPaths = list(map( MapPath, args[ :n_args ] ))
         if str( newPaths ) != str( args[ :n_args ] ):
           logging.info( 'MiscUtil.remap: applying ' + orig_func.__name__ + ' to ' + ','.join( newPaths ) + ' instead of ' + ','.join( args[ :n_args ] ) )
         specialPaths = dict( stdin = sys.stdin, stdout = sys.stdout )
@@ -221,10 +222,10 @@
       try:
           import posix
 
-          map( functools.partial( remap, module = posix ),
+          list(map( functools.partial( remap, module = posix ),
                ( posix.access, posix.chdir, posix.chmod, posix.chown, posix.listdir, posix.link, posix.lstat,
                  posix.stat, posix.mknod, posix.mkdir, posix.pathconf, posix.remove, posix.readlink,
-                 posix.rmdir, posix.stat, posix.statvfs, posix.unlink, posix.utime ) )
+                 posix.rmdir, posix.stat, posix.statvfs, posix.unlink, posix.utime ) ))
           
           remap( posix.rename, module = posix, n_args = 2 )
           logging.info( 'REMAPPED os.rename' )
@@ -234,7 +235,7 @@
       try:
           import posixpath
 
-          map( remap, ( posixpath.walk, ) )
+          list(map( remap, ( posixpath.walk, ) ))
       except ImportError: pass
 
       global orig_open
@@ -242,11 +243,11 @@
 
       remap( open )
       
-      map( functools.partial( remap, module = os ),
+      list(map( functools.partial( remap, module = os ),
            ( os.access, os.chdir, os.chmod, os.chown, os.listdir, os.link, os.lstat,
              os.stat, os.mknod, os.mkdir, os.makedirs, os.open, os.pathconf, os.remove, os.readlink, os.removedirs,
              os.rmdir, os.stat, os.statvfs, os.unlink, os.utime, os.walk, os.path.walk,
-             os.path.exists ) )
+             os.path.exists ) ))
 
       remap( os.rename, module = os, n_args = 2 )
       logging.info( 'REMAPPED os.rename' )
@@ -474,15 +475,15 @@
     """
     if hasattr(func,'im_func'):
         # func is a method
-        func = func.im_func
-    code = func.func_code
+        func = func.__func__
+    code = func.__code__
     fname = code.co_name
     callargs = code.co_argcount
     # XXX Uses hard coded values taken from Include/compile.h
     args = list(code.co_varnames[:callargs])
-    if func.func_defaults:
-        i = len(args) - len(func.func_defaults)
-        for default in func.func_defaults:
+    if func.__defaults__:
+        i = len(args) - len(func.__defaults__)
+        for default in func.__defaults__:
             try:
                 r = repr(default)
             except:
@@ -613,7 +614,7 @@
 
     def __init__(self, *args, **kwargs):
         assert not args
-        for k, v in kwargs.items():
+        for k, v in list(kwargs.items()):
             setattr(self, k, v)
 
     def __getattr__(self,name):
@@ -658,7 +659,7 @@
 
     # Parse and validate the field names.  Validation serves two purposes,
     # generating informative error messages and preventing template injection attacks.
-    if isinstance(field_names, basestring):
+    if isinstance(field_names, str):
         field_names = field_names.replace(',', ' ').split() # names separated by whitespace and/or commas
     field_names = tuple(field_names)
     for name in (typename,) + field_names:
@@ -713,8 +714,8 @@
     # Execute the template string in a temporary namespace
     namespace = dict(itemgetter=_itemgetter)
     try:
-        exec template in namespace
-    except SyntaxError, e:
+        exec(template, namespace)
+    except SyntaxError as e:
         raise SyntaxError(e.message + ':\n' + template)
     result = namespace[typename]
 
@@ -750,7 +751,7 @@
     """
     return ( isinstance( val, ( collections.Sequence, types.GeneratorType ) ) or
              ( hasattr( val, '__getitem__' ) and hasattr( val, '__len__' ) ) )  \
-             and not isinstance( val, ( types.StringTypes, collections.Mapping, AtomicForIsSeq ) )
+             and not isinstance( val, ( (str,), collections.Mapping, AtomicForIsSeq ) )
 
 def MakeSeq( val ):
     """If val is a sequence (tuple or list) then return val, else return a new singleton tuple
@@ -759,7 +760,7 @@
 
 def IsScalar( val ):
     """Check if the value is a scalar value"""
-    return isinstance( val, ( numbers.Number, types.StringTypes ) )
+    return isinstance( val, ( numbers.Number, (str,) ) )
 
 def ToString( s ):
     """Convert s to string; handle numpy.charray correctly."""
@@ -786,30 +787,30 @@
 def MergeDicts( *dicts ):
     """Construct a merged dictionary from the given dicts.
     If two dicts define the same key, the key from the dict later in the list is chosen."""
-    return dict( reduce( operator.add, map( dict.items, dicts ), [] ) )
+    return dict( reduce( operator.add, list(map( dict.items, dicts )), [] ) )
 
 def RestrictDict( aDict, restrictSet ):
     """Return a dict which has the mappings from the original dict only for keys in the given set"""
     restrictSet = frozenset( restrictSet )
-    return dict( item for item in aDict.items() if item[0] in restrictSet )
+    return dict( item for item in list(aDict.items()) if item[0] in restrictSet )
 
 
 def DictExcept( aDict, exceptSet ):
     """Return a dict which has the mappings from the original dict except for keys in the given set"""
     if not isinstance( exceptSet, collections.Set ):
         exceptSet = frozenset( MakeSeq( exceptSet ) )
-    return dict( item for item in aDict.items() if item[0] not in exceptSet )
+    return dict( item for item in list(aDict.items()) if item[0] not in exceptSet )
 
 
 def RestrictDictValues( aDict, restrictSet ):
     """Return a dict which has the mappings from the original dict only for values in the given set"""
-    return dict( item for item in aDict.items() if item[1] in restrictSet )
+    return dict( item for item in list(aDict.items()) if item[1] in restrictSet )
 
 
 def InvertDict( aDict ):
     """Return an inverse mapping for the given dictionary"""
     assert len( set( aDict.values() ) )  == len( aDict )
-    return dict( ( v, k ) for k, v in aDict.items() )
+    return dict( ( v, k ) for k, v in list(aDict.items()) )
 
 def DictGet( d, k, dflt ):
     """Get a value from the dictionary; if not such value, return the specified default"""
@@ -847,17 +848,17 @@
     Taken from the "Python recipes" site.
     """
     # visualize an odometer, with "wheels" displaying "digits"...:
-    wheels = map(iter, sequences)
-    digits = [it.next( ) for it in wheels]
+    wheels = list(map(iter, sequences))
+    digits = [next(it) for it in wheels]
     while True:
         yield tuple(digits)
         for i in range(len(digits)-1, -1, -1):
             try:
-                digits[i] = wheels[i].next( )
+                digits[i] = next(wheels[i])
                 break
             except StopIteration:
                 wheels[i] = iter(sequences[i])
-                digits[i] = wheels[i].next( )
+                digits[i] = next(wheels[i])
         else:
             break
 
@@ -926,22 +927,22 @@
     """Check that the two values are equal, if not then print them and abort"""
 
     if x != y:
-        print 'x (', type(x), ') = ', x
-        print 'y (', type(y), ') = ', y
+        print('x (', type(x), ') = ', x)
+        print('y (', type(y), ') = ', y)
         assert x == y
     
 def PV( valName, val, printVal = None ):
     """Print value and return it"""
     if printVal != None:
-        print valName, ' = ', printVal, ' type=', type( printVal )
+        print(valName, ' = ', printVal, ' type=', type( printVal ))
     else:
-        print valName, ' = ', val, ' type=', type( val )
+        print(valName, ' = ', val, ' type=', type( val ))
     return val
     
 def ListIdx( lst ):
     """Build a map from list value to its index in the list.
     If the value appears more than once, the index of the last occurrence is used."""
-    return dict( map( reversed, enumerate( lst ) ) )
+    return dict( list(map( reversed, enumerate( lst ) )) )
 
 def iter_ith( it, item ):
   """Return i'ith item from iterator"""
@@ -958,22 +959,22 @@
     """
 
     if not os.path.exists(target):
-        raise OSError, 'Target does not exist: '+target
+        raise OSError('Target does not exist: '+target)
 
     if not os.path.isdir(base):
-        raise OSError, 'Base is not a directory or does not exist: '+base
+        raise OSError('Base is not a directory or does not exist: '+base)
 
     base_list = (os.path.abspath(base)).split(os.sep)
     target_list = (os.path.abspath(target)).split(os.sep)
 
     # On the windows platform the target may be on a completely different drive from the base.
-    if os.name in ['nt','dos','os2'] and base_list[0] <> target_list[0]:
-        raise OSError, 'Target is on a different drive to base. Target: '+target_list[0].upper()+', base: '+base_list[0].upper()
+    if os.name in ['nt','dos','os2'] and base_list[0] != target_list[0]:
+        raise OSError('Target is on a different drive to base. Target: '+target_list[0].upper()+', base: '+base_list[0].upper())
 
     # Starting from the filepath root, work out how much of the filepath is
     # shared by base and target.
     for i in range(min(len(base_list), len(target_list))):
-        if base_list[i] <> target_list[i]: break
+        if base_list[i] != target_list[i]: break
     else:
         # If we broke out of the loop, i is pointing to the first differing path elements.
         # If we didn't break out of the loop, i is pointing to identical path elements.
@@ -1060,7 +1061,7 @@
           assert self.EnumType is other.EnumType, "Only values from the same enum are comparable"
           return cmp(self.__value, other.__value)
        def __invert__(self):      return constants[maximum - self.__value]
-       def __nonzero__(self):     return bool(self.__value)
+       def __bool__(self):     return bool(self.__value)
        def __repr__(self):        return str(names[self.__value])
 
     maximum = len(names) - 1
@@ -1122,7 +1123,7 @@
     def underscorify( val ):
         """prepend undescrore if needed"""
         if not val and val != 0: return ''
-        noPrefix = isinstance( val, types.StringTypes) and val.startswith('#')
+        noPrefix = isinstance( val, (str,)) and val.startswith('#')
         alphaVal = MakeAlphaNum( str( val ) )
         return alphaVal[1:] if noPrefix else ( alphaVal if alphaVal.startswith( '_' ) else '_' + alphaVal )
     
@@ -1270,10 +1271,10 @@
 
     def getAllBinIds( self, minBinId = None, maxBinId = None ):
         """Return a sequence of all bin IDs, including empty bins."""
-        keys = self.bin2count.keys()
+        keys = list(self.bin2count.keys())
         if not keys: keys = ( 0, )
-        return range( min( keys ) if minBinId is None else minBinId ,
-                      max( keys )+1 if maxBinId is None else maxBinId, 1 )
+        return list(range( min( keys ) if minBinId is None else minBinId ,
+                      max( keys )+1 if maxBinId is None else maxBinId, 1))
     
     def getAllBinLefts( self, minBinId = None, maxBinId = None ):
         """Return the left boundaries of all bins"""
@@ -1316,7 +1317,7 @@
 
     def getBinCounts( self, normed = False, cumulative = False ):
         """Return the counts of all non-empty bins"""
-        return self.__fixBinCounts( binCounts = map( operator.itemgetter( 1 ), sorted( self.bin2count.items() ) ),
+        return self.__fixBinCounts( binCounts = list(map( operator.itemgetter( 1 ), sorted( self.bin2count.items() ) )),
                                     **Dict( 'cumulative normed' ) )
 
     def coarsenBy( self, factor ):
@@ -1326,7 +1327,7 @@
         """
 
         newHist = Histogrammer( binSize = self.binSize * factor, binShift = self.binShift )
-        for bin, count in self.bin2count.items():
+        for bin, count in list(self.bin2count.items()):
             newBin = int( math.floor( bin / factor ) )
             newHist.bin2count[ newBin ] += count
 
@@ -1350,7 +1351,7 @@
             assert abs( self.binSize - v.binSize ) < 1e-12
             assert abs( self.binShift - v.binShift ) < 1e-12
 
-            for bin, count in v.bin2count.items():
+            for bin, count in list(v.bin2count.items()):
                 self.bin2count[ bin ] += count
 
             self.valSum += v.valSum
@@ -1404,13 +1405,13 @@
                     with htag('tr'):
                         for heading in headings:
                             with htag('th'):
-                                print heading
+                                print(heading)
                 with htag('tbody'):
                     for r in binRecords:
                         with htag('tr'):
                             for val in map( str, r ):
                                 with htag('td'):
-                                    print val
+                                    print(val)
         else:    
 
             tabwrite( out, *headings )
@@ -1434,7 +1435,7 @@
             tabwrite( f, 'stat', 'val', 'encodedVal' )
             isFirst = True
             for stat, val in sorted( self.__dict__.items() ):
-                if isinstance( val, ( numbers.Number, types.StringTypes ) ):
+                if isinstance( val, ( numbers.Number, (str,) ) ):
                     if not isFirst: f.write( '\n' )
                     tabwriten( f, stat, val, base64.urlsafe_b64encode( pickle.dumps( val ) ) )
                     isFirst = False
@@ -1453,7 +1454,7 @@
                 attrs[ stat ] = pickle.loads( base64.urlsafe_b64decode( encodedVal ) ) 
 
         result = Histogrammer( binSize = attrs[ 'binSize' ] )
-        for attr, val in attrs.items():
+        for attr, val in list(attrs.items()):
             setattr( result, attr, val )
 
         with TableIter( fname ) as f:
@@ -1469,7 +1470,7 @@
     def printVals( self ):
         """Print the histogram"""
 
-        print 'binSize=', self.binSize, ' bin2count=', self.bin2count, ' avgVal=', self.getValAvg()
+        print('binSize=', self.binSize, ' bin2count=', self.bin2count, ' avgVal=', self.getValAvg())
 
     __hash__ = None
 
@@ -1491,12 +1492,12 @@
 @contextmanager
 def htag(x, **kwargs):
     """Write out an html tag"""
-    print '<%s' % x
-    for k, v in kwargs.items():
-        print ' %s="%s"' % ( k, v )
-    print '>'
+    print('<%s' % x)
+    for k, v in list(kwargs.items()):
+        print(' %s="%s"' % ( k, v ))
+    print('>')
     yield
-    if x != 'col': print '</%s>' % x
+    if x != 'col': print('</%s>' % x)
 
 def make_htag( out ):
     """Create an htag function for writing out HTML tags to a given outfile"""
@@ -1505,7 +1506,7 @@
     def htag(x, **kwargs):
         """Write out an html tag"""
         out.write( '<%s' % x )
-        for k, v in kwargs.items():
+        for k, v in list(kwargs.items()):
             out.write( ' %s="%s"' % ( k, v ) )
         out.write( '>' )
         yield
@@ -1516,9 +1517,9 @@
 @contextmanager
 def hparen(x = '()'):
     """Print open and close parentheses"""
-    print x[0]
+    print(x[0])
     yield
-    print x[1]
+    print(x[1])
     
     
 def chomp( s ):
@@ -1533,7 +1534,7 @@
     floats are converted to float.  Strings not interpretable as a Boolean, int or float are returned as-is,
     and no exception is generated.
     """
-    if not isinstance( s, types.StringTypes ): return s
+    if not isinstance( s, (str,) ): return s
     if s == 'True': return True
     if s == 'False': return False
     try: return int(s)
@@ -1559,7 +1560,7 @@
             if name.startswith( '__' ): return object.__getattr__( self, name )
             try: return coerceVal( self.lineVals[ self.colName2num[ name ] ] )
             except KeyError:
-                print 'lineVals=', self.lineVals, ' fname=', self.fname, ' headings=', self.headings
+                print('lineVals=', self.lineVals, ' fname=', self.fname, ' headings=', self.headings)
                 raise
                 
 
@@ -1575,10 +1576,10 @@
             return retVal
 
         def __iter__(self):
-            return itertools.imap( coerceVal, self.lineVals )
+            return map( coerceVal, self.lineVals )
 
         def __str__(self):
-            return str( zip( self.headings, self.lineVals ) )
+            return str( list(zip( self.headings, self.lineVals )) )
 
         def __repr__(self):
             return self.__str__()
@@ -1593,11 +1594,11 @@
         self.headings = tuple( headings )
 
         if not len( set( headings ) ) == len( headings ):
-            print 'headings=', headings
+            print('headings=', headings)
             seen = set()
             for h in headings:
                 if h in seen:
-                    print 'duplicate heading: ', h
+                    print('duplicate heading: ', h)
                 seen.add( h )
             
         assert len( set( headings ) ) == len( headings )
@@ -1616,7 +1617,7 @@
         """Returns an TblIter that gets only specified columns"""
 
         return IterReader( headings = tuple( self.headings[h] if isinstance(h,int) else h for h in MakeSeq(item) ),
-                           recordsIter = itertools.imap( operator.itemgetter(*MakeSeq(item)), self ) )
+                           recordsIter = map( operator.itemgetter(*MakeSeq(item)), self ) )
 
     def __getattr__(self,name):
         """Return a given column"""
@@ -1636,7 +1637,7 @@
         return IterReader( headings = [ h if h not in renamings else renamings[h] for h in self.headings ],
                            recordsIter = self )
     
-    def next(self):
+    def __next__(self):
         try:
             lineVals = self._nextLine()
             #print '***lineVals: ', lineVals
@@ -1645,14 +1646,14 @@
 
             if self.lineNum % 20000  ==  0:
                 logging.info( joinstr(' ', self.lineNum, ' of ', \
-                                  self.fname, ': ', zip( self.headings, lineVals ) ) )
+                                  self.fname, ': ', list(zip( self.headings, lineVals )) ) )
 
             if len( lineVals ) != len( self.headings ):
-                print 'lineVals=', lineVals, ' self.headings=', self.headings, ' \nzip=', zip( self.headings, lineVals )
+                print('lineVals=', lineVals, ' self.headings=', self.headings, ' \nzip=', list(zip( self.headings, lineVals )))
                 
             assert len( lineVals ) == len( self.headings ), 'lineVals=%d self.headings=%d: %s' % ( len( lineVals ), len( self.headings ), self.headings )
 
-            if self.valueFixer: lineVals = map( self.valueFixer, lineVals )
+            if self.valueFixer: lineVals = list(map( self.valueFixer, lineVals ))
 
             return TableReaderBase.TSVRecord( lineVals, self.colName2num, self.fname, self.headings )
         except StopIteration:
@@ -1687,16 +1688,16 @@
 
     def _nextLine(self):
         """Get next line"""
-        rawLine = chomp( self.freader.next() )
+        rawLine = chomp( next(self.freader) )
         if not rawLine: raise StopIteration
 
         # skip any comment lines.  normally these only appear at the beginning.
         while rawLine.startswith( '#' ):
-            rawLine = chomp( self.freader.next() )
+            rawLine = chomp( next(self.freader) )
             if not rawLine: raise StopIteration
 
         while self.skipFirstLines > 0:
-            rawLine = chomp( self.freader.next() )
+            rawLine = chomp( next(self.freader) )
             if not rawLine: raise StopIteration
             self.skipFirstLines -= 1
 
@@ -1727,7 +1728,7 @@
 
     def _nextLine(self):
         """Get next line"""
-        nextLine =  tuple( chomp( freader.next() ) for freader in self.freaders )
+        nextLine =  tuple( chomp( next(freader) ) for freader in self.freaders )
         if not nextLine: raise StopIteration
         return nextLine
 
@@ -1743,13 +1744,13 @@
     def __init__(self, headings, recordsIter):
         TableReaderBase.__init__(self, headings = headings)
 
-        self.recordsIter = itertools.ifilter( lambda x: x != None, iter( recordsIter ) )
+        self.recordsIter = filter( lambda x: x != None, iter( recordsIter ) )
         if len(headings)==1:
-            self.recordsIter = itertools.imap( lambda v: (v,) if isinstance( v, (int, float, str)) else v,
+            self.recordsIter = map( lambda v: (v,) if isinstance( v, (int, float, str)) else v,
                                                self.recordsIter )
 
 
-    def _nextLine(self): return self.recordsIter.next()
+    def _nextLine(self): return next(self.recordsIter)
         
 @contextmanager
 def TableIter( fname, sep = '\t', fileType = None ):
@@ -1777,12 +1778,12 @@
 
         cols = tuple( SlurpFileLines( headerFN ) )
 
-        tsvFiles = [ f for f in reduce( operator.concat, map( operator.itemgetter(2), os.walk( fname ) ) ) if f.endswith('.csv') ]
+        tsvFiles = [ f for f in reduce( operator.concat, list(map( operator.itemgetter(2), os.walk( fname ) )) ) if f.endswith('.csv') ]
 
         col2file = dict( ( os.path.splitext( os.path.splitext( tsvF )[0] )[0], tsvF ) for tsvF in tsvFiles )
         allFiles = [ fname + col2file[ col ] for col in cols ]
         
-        with nested( *map( open, allFiles ) ) as fs:
+        with nested( *list(map( open, allFiles )) ) as fs:
             yield DotDataReader( fname = fname, fs = fs, cols = cols, allFiles = allFiles )
             
 
@@ -1811,12 +1812,12 @@
 
         cols = tuple( SlurpFileLines( headerFN ) )
 
-        tsvFiles = [ f for f in reduce( operator.concat, map( operator.itemgetter(2), os.walk( fname ) ) ) if f.endswith('.csv') ]
+        tsvFiles = [ f for f in reduce( operator.concat, list(map( operator.itemgetter(2), os.walk( fname ) )) ) if f.endswith('.csv') ]
 
         col2file = dict( ( tsvF.split('.')[0], tsvF ) for tsvF in tsvFiles )
         allFiles = [ fname + col2file[ col ] for col in cols ]
         
-        return DotDataReader( fname = fname, fs = map( open, allFiles ), cols = cols, allFiles = allFiles, closeWhenDone = True,
+        return DotDataReader( fname = fname, fs = list(map( open, allFiles )), cols = cols, allFiles = allFiles, closeWhenDone = True,
                               valueFixer = valueFixer )
 
             
@@ -1841,7 +1842,7 @@
 
     recordsIter = iter( tableIter )
     
-    firstRec = recordsIter.next()
+    firstRec = next(recordsIter)
     colTypes = [ type(coerceVal(val)) for val in firstRec ]
 
     type2name = { float: 'float', int: 'int', str: 'str' }
@@ -1892,19 +1893,19 @@
             
 def makeIdxPrepender( idx ):
     """Creates a function that prepends a given index to its argument, returning a tuple."""
-    return lambda( v ): ( idx, v )
+    return lambda v: ( idx, v )
 
 class attrDbg(object):
 
     def __getattribute__(self, attr):
-        print 'getting attr ', attr
+        print('getting attr ', attr)
         return attr
     
 def makeKeyGetter( k ):
     """Creates a function that gets a key"""
     def myFunc( v ):
         return k( v[1] )
-    print 'making key getter for k=', k
+    print('making key getter for k=', k)
     return myFunc
 
 def makeKeyApplier( k ):
@@ -1949,9 +1950,9 @@
             if isinstance( k, str ): return operator.attrgetter( k )
             if isinstance( k, ( tuple, list ) ):
                 return functools.partial( lambda keys, vals: tuple( a_key( a_val ) for a_key, a_val in zip( keys, vals ) ),
-                                          map( fixKey, k ) )
-
-                return fixTuple( map( fixKey, k ) )
+                                          list(map( fixKey, k )) )
+
+                return fixTuple( list(map( fixKey, k )) )
 
             return k
         
@@ -1959,22 +1960,22 @@
                    ( operator.attrgetter( k ) if isinstance( k, str ) else k ) ) for k in keys ]
     
     if ids:
-        return itermerge( iters = [ itertools.imap( makeIdxPrepender( idx ), i ) for idx, i in
-                                    ( enumerate( iters ) if ids == True else zip( ids, iters ) ) ],
+        return itermerge( iters = [ map( makeIdxPrepender( idx ), i ) for idx, i in
+                                    ( enumerate( iters ) if ids == True else list(zip( ids, iters )) ) ],
                           keys = ( operator.itemgetter( 1 ), ) * len( iters ) if keys == None else
-                          map( makeKeyGetter, keys ),
+                          list(map( makeKeyGetter, keys )),
                           includeKeys = includeKeys )
     
     if keys:
-        print 'keys are: ', keys
-        result = itermerge( iters = [ itertools.imap( makeKeyApplier( k ), i )
+        print('keys are: ', keys)
+        result = itermerge( iters = [ map( makeKeyApplier( k ), i )
                                       for k, i in zip( keys, iters ) ] )
-        if not includeKeys: result = itertools.imap( operator.itemgetter( 1 ), result )
+        if not includeKeys: result = map( operator.itemgetter( 1 ), result )
         return result
     
     def merge( i1, i2 ):
-        next1 = iter( i1 ).next
-        next2 = iter( i2 ).next
+        next1 = iter( i1 ).__next__
+        next2 = iter( i2 ).__next__
         try:        
             v1 = next1()
         except StopIteration:
@@ -2020,8 +2021,8 @@
         self.gen = gen
 
     def __iter__(self): return self
-    def next(self):
-        v =  self.gen.next()
+    def __next__(self):
+        v =  next(self.gen)
         #print 'yiiiield ', v
         return v 
     
@@ -2031,7 +2032,7 @@
     def tmaker( *args, **kwargs):
 
         gen = fn( *args, **kwargs )
-        headings = gen.next()
+        headings = next(gen)
 
         logging.info( 'creating IterReader with headings %s' % headings )
         return IterReader( headings = headings,
@@ -2069,8 +2070,8 @@
 
             origins = [ r[1][0] for r in records ]
             if not is_sorted( origins, strict = True ):
-                print 'records are ', records
-                print 'origins are ', origins
+                print('records are ', records)
+                print('origins are ', origins)
             assert is_sorted( origins, strict = True )
 
             recordsList = [ None ] * len( tableIters )
@@ -2085,8 +2086,8 @@
                     positionFilled[ i ] = True
 
             if all( positionFilled ):
-                rec = map( tuple, recordsList )
-                assert map( len, rec ) == headingLens
+                rec = list(map( tuple, recordsList ))
+                assert list(map( len, rec )) == headingLens
                 if concat: rec = reduce( operator.concat, rec )
                 yield rec
 
@@ -2117,14 +2118,14 @@
 
 
 def mergeRecs( *vals ):
-    return reduce( operator.concat, map( tuple, vals ) )
+    return reduce( operator.concat, list(map( tuple, vals )) )
 
 
 def ihstack( *tableIters ):
     """Create a new table iterator that yields these tables stacked horizontally"""
 
-    return IterReader( headings = reduce( operator.concat, map( operator.attrgetter( 'headings' ), tableIters ) ),
-                       recordsIter = itertools.imap( mergeRecs, *tableIters ) )
+    return IterReader( headings = reduce( operator.concat, list(map( operator.attrgetter( 'headings' ), tableIters )) ),
+                       recordsIter = map( mergeRecs, *tableIters ) )
 
 
 def ivstack( *tableIters ):
@@ -2160,14 +2161,14 @@
     """Create a TblIter from DotData"""
 
     return IterReader( headings = dotData.dtype.names,
-                       recordsIter = dotData if dotData.numCols() > 1 else itertools.imap( (lambda x: (x,)), dotData ) )
+                       recordsIter = dotData if dotData.numCols() > 1 else map( (lambda x: (x,)), dotData ) )
 
 
 def TblIterFilter( pred, tableIter ):
     """Filter a TableIter with a predicate"""
 
     return IterReader( headings = tableIter.headings,
-                       recordsIter = itertools.ifilter( pred, tableIter ) )
+                       recordsIter = filter( pred, tableIter ) )
     
 
 def is_sorted( s, strict = True ):
@@ -2201,7 +2202,7 @@
                       'CGT': 'B',
                       'ACGT': 'N' }
 
-IUPAC_ambig_codes_set = dict( ( frozenset( k ), v ) for k, v in IUPAC_ambig_codes.items() )
+IUPAC_ambig_codes_set = dict( ( frozenset( k ), v ) for k, v in list(IUPAC_ambig_codes.items()) )
 
 def ambiguousIUPACdnaCode( codes ):
     """Given a set of DNA codes, return the one-letter IUPAC code representing them;
@@ -2226,7 +2227,7 @@
 
     argInfo = inspect.getargspec( fn )
     if not argInfo.defaults: return {}
-    return dict( zip( argInfo.args[ -len( argInfo.defaults ): ], argInfo.defaults ) )
+    return dict( list(zip( argInfo.args[ -len( argInfo.defaults ): ], argInfo.defaults )) )
 
 def IsValidFileName( fname ):
     """Return true if fname is a valid file name"""
@@ -2252,7 +2253,7 @@
         self.valNum = 0
         self.lastVal = None
 
-    def next(self):
+    def __next__(self):
         self.valNum += 1
         self.lastVal = next( self.it )
         return self.lastVal
@@ -2465,11 +2466,11 @@
 def PrintDictDiff( d1, d2 ):
     """Print the differences between dictionaries"""
 
-    d1only = [ k1 for k1 in d1.keys() if k1 not in d2 ]
-    d2only = [ k2 for k2 in d2.keys() if k2 not in d1 ]
-    diffVals = [ k for k in d1.keys() if k in d2.keys() and d1[k] != d2[k] ]
-
-    print 'd1only=', d1only, ' d2only=', d2only, ' diffVals=', diffVals
+    d1only = [ k1 for k1 in list(d1.keys()) if k1 not in d2 ]
+    d2only = [ k2 for k2 in list(d2.keys()) if k2 not in d1 ]
+    diffVals = [ k for k in list(d1.keys()) if k in list(d2.keys()) and d1[k] != d2[k] ]
+
+    print('d1only=', d1only, ' d2only=', d2only, ' diffVals=', diffVals)
     
         
 def GetHostName():
@@ -2498,8 +2499,7 @@
     """Parse list of numbers, possibly with ranges"""
 
     return reduce( operator.concat,
-                   map( lambda term: apply( range, map( int, term.split( '-' ) ) ) if '-' in term else [ int( term ) ],
-                        s.split( ',' ) ),
+                   [list(range(*list(map( int, term.split( '-' ) )))) if '-' in term else [ int( term ) ] for term in s.split( ',' )],
                    [] )
 
     
@@ -2509,7 +2509,7 @@
     
 def mapValues( d, f ):
   """Return a new dict, with each value mapped by the given function"""
-  return dict([ ( k, f( v ) ) for k, v in d.items() ]) 
+  return dict([ ( k, f( v ) ) for k, v in list(d.items()) ]) 
         
 
 class EventCounter(object):
@@ -2637,7 +2637,7 @@
                          headings = None, getio = None ):
   """Vertically stack tsv files"""
 
-  inFNs = filter( os.path.exists, inFNs )
+  inFNs = list(filter( os.path.exists, inFNs ))
 
   if getio: return dict( depends_on = inFNs, creates = outFN, attrs = dict( piperun_short = True )  )
 
@@ -2710,7 +2710,7 @@
 
     From http://code.activestate.com/recipes/303060-group-a-list-into-sequential-n-tuples/
     """
-    return itertools.izip(*[itertools.islice(lst, i, None, n) for i in range(n)])
+    return zip(*[itertools.islice(lst, i, None, n) for i in range(n)])
 
 def IsEven( x ):
   """Tests if x is even"""
@@ -2888,7 +2888,7 @@
                 fcntl.flock(self._fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                 # Lock acquired!
                 return
-             except IOError, ex:
+             except IOError as ex:
                 if ex.errno != errno.EAGAIN: # Resource temporarily unavailable
                    raise
                 elif self._timeout is not None and time.time() > (start_lock_search + self._timeout):
RefactoringTool: Refactored ./old/Operations/MiscUtil2.py
--- ./old/Operations/MiscUtil2.py	(original)
+++ ./old/Operations/MiscUtil2.py	(refactored)
@@ -50,8 +50,8 @@
         self.gen = gen
 
     def __iter__(self): return self
-    def next(self):
-        return self.gen.next()
+    def __next__(self):
+        return next(self.gen)
 
 
 
RefactoringTool: No changes to ./old/Operations/TypeInfer.py
RefactoringTool: No changes to ./old/Operations/bioutil.py
RefactoringTool: Refactored ./old/Operations/tsvutils.py
--- ./old/Operations/tsvutils.py	(original)
+++ ./old/Operations/tsvutils.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import with_statement, division
+
 
 __all__ = ( 'DefineRulesTo_normalizeColumnsWithinGroups', )
 
@@ -226,7 +226,7 @@
   result = IDotData( inFN ).sortedOn( *MakeSeq( keyCols ) )
   if reverse:
     d = result.toDotData()
-    result = d[ range( len( d )-1, -1, -1 ) ]
+    result = d[ list(range( len( d )-1, -1, -1)) ]
   result.save( outFN )
 
 
@@ -346,7 +346,7 @@
   if getio: return dict( depends_on = inFNs, creates = outFN,
                          uses = computeMeanStd_binned )
 
-  computeMeanStd_binned( inDatas = itertools.imap( lambda f: pd.read_table( f, usecols = ( valCol, binCol ) ).dropna(),
+  computeMeanStd_binned( inDatas = map( lambda f: pd.read_table( f, usecols = ( valCol, binCol ) ).dropna(),
                                                    MakeSeq( inFNs ) ),
                          **Dict( 'valCol binCol binMin binMax binStep' ) ).to_csv( outFN, sep = '\t',
                                                                                    index_label = 'binId',
RefactoringTool: Refactored ./old/Operations/DotDataFunctions/DictionaryOps.py
--- ./old/Operations/DotDataFunctions/DictionaryOps.py	(original)
+++ ./old/Operations/DotDataFunctions/DictionaryOps.py	(refactored)
@@ -1,11 +1,12 @@
 
 import operator
+from functools import reduce
 
 def MergeDicts( *dicts ):
     """Construct a merged dictionary from the given dicts.
     If two dicts define the same key, the key from the dict later in the list is chosen."""
-    return dict( reduce( operator.add, map( dict.items, dicts ) ) )
+    return dict( reduce( operator.add, list(map( dict.items, dicts )) ) )
 
 def RestrictDict( aDict, restrictSet ):
     """Return a dict which has the mappings from the original dict only for keys in the given set"""
-    return dict( [ ( k, aDict[k] ) for k in frozenset( restrictSet ) & frozenset( aDict.keys() ) ] )
+    return dict( [ ( k, aDict[k] ) for k in frozenset( restrictSet ) & frozenset( list(aDict.keys()) ) ] )
RefactoringTool: Refactored ./old/Operations/DotDataFunctions/DotDataListFromDirectory.py
--- ./old/Operations/DotDataFunctions/DotDataListFromDirectory.py	(original)
+++ ./old/Operations/DotDataFunctions/DotDataListFromDirectory.py	(refactored)
@@ -1,7 +1,7 @@
 from System.Utils import *
 from numpy import int64
 import logging
-import cPickle
+import pickle
 from Operations.MiscUtil import dbg, chomp
 
 def DotDataListFromDirectory(path, data_list = None, attribute_names = None, rootpath = '', rootheader = None, coloring = None, ToLoad = None, Nrecords = None):
@@ -32,14 +32,14 @@
 			expectedheader = rootpath.strip('/').split('/')[-1][:-5] + '.header.txt'
 			if len(H) == 1:
 				if H[0] != expectedheader:
-					print "Warning: the file, ", rootpath + H[0], " is being used to determine the order of the attribute names, even though ", rootpath + expectedheader, " was expected."
+					print("Warning: the file, ", rootpath + H[0], " is being used to determine the order of the attribute names, even though ", rootpath + expectedheader, " was expected.")
 				rootheader =  open_for_read(path + H[0])[0].read().strip('\n').split('\n')
 			elif len(H) > 1:
 				if expectedheader in H:
 					rootheader = open_for_read(path + H[0])[0].read().strip('\n').split('\n')
-					print "Warning: the file, ", rootpath + expectedheader, " is being used to determine the order of the attribute names. Multiple .header.txt files were provided."
+					print("Warning: the file, ", rootpath + expectedheader, " is being used to determine the order of the attribute names. Multiple .header.txt files were provided.")
 				else:
-					print "Warning: there are multiple .header.txt files in the directory ", rootpath, ", and so none will be used."
+					print("Warning: there are multiple .header.txt files in the directory ", rootpath, ", and so none will be used.")
 	if coloring == None:
 		coloring = {}	
 
@@ -83,10 +83,10 @@
 							data_list += [attribute_typed]
 							attribute_names += [attribute_name]		 		
 					except:
-						print "Warning: the data in the .csv file ", path + l if l != path else path, " does not match the given data type, ", parsed_filename[-2], ", and was not loaded"
+						print("Warning: the data in the .csv file ", path + l if l != path else path, " does not match the given data type, ", parsed_filename[-2], ", and was not loaded")
 						raise
 				else:
-					print "Warning: the column" + path + (l if l != path else path) + " has " + str(len(attribute_string)) + " records, which does not agree with the number of records in first column loaded, '" + attribute_names[0] + "', which has " + str(Nrecords) + " records -- only the first column loaded, as well as all the other columns which also have " + str(Nrecords) + " records, will be loaded."
+					print("Warning: the column" + path + (l if l != path else path) + " has " + str(len(attribute_string)) + " records, which does not agree with the number of records in first column loaded, '" + attribute_names[0] + "', which has " + str(Nrecords) + " records -- only the first column loaded, as well as all the other columns which also have " + str(Nrecords) + " records, will be loaded.")
 					raise
 							
 		elif parsed_filename[-1] == 'data' and IsDir(path):
@@ -99,7 +99,7 @@
 			
 	if len(CSVList) > 0:
 		for color in coloring_names:
-			coloring[color] = list(set(coloring[color] + CSVList if color in coloring.keys() else CSVList))
+			coloring[color] = list(set(coloring[color] + CSVList if color in list(coloring.keys()) else CSVList))
 
 	if path == rootpath and len(rootheader) > 1:
 	#Use header in top directory to order attributes and colorings
@@ -114,7 +114,7 @@
 	if  IsDir(path):
 		if '__rowdata__.pickle' in listdir(path):
 			try:
-				rowdata = cPickle.load(open(Backslash(path) + '__rowdata__.pickle','r'))
+				rowdata = pickle.load(open(Backslash(path) + '__rowdata__.pickle','r'))
 				if len(rowdata) != len(data_list[0]):
 					rowdata = None
 			except:
RefactoringTool: Refactored ./old/Operations/DotDataFunctions/DotDataListFromSV.py
--- ./old/Operations/DotDataFunctions/DotDataListFromSV.py	(original)
+++ ./old/Operations/DotDataFunctions/DotDataListFromSV.py	(refactored)
@@ -33,20 +33,20 @@
 
 	if Path[-4:] == '.csv':
 		if Delimiter == None:
-			print "Warning: no Delimiter argument specified, assuming ',' because of file extension."
+			print("Warning: no Delimiter argument specified, assuming ',' because of file extension.")
 			Delimiter = ','
 	elif Path[-4:] == '.tsv':
 		if Delimiter == None:
-			print "Warning: no Delimiter argument specified, assuming '\\t' because of file extension."
+			print("Warning: no Delimiter argument specified, assuming '\\t' because of file extension.")
 			Delimiter = '\t'
 
 	if Delimiter == None:
-		print "Warning: no Delimiter argument specified, defaulting to '\\t'."
+		print("Warning: no Delimiter argument specified, defaulting to '\\t'.")
 		Delimiter = '\t'
 	
 	if LineBreak == None or LineBreak == '\n':
 		if LineBreak == None:
-			print "Warning: no LineBreak argument specified, using '\\n'."
+			print("Warning: no LineBreak argument specified, using '\\n'.")
 			LineBreak = '\n'
 		F = open(Path,'rU').read().strip(LineBreak).split(LineBreak)
 		
@@ -57,12 +57,12 @@
 
 	F = F[SkipFirstLines:]
 
-	if LineFixer: F = map( LineFixer, F )
+	if LineFixer: F = list(map( LineFixer, F ))
 	if DelimiterRegExp:
-		if isinstance( DelimiterRegExp, types.StringType ):
+		if isinstance( DelimiterRegExp, bytes ):
 			import re
 			DelimiterRegExp = re.compile( DelimiterRegExp )
-		F = map( lambda line: DelimiterRegExp.sub( Delimiter, line ), F )
+		F = [DelimiterRegExp.sub( Delimiter, line ) for line in F]
 	
 	if SVHash != None:
 		headerlines = 0
@@ -72,7 +72,7 @@
 			else:
 				break
 		if Header:
-			print "Assuming that the last line in ", Path, " beginning with '", SVHash, "' has attribute names."
+			print("Assuming that the last line in ", Path, " beginning with '", SVHash, "' has attribute names.")
 			F = F[headerlines-1:]
 			F[0] = F[0][1:]
 		else:
@@ -91,7 +91,7 @@
 		#Extract records to a list of strings
 		records_list = F
 
-	print 'attribute_names=', attribute_names
+	print('attribute_names=', attribute_names)
 		
 	# Parse records into a list of lists
 	# records_parsed - list of lists of strings, where each inner list of strings is the parsing of one file line
@@ -105,7 +105,7 @@
 	if attribute_names:
 		for r in records_parsed:
 			if not len( r ) >= len( attribute_names ):
-				print 'r=', r
+				print('r=', r)
 			assert len( r ) >= len( attribute_names )
 	
 	# Type the columns	and save to a list of (attribute) columns
@@ -116,7 +116,7 @@
 		# Restrict the columns to the subset requested by the user
 		for TL in ToLoad:
 			if TL not in attribute_names:
-				print 'column ', TL, ' not in ', attribute_names
+				print('column ', TL, ' not in ', attribute_names)
 			
 		load_ind = [attribute_names.index(TL) for TL in ToLoad]
 		attribute_names = [ attribute_names[ i ] for i in load_ind ]
RefactoringTool: No changes to ./old/Operations/DotDataFunctions/SaveColumnsInDotData.py
RefactoringTool: Refactored ./old/Operations/DotDataFunctions/SaveDotData.py
--- ./old/Operations/DotDataFunctions/SaveDotData.py	(original)
+++ ./old/Operations/DotDataFunctions/SaveDotData.py	(refactored)
@@ -1,5 +1,5 @@
 from System.Utils import *
-import logging, cPickle
+import logging, pickle
 
 def SaveDotData(Data, TargetFolderName, HeaderOn = 1):
 
@@ -10,7 +10,7 @@
 		
 	MakeDir(TargetFolderName)
 
-	KeyList = Data.coloring.keys()
+	KeyList = list(Data.coloring.keys())
 
 	Nkeys = len(KeyList)		
 	pairwise = [[set(Data.coloring[key1]) > set(Data.coloring[key2]) for key1 in KeyList] for key2 in KeyList]
@@ -44,7 +44,7 @@
 		
 	if Data.rowdata != None:
 		G = open(TargetFolderName + '__rowdata__.pickle','w')
-		cPickle.dump(Data.rowdata,G)
+		pickle.dump(Data.rowdata,G)
 		G.close()
 
 	logging.info( 'Saved DotData (%d rows, %d cols) to %s' % ( Data.numRows(), Data.numCols(), TargetFolderName ) )
RefactoringTool: Refactored ./old/Operations/DotDataFunctions/SaveDotDataAsSV.py
--- ./old/Operations/DotDataFunctions/SaveDotDataAsSV.py	(original)
+++ ./old/Operations/DotDataFunctions/SaveDotDataAsSV.py	(refactored)
@@ -11,19 +11,19 @@
 		if sep == None:
 			sep = ','
 		elif sep != ',':
-			print "WARNING: You've specified a .csv ending for the file", TargetFileName , "but have given something other than a comma as the delimiter."
+			print("WARNING: You've specified a .csv ending for the file", TargetFileName , "but have given something other than a comma as the delimiter.")
 	elif TargetFileName[-4:] == '.tsv':
 		if sep == None:
 			sep = '\t'
 		elif sep != '\t':
-			print "WARNING: You've specified a .tsv ending for the file", TargetFileName , "but have given something other than a tab as the delimiter."
+			print("WARNING: You've specified a .tsv ending for the file", TargetFileName , "but have given something other than a tab as the delimiter.")
 
 	if sep == None:
 		#print 'NOTICE:  No delimiter specified, using tab.' 
 		sep = '\t'
 
 	if linesep == None:
-		print 'NOTICE:  No lineseparator specified, using \\n.' 
+		print('NOTICE:  No lineseparator specified, using \\n.') 
 		linesep = '\n'
 
 	Header = sep.join(Data.dtype.names)
@@ -39,7 +39,7 @@
 				D = D.flatten()	
 			if typevec[-1] == 'str':
 				if sum([sep in d for d in D]) > 0:
-					print "\nWARNING: An entry in the '" + attribute_name +"' column contains at least one instance of the delimiter '" + sep + "', and therefore will use Python csv module quoting convention (see online documentation for Python's csv module).  You may want to choose another delimiter not appearing in records, for performance reasons.\n"
+					print("\nWARNING: An entry in the '" + attribute_name +"' column contains at least one instance of the delimiter '" + sep + "', and therefore will use Python csv module quoting convention (see online documentation for Python's csv module).  You may want to choose another delimiter not appearing in records, for performance reasons.\n")
 					UseComplex = True
 					break
 				else:
@@ -52,7 +52,7 @@
 		if UseHeader: F.write(Header + linesep)
 		if UseComplex:
 			csv.writer(F,delimiter = sep, lineterminator = linesep).writerows(Data if len( Data.dtype ) > 1 else \
-												  itertools.imap( lambda x: (x,), Data ))
+												  map( lambda x: (x,), Data ))
 		else:
 			if len( D ) > 0:
 				F.write(linesep.join([sep.join([col[i] for col in ColStr]) for i in range(len(ColStr[0]))]))
RefactoringTool: No changes to ./old/Operations/DotDataFunctions/datahstack.py
RefactoringTool: Refactored ./old/Operations/DotDataFunctions/datavstack.py
--- ./old/Operations/DotDataFunctions/datavstack.py	(original)
+++ ./old/Operations/DotDataFunctions/datavstack.py	(refactored)
@@ -3,7 +3,7 @@
 '''
 import numpy
 from System.Utils import uniqify, ListUnion, SimpleStack1, ListArrayTranspose
-import Classes.DotData, cPickle
+import Classes.DotData, pickle
 
 def datavstack(ListOfDatas):
 #if all([isinstance(l,DotData) for l in ListOfDatas]):	
@@ -19,15 +19,15 @@
 		try:
 			return numpy.row_stack(ListOfDatas)
 		except:
-			print "The data arrays you tried to stack couldn't be stacked."
+			print("The data arrays you tried to stack couldn't be stacked.")
 	else:
 		
 		A = SimpleStack1([l[CommonAttributes] for l in ListOfDatas])
 			
 		if all(['coloring' in dir(l) for l in ListOfDatas]):
-			restrictedcoloring = dict([(a,list(set(ListOfDatas[0].coloring[a]).intersection(set(CommonAttributes)))) for a in ListOfDatas[0].coloring.keys()])
+			restrictedcoloring = dict([(a,list(set(ListOfDatas[0].coloring[a]).intersection(set(CommonAttributes)))) for a in list(ListOfDatas[0].coloring.keys())])
 			for l in ListOfDatas[1:]:
-				restrictedcoloring.update(dict([(a,list(set(l.coloring[a]).intersection(set(CommonAttributes)))) for a in l.coloring.keys()]))
+				restrictedcoloring.update(dict([(a,list(set(l.coloring[a]).intersection(set(CommonAttributes)))) for a in list(l.coloring.keys())]))
 		else:
 			restrictedcoloring = {}
 		if not all ([l.rowdata == None for l in ListOfDatas]):
RefactoringTool: Refactored ./old/Operations/DotDataFunctions/ondiskops.py
--- ./old/Operations/DotDataFunctions/ondiskops.py	(original)
+++ ./old/Operations/DotDataFunctions/ondiskops.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import with_statement
+
 
 from contextlib import contextmanager, nested
 import os, operator, itertools, glob
@@ -14,7 +14,7 @@
 
     if getio: return dict( depends_on = inFiles, creates = outFile )
 
-    IDotData.vstackFromIterable( itertools.imap( IDotData, inFiles ) ).save( outFile )
+    IDotData.vstackFromIterable( map( IDotData, inFiles ) ).save( outFile )
 
 def VStackTableFiles( inFiles, outFile, fileIdColumn = None, fileIdVals = None, getio = None ):
     """Vertically stack the specified DotData or tsv files"""
@@ -44,8 +44,8 @@
                     theHeader = header
                 else:
                     if header != theHeader:
-                        print 'theHeader=', theHeader
-                        print 'header=', header
+                        print('theHeader=', theHeader)
+                        print('header=', header)
                     assert header == theHeader
                     
                 if needNewline: outF.write( '\n' )
@@ -68,7 +68,7 @@
     if source.endswith('/'): source = source[:-1]
     if target.endswith('/'): target = target[:-1]
 
-    print 'linking ' + source + ' to ' + target
+    print('linking ' + source + ' to ' + target)
     SystemSucceed( 'mkdir -p ' + target )
     for f in os.listdir( source ):
         os.link( os.path.join( source, f ),
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/RBTree.py
--- ./old/Operations/Ilya_Operations/RBTree.py	(original)
+++ ./old/Operations/Ilya_Operations/RBTree.py	(refactored)
@@ -57,7 +57,7 @@
     def __str__(self):
         return repr(self.key) + ': ' + repr(self.value)
 
-    def __nonzero__(self):
+    def __bool__(self):
         return self.nonzero
 
     def __len__(self):
@@ -86,7 +86,7 @@
         """
         return self.node.value
 
-    def next (self):
+    def __next__ (self):
         """ Return the next item in the container
             Once we go off the list we stay off even if the list changes
         """
@@ -290,7 +290,7 @@
                 if self.unique == False: 
                     current.count += 1
                 else: # raise an Error
-                    print "Warning: This element is already in the list ... ignored!"
+                    print("Warning: This element is already in the list ... ignored!")
                     #SF I don't want to raise an error because I want to keep 
                     #SF the code compatible to previous versions
                     #SF But here would be the right place to do this
@@ -563,7 +563,7 @@
 
     def __str__ (self):
         # eval(str(self)) returns a regular list
-        return '['+ string.join(map(lambda x: str(x.value), self.nodes()), ', ')+']'
+        return '['+ string.join([str(x.value) for x in self.nodes()], ', ')+']'
 
     def findNodeByIndex (self, index):
         if (index < 0) or (index >= self.elements):
@@ -658,10 +658,10 @@
         self.elements = 0
 
     def values (self):
-        return map (lambda x: x.value, self.nodes())
+        return [x.value for x in self.nodes()]
 
     def reverseValues (self):
-        values = map (lambda x: x.value, self.nodes())
+        values = [x.value for x in self.nodes()]
         values.reverse()
         return values
 
@@ -670,12 +670,12 @@
 
     def __init__(self, dict={}, cmpfn=cmp):
         RBTree.__init__(self, cmpfn)
-        for key, value in dict.items():
+        for key, value in list(dict.items()):
             self[key]=value
 
     def __str__(self):
         # eval(str(self)) returns a regular dictionary
-        return '{'+ string.join(map(str, self.nodes()), ', ')+'}'
+        return '{'+ string.join(list(map(str, self.nodes())), ', ')+'}'
 
     def __repr__(self):
         return "<RBDict object " + str(self) + ">"
@@ -707,16 +707,16 @@
         return default
 
     def keys(self):
-        return map(lambda x: x.key, self.nodes())
+        return [x.key for x in self.nodes()]
 
     def values(self):
-        return map(lambda x: x.value, self.nodes())
+        return [x.value for x in self.nodes()]
 
     def items(self):
-        return map(tuple, self.nodes())
+        return list(map(tuple, self.nodes()))
 
     def has_key(self, key):
-        return self.findNode(key) <> None
+        return self.findNode(key) != None
 
     def clear(self):
         """delete all entries"""
@@ -742,11 +742,11 @@
         Will overwrite old entries with new ones.
 
         """
-        for key in other.keys():
+        for key in list(other.keys()):
             self[key] = other[key]
 
     def setdefault(self, key, value=None):
-        if self.has_key(key):
+        if key in self:
             return self[key]
         self[key] = value
         return value
@@ -757,13 +757,13 @@
 """
 def testRBlist():
     import random
-    print "--- Testing RBList ---"
-    print "    Basic tests..."
+    print("--- Testing RBList ---")
+    print("    Basic tests...")
 
     initList = [5,3,6,7,2,4,21,8,99,32,23]
     rbList = RBList (initList)
     initList.sort()
-    assert rbList.values() == initList
+    assert list(rbList.values()) == initList
     initList.reverse()
     assert rbList.reverseValues() == initList
     #
@@ -774,48 +774,48 @@
     # remove odd values
     for i in range (1,10,2):
         rbList.remove (i)
-    assert rbList.values() == [0,2,4,6,8]
+    assert list(rbList.values()) == [0,2,4,6,8]
 
     # pop tests
     assert rbList.pop() == 8
-    assert rbList.values() == [0,2,4,6]
+    assert list(rbList.values()) == [0,2,4,6]
     assert rbList.pop (1) == 2
-    assert rbList.values() == [0,4,6]
+    assert list(rbList.values()) == [0,4,6]
     assert rbList.pop (0) == 0
-    assert rbList.values() == [4,6]
+    assert list(rbList.values()) == [4,6]
 
     # Random number insertion test
     rbList = RBList()
     for i in range(5):
         k = random.randrange(10) + 1
         rbList.insert (k)
-    print "    Random contents:", rbList
+    print("    Random contents:", rbList)
 
     rbList.insert (0)
     rbList.insert (1)
     rbList.insert (10)
 
-    print "    With 0, 1 and 10:", rbList
+    print("    With 0, 1 and 10:", rbList)
     n = rbList.findNode (0)
-    print "    Forwards:",
+    print("    Forwards:", end=' ')
     while n is not None:
-        print "(" + str(n) + ")",
+        print("(" + str(n) + ")", end=' ')
         n = rbList.nextNode (n)
-    print
+    print()
 
     n = rbList.findNode (10)
-    print "    Backwards:",
+    print("    Backwards:", end=' ')
     while n is not None:
-        print "(" + str(n) + ")",
+        print("(" + str(n) + ")", end=' ')
         n = rbList.prevNode (n)
 
     if rbList.nodes() != rbList.nodesByTraversal():
-        print "node lists don't match"
-    print
+        print("node lists don't match")
+    print()
 
 def testRBdict():
     import random
-    print "--- Testing RBDict ---"
+    print("--- Testing RBDict ---")
 
     rbDict = RBDict()
     for i in range(10):
@@ -824,22 +824,22 @@
     rbDict[1] = 0
     rbDict[2] = "testing..."
 
-    print "    Value at 1", rbDict.get (1, "Default")
-    print "    Value at 2", rbDict.get (2, "Default")
-    print "    Value at 99", rbDict.get (99, "Default")
-    print "    Keys:", rbDict.keys()
-    print "    values:", rbDict.values()
-    print "    Items:", rbDict.items()
+    print("    Value at 1", rbDict.get (1, "Default"))
+    print("    Value at 2", rbDict.get (2, "Default"))
+    print("    Value at 99", rbDict.get (99, "Default"))
+    print("    Keys:", list(rbDict.keys()))
+    print("    values:", list(rbDict.values()))
+    print("    Items:", list(rbDict.items()))
 
     if rbDict.nodes() != rbDict.nodesByTraversal():
-        print "node lists don't match"
+        print("node lists don't match")
 
     # convert our RBDict to a dictionary-display,
     # evaluate it (creating a dictionary), and build a new RBDict
     # from it in reverse order.
     revDict = RBDict(eval(str(rbDict)),lambda x, y: cmp(y,x))
-    print "    " + str(revDict)
-    print
+    print("    " + str(revDict))
+    print()
 
 
 if __name__ == "__main__":
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/SnpStats.py
--- ./old/Operations/Ilya_Operations/SnpStats.py	(original)
+++ ./old/Operations/Ilya_Operations/SnpStats.py	(refactored)
@@ -10,7 +10,8 @@
 from Operations.Shari_Operations.localize.PopConsts import AllFreqs, AllPops, AllAges, CAUSAL_POS
 from Operations.IDotData import IDotData
 import operator, os, logging, contextlib, functools, collections, types, ast
-from itertools import izip
+from functools import reduce
+
 import itertools, string
 from UserDict import DictMixin
 import matplotlib
@@ -141,7 +142,7 @@
 
     snpTableFiles = [ os.path.join( Ddata,  'snpStats' + thinSfx, scenDir,
                                     AddFileSfx( snpTable + ( '.tsv' if '.' not in snpTable else '' ),
-                                                scenSfx if isinstance( scenSfx, types.StringTypes )
+                                                scenSfx if isinstance( scenSfx, (str,) )
                                                 else scenSfx[ os.path.splitext( snpTable )[0] ] ) )
                       for snpTable in snpTables ]
 
@@ -159,8 +160,8 @@
     
     replicaTableVals = [ DotData( SVPath = f ) for f in replicaTableFiles ]
 
-    replicasToUse = [ eval( replicaCondExpr, globals(), dict( zip( replicaTables, replicaTableRows ) ) )
-                      for replicaTableRows in izip( *replicaTableVals ) ]
+    replicasToUse = [ eval( replicaCondExpr, globals(), dict( list(zip( replicaTables, replicaTableRows )) ) )
+                      for replicaTableRows in zip( *replicaTableVals ) ]
 
     #dbg( 'sum(replicasToUse)' )
 
@@ -169,7 +170,7 @@
     histogramBuilder = Histogrammer( binSize = binSize, binShift = binShift )
 
     lastReplica = np.nan
-    for snpTableRows in izip( *snpTableVals ):
+    for snpTableRows in zip( *snpTableVals ):
         r0 = snpTableRows[ 0 ]
         assert all([ r.Chrom == r0.Chrom for r in snpTableRows ]) or all([ np.isnan( r.Chrom ) for r in snpTableRows ])
         assert all([ r.Pos == r0.Pos for r in snpTableRows ])
@@ -178,7 +179,7 @@
         useThisReplica = not replicaTables or replicasToUse[ replica ]
         if replica != lastReplica: dbg( 'replica useThisReplica histogramBuilder.getNumVals()' )
         if useThisReplica:
-            snpDict = dict( zip( snpTables, snpTableRows ) )
+            snpDict = dict( list(zip( snpTables, snpTableRows )) )
             if eval( snpCondExpr, globals(), snpDict ):
                 val = eval( snpStatExpr, globals(), snpDict )
                 histogramBuilder.addVal( val )
@@ -264,7 +265,7 @@
     if getio: return dict( depends_on = histFiles, creates = ( outFile, outFileStats ),
                            attrs = dict( piperun_short = True ) )
 
-    sumHist = reduce( operator.add, map( Histogrammer.load, histFiles ) )
+    sumHist = reduce( operator.add, list(map( Histogrammer.load, histFiles )) )
     sumHist.save( outFile )
     
 def GraphHistograms( histFiles, outFile = None, xlabel = '', ylabel = '', title = '',
@@ -326,9 +327,9 @@
         theLabels = []
         theHandles = []
 
-        hists = map( Histogrammer.load, histFiles )
+        hists = list(map( Histogrammer.load, histFiles ))
         if coarsenBy: hists = [ hist.coarsenBy( coarsenBy ) for hist in hists ]
-        allBinIds = reduce( operator.concat, [ hist.bin2count.keys() for hist in hists ] )
+        allBinIds = reduce( operator.concat, [ list(hist.bin2count.keys()) for hist in hists ] )
         if not allBinIds: allBinIds = ( 0, )
         minBinId = min( allBinIds )
         maxBinId = max( allBinIds ) + 1
@@ -705,13 +706,13 @@
     
     for replicaCond in map( operator.itemgetter( 1 ), replicaConds ):
         replicaCondExpr = compile_expr( replicaCond )
-        replicasToUse = [ int( eval( replicaCondExpr, globals(), dict( zip( replicaTables, replicaTableRows ) ) ) )
-                          for replicaTableRows in izip( *replicaTableVals ) ]
+        replicasToUse = [ int( eval( replicaCondExpr, globals(), dict( list(zip( replicaTables, replicaTableRows )) ) ) )
+                          for replicaTableRows in zip( *replicaTableVals ) ]
         matchingReplicas.append( replicasToUse )
 
     Records = []
     condNames = tuple( map( operator.itemgetter( 0 ), replicaConds ) )
-    for replicaNum, condResults in enumerate( izip( *matchingReplicas ) ):
+    for replicaNum, condResults in enumerate( zip( *matchingReplicas ) ):
         Records.append( ( replicaNum, ','.join( replicaCondName for condNum, replicaCondName
                                                 in enumerate( condNames )
                                                 if condResults[ condNum ]  ) )
@@ -749,10 +750,10 @@
     condsFile = IDotData( condsFileFN )
     inFile = IDotData( inFileFN )
 
-    with contextlib.nested( *map( functools.partial( IDotData.openForWrite, headings = inFile.headings ),
-                                  outFileFNs ) ) as outFiles:
-
-        for (replica, replicaRows), condValues in izip( inFile.groupby( replicaColName, multiPass = False ), condsFile ):
+    with contextlib.nested( *list(map( functools.partial( IDotData.openForWrite, headings = inFile.headings ),
+                                  outFileFNs )) ) as outFiles:
+
+        for (replica, replicaRows), condValues in zip( inFile.groupby( replicaColName, multiPass = False ), condsFile ):
 
             assert condValues.replicaNum == replica
 
@@ -780,7 +781,7 @@
     if getio: return dict( depends_on = [ condsFileFN ] + inFileFNs, creates = outFileFN,
                            mediumRuleNameSfx = scenario.scenDir() )
 
-    inFiles = map( IDotData, inFileFNs )
+    inFiles = list(map( IDotData, inFileFNs ))
     dbg( 'inFiles' )
 
     condsFile = IDotData( condsFileFN )
@@ -896,7 +897,7 @@
             scenSfx = DictGet( scen2sfxs, scen, '' )
             if scenSfx:
                 if IsSeq( scenSfx ): scenSfx = dict( scenSfx )
-                if not isinstance( scenSfx, types.StringTypes ): scenSfx = DictGet( dict( scenSfx ),
+                if not isinstance( scenSfx, (str,) ): scenSfx = DictGet( dict( scenSfx ),
                                                                                     os.path.splitext( table )[0], '' )
             
             tableFile = os.path.join( Ddata, whichStats+ thinSfx, scen.scenDir(),
@@ -907,7 +908,7 @@
             thisScenDict[ table ] = tableFile
         scen2table2file[ scen ] = thisScenDict
 
-    tableNames = map( operator.itemgetter( 0 ), map( os.path.splitext, tables ) )
+    tableNames = list(map( operator.itemgetter( 0 ), list(map( os.path.splitext, tables )) ))
     return tableNames, tables, ourScens, scen2table2file, depends_on
                 
 
@@ -938,7 +939,7 @@
 def FindTables( *exprs ):
     """Find tables referenced in specified expressions"""
 
-    return tuple( set( reduce( operator.concat, map( NameCollector.getNamesIn, exprs ) ) ) - set( ( 'True', 'False' ) ) )
+    return tuple( set( reduce( operator.concat, list(map( NameCollector.getNamesIn, exprs )) ) ) - set( ( 'True', 'False' ) ) )
     
 
 def findReplicasMatchingConds( Ddata, 
@@ -969,9 +970,9 @@
 
     replicaCondExpr = compile_expr( replicaCond )
     showVals = MakeSeq( showVals )
-    showValsExpr = map( compile_expr, showVals )
+    showValsExpr = list(map( compile_expr, showVals ))
     if not showHeadings:
-        showHeadings = map( MakeAlphaNum, showVals )
+        showHeadings = list(map( MakeAlphaNum, showVals ))
         showHeadings2 = []
         for h in showHeadings:
             h_new = h
@@ -997,19 +998,19 @@
             replicaTableVals = [ IDotData( thisScenDict[ replicaTable ] ) for replicaTable in replicaTables ]
 
             for replicaTableRows in \
-                    IDotData.TableIterInnerJoinAuxAsTuples( tableIters = map( iter, replicaTableVals ),
-                                                            cols = map( FindChromCol, replicaTableVals ),
+                    IDotData.TableIterInnerJoinAuxAsTuples( tableIters = list(map( iter, replicaTableVals )),
+                                                            cols = list(map( FindChromCol, replicaTableVals )),
                                                             blanks = ( None, ) * len( replicaTableVals ),
-                                                            headingLens = map( IDotData.rootClass.numCols,
-                                                                               replicaTableVals ) ):
-
-                vdict = dict( zip( replicaTableNames, replicaTableRows ) )
+                                                            headingLens = list(map( IDotData.rootClass.numCols,
+                                                                               replicaTableVals )) ):
+
+                vdict = dict( list(zip( replicaTableNames, replicaTableRows )) )
                 dbg( 'scen vdict' )
                     
                 evalHere = lambda expr: eval( expr, globals(), vdict )
                 if evalHere( replicaCondExpr ):
                     numReplicasAllowed += 1
-                    yield [ scen.scenName(), replicaTableRows[0].replicaNum ] + map( evalHere, showValsExpr )
+                    yield [ scen.scenName(), replicaTableRows[0].replicaNum ] + list(map( evalHere, showValsExpr ))
                 else:
                     numReplicasSkipped += 1
 
@@ -1052,8 +1053,8 @@
 
     snpCondExpr = compile_expr( snpCond )
     showVals = MakeSeq( showVals )
-    showValsExpr = map( compile_expr, showVals )
-    if not showHeadings: showHeadings = map( MakeAlphaNum, showVals )
+    showValsExpr = list(map( compile_expr, showVals ))
+    if not showHeadings: showHeadings = list(map( MakeAlphaNum, showVals ))
 
     numSnpsSkippedTot, numSnpsAllowedTot = 0, 0
 
@@ -1084,12 +1085,12 @@
             numSnpsSkippedTot, numSnpsAllowedTot = 0, 0
             
             for snpTableRows in \
-                    IDotData.TableIterInnerJoinAuxAsTuples( tableIters = map( iter, snpTableVals ),
-                                                            cols = zip( map( FindChromCol, snpTableVals ),
-                                                                        map( FindPosCol, snpTableVals ) ),
+                    IDotData.TableIterInnerJoinAuxAsTuples( tableIters = list(map( iter, snpTableVals )),
+                                                            cols = list(zip( list(map( FindChromCol, snpTableVals )),
+                                                                        list(map( FindPosCol, snpTableVals )) )),
                                                             blanks = ( None, ) * len( snpTableVals ),
-                                                            headingLens = map( IDotData.rootClass.numCols,
-                                                                               snpTableVals ) ):
+                                                            headingLens = list(map( IDotData.rootClass.numCols,
+                                                                               snpTableVals )) ):
 
                 thisReplica = snpTableRows[0][ replicaCol ]
                 if thisReplica != lastReplica:
@@ -1099,12 +1100,12 @@
                     lastReplica = thisReplica
                     
                 if thisReplicaResult:
-                    localDict = dict( zip( snpTableNames, snpTableRows ) )
+                    localDict = dict( list(zip( snpTableNames, snpTableRows )) )
                     evalHere = lambda expr: eval( expr, globals(), localDict )
                     evalResult = evalHere( snpCondExpr )
                     if evalResult:
                         v = [ scen.scenName(), thisReplica, snpTableRows[0][ posCol ] ] \
-                            + map( evalHere, showValsExpr )
+                            + list(map( evalHere, showValsExpr ))
                         numSnpsAllowed += 1
                         yield v
                     else: numSnpsSkipped += 1
@@ -1138,7 +1139,7 @@
 def DefineRulesTo_gatherCausalStat( pr, Ddata, scen2snpStatFN, posCol = 'Pos' ):
     """Define rules to gather a specified per-SNP statistic for the causal SNPs into a replica stat."""
 
-    for scenario, snpStatFN in scen2snpStatFN.items():
+    for scenario, snpStatFN in list(scen2snpStatFN.items()):
         pr.addInvokeRule( invokeFn = gatherCausalStat, invokeArgs = Dict( 'Ddata scenario snpStatFN posCol' ) )
         
         
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/sweep.py
--- ./old/Operations/Ilya_Operations/sweep.py	(original)
+++ ./old/Operations/Ilya_Operations/sweep.py	(refactored)
@@ -24,7 +24,7 @@
 
     moreOpts = ''
     if backgroundSweepDir: moreOpts += ' --background-sweep-dir ' + backgroundSweepDir
-    if backgroundPopMap: moreOpts += ' --background-pop-map ' + ','.join( k + ':' + v for k, v in backgroundPopMap.items() )
+    if backgroundPopMap: moreOpts += ' --background-pop-map ' + ','.join( k + ':' + v for k, v in list(backgroundPopMap.items()) )
     if backgroundPopOrder: moreOpts += ' --background-pop-order ' + ','.join( backgroundPopOrder )
 
     if tests: moreOpts += ' --tests ' + ','.join( tests )
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/CMS/fastcms.py
--- ./old/Operations/Ilya_Operations/CMS/fastcms.py	(original)
+++ ./old/Operations/Ilya_Operations/CMS/fastcms.py	(refactored)
@@ -1,6 +1,6 @@
 """Fast CMS computation"""
 
-from __future__ import print_function, absolute_import, division
+
 
 import os, logging
 #import blaze as bz
@@ -28,10 +28,10 @@
 def gatherXPOPscores( pops, chrom, selPop, sweepDir, outFN, getio = None ):
     """Gather xpop scores into a convenient form."""
 
-    pops = filter( lambda p: p != selPop, pops )
+    pops = [p for p in pops if p != selPop]
     pop2FN = dict([ ( pop, getFN_xpop_signif( pop1 = selPop, pop2 = pop, **Dict( 'sweepDir chrom' ) ) ) for pop in pops ])
 
-    if getio: return dict( depends_on = pop2FN.values(), creates = outFN, attrs = Dict( 'chrom', pop = pops, piperun_short = True ) )
+    if getio: return dict( depends_on = list(pop2FN.values()), creates = outFN, attrs = Dict( 'chrom', pop = pops, piperun_short = True ) )
 
     def LoadComparison( pop ):
         """Load comparison with one pop"""
@@ -60,7 +60,7 @@
     # end: def LoadComparison( pop )
 
     comparisons = reduce( lambda d1, d2: d1.join( d2, how = 'inner' ),
-                          map( LoadComparison, pops ) ).max( axis = 1,
+                          list(map( LoadComparison, pops )) ).max( axis = 1,
                                                              columns = ( 'max_xpop', ) )
     comparisons.index.name = 'pos'
     comparisons.name = 'max_xpop'
@@ -73,7 +73,7 @@
 def gather_snp_info( pops, pop2snpInfoFN, pop2ancFreqFN, pop2sampleSizeFN, getio = None ):
     """Gather SNP freq info"""
     
-    if getio: return dict( depends_on = pop2snpInfoFN.values(), creates = ( pop2ancFreqFN, pop2sampleSizeFN ),
+    if getio: return dict( depends_on = list(pop2snpInfoFN.values()), creates = ( pop2ancFreqFN, pop2sampleSizeFN ),
                            attrs = dict( pop = pops ) )
 
 
@@ -285,7 +285,7 @@
 
     pd.DataFrame( dict( std = ( stdKeeper.getStd(), ) ) ).to_csv( globalStatFN, sep = '\t', na_rep = 'NaN',
                                                                   header = True, index = False )
-    pd.DataFrame( dict( mean = map( StatKeeper.getMean, meanKeepers ) ) ).to_csv( binsStatFN,
+    pd.DataFrame( dict( mean = list(map( StatKeeper.getMean, meanKeepers )) ) ).to_csv( binsStatFN,
                                                                                   sep = '\t', na_rep = 'NaN',
                                                                                   header = True, index_label = 'binId' )
 
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/PipeRun/python/FunctionChecksums.py
--- ./old/Operations/Ilya_Operations/PipeRun/python/FunctionChecksums.py	(original)
+++ ./old/Operations/Ilya_Operations/PipeRun/python/FunctionChecksums.py	(refactored)
@@ -14,7 +14,7 @@
     # So we must recursively convert any such "constants" to strings, when constructing a string representation
     # of this code object.
     
-    constsStr = str( map( lambda x: x if not inspect.iscode( x ) else CodeContents( x ), code.co_consts ) )
+    constsStr = str( [x if not inspect.iscode( x ) else CodeContents( x ) for x in code.co_consts] )
     return code.co_code + constsStr + str( code.co_argcount ) + str( code.co_flags ) + str( code.co_nlocals ) + str( code.co_names ) + str( code.co_varnames ) 
     
 
@@ -23,11 +23,11 @@
     The first represents the code, and the second represents the default args.
     """
 
-    assert not func_obj or func_obj.func_code is code
+    assert not func_obj or func_obj.__code__ is code
 
     funcContents = CodeContents( code )
 
-    funcDefaults = ( str( func_obj.func_defaults ) + str( func_obj.func_dict ) ) if func_obj else ''
+    funcDefaults = ( str( func_obj.__defaults__ ) + str( func_obj.__dict__ ) ) if func_obj else ''
     funcContentsChecksum = base64.urlsafe_b64encode( hashlib.sha512( funcContents ).digest() )
     funcDefaultsChecksum = base64.urlsafe_b64encode( hashlib.sha512( funcDefaults ).digest() )
 
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/PipeRun/python/PipeRun.py
--- ./old/Operations/Ilya_Operations/PipeRun/python/PipeRun.py	(original)
+++ ./old/Operations/Ilya_Operations/PipeRun/python/PipeRun.py	(refactored)
@@ -29,14 +29,15 @@
     
 """
 
-from __future__ import with_statement, division
+
 import platform
+from functools import reduce
 assert platform.python_version_tuple() >= ( 2, 6 )
 from xml.dom.minidom import Document
 import os, sys, inspect, binascii, base64, zlib, pickle, types, logging, hashlib, string, tempfile, pprint, numpy, stat, \
     traceback, functools, itertools, contextlib, operator, socket, __main__
 from types import StringType
-from urlparse import urlparse
+from urllib.parse import urlparse
 from Operations.MiscUtil import SysTypeString, MakeAlphaNum, Str, ReserveTmpFileName, SystemSucceed, WaitForFileToAppear, \
     relpath, dbg, SlurpFile, Dict, MakeSeq, PV, MergeDicts, Sfx, DumpFile, RegisterTmpFile, AttrDict, GetDefaultArgs, \
     IsValidFileName, MAX_FILE_NAME_LEN, AddFileSfx, EnsureDirExists, DictGet
@@ -169,7 +170,7 @@
     def settingAttrs( self, *args, **kwargs ):
         """Add the specified attrs to all rules defined within this context"""
         attrs = args[0] if args else kwargs
-        if isinstance( attrs, types.StringTypes ):
+        if isinstance( attrs, (str,) ):
 
             caller = inspect.currentframe()
             thisFileName = caller.f_code.co_filename
@@ -239,12 +240,12 @@
         """
 
         if hasattr( invokeFn, 'im_class' ):
-            staticMethodClass = invokeFn.im_class
-            invokeFn = invokeFn.im_func
+            staticMethodClass = invokeFn.__self__.__class__
+            invokeFn = invokeFn.__func__
 
         # make sure we can invoke invokeFn using the doOp.py script,
         # by starting a fresh Python interpreter.
-        assert os.path.exists( invokeFn.func_code.co_filename )
+        assert os.path.exists( invokeFn.__code__.co_filename )
 
         # for compatibility with Dan and Elain's system, the 'creates' argument may be used
         # in place of the 'targets' argument, and 'depends_on' argument may be used in place
@@ -267,7 +268,7 @@
 
         # var: paramMap - map from function parameter to its value during the invocation; includes
         #   the default parameters.
-        paramMap = MergeDicts( dict( zip( args[ -len( defaults ): ], defaults ) if defaults else () ),
+        paramMap = MergeDicts( dict( list(zip( args[ -len( defaults ): ], defaults )) if defaults else () ),
                                invokeArgs )
 
         extraInfo = {}
@@ -277,7 +278,7 @@
             return Template( str( s ) ).substitute( paramMap )
 
         def fixParams( v, filterNone = True ):
-            if filterNone: v = filter( None, MakeSeq( v ) )
+            if filterNone: v = [_f for _f in MakeSeq( v ) if _f]
             return applyParams( v ) if not isinstance( v, ( tuple, list ) ) else tuple( map( applyParams, v ) )
 
         splitInfo = None
@@ -331,7 +332,7 @@
                 fileDescrs = dict( ( file, applyParams( descr ) if isinstance( descr, str ) else
                                      ( applyParams( descr[0] ),
                                        tuple( ( col, applyParams( colDescr ) ) for col, colDescr in descr[1] ) ) )
-                                     for file, descr in fileDescrs.items() )
+                                     for file, descr in list(fileDescrs.items()) )
 
             if 'fileTypes' in ruleElems:
                 fileTypes = MergeDicts( ruleElems[ 'fileTypes' ], fileDescrs )
@@ -371,7 +372,7 @@
         # for example, the user can configure to always extract a 'chr' argument passed to invokeFn
         # as a 'chrom' attribute of the created rule.
         if invokeArgs:
-            for arg, val in invokeArgs.iteritems():
+            for arg, val in invokeArgs.items():
                 if arg in self.arg2attr:
                     if attrs == None: attrs = { }
                     attrs[ self.arg2attr[ arg ] ] = val
@@ -381,7 +382,7 @@
         # then we can import invokeFn from that module inside doOp.py, and call it
         # with invokeArgs.            
 
-        relFilePath = relpath( invokeFn.func_code.co_filename )
+        relFilePath = relpath( invokeFn.__code__.co_filename )
 
         #relFilePath = relFilePath.replace( '../../../../selection/sweep2/nsvn/', '../' )
 
@@ -392,12 +393,12 @@
             ( inspect.getdoc( invokeFn ) if invokeFn.__doc__ else invokeFn.__name__ )
 
         defaultArgs = GetDefaultArgs( invokeFn )
-        if isinstance( ignoreArgsHere, types.StringTypes ): ignoreArgsHere = ignoreArgsHere.strip().split()
+        if isinstance( ignoreArgsHere, (str,) ): ignoreArgsHere = ignoreArgsHere.strip().split()
         ignoreArgsHere = set()
-        invokeArgsTrimmed = dict( ( arg, val ) for arg, val in invokeArgs.items()
+        invokeArgsTrimmed = dict( ( arg, val ) for arg, val in list(invokeArgs.items())
                                   if arg not in ignoreArgsHere and ( arg not in defaultArgs or defaultArgs[ arg ] != val ) )
 
-        func_name = ( '' if staticMethodClass is None else ( staticMethodClass.__name__ + '.' ) ) + invokeFn.func_name 
+        func_name = ( '' if staticMethodClass is None else ( staticMethodClass.__name__ + '.' ) ) + invokeFn.__name__ 
 
         def GetEncodedArgs( args, sfx ):
 
@@ -455,7 +456,7 @@
                     # as well as the arguments we are passing to it, in readable form.
                     commandsReadable = commandsReadable or \
                         func_name + '(' + ', '.join( [ arg + ' = ' + pprint.pformat( val ) \
-                                                                    for arg, val in invokeArgs.iteritems() ]  )+ ')',
+                                                                    for arg, val in invokeArgs.items() ]  )+ ')',
                     # the rule comment is normally taken from the Python doc string of invokeFn,
                     # but can be overridden with the comment argument to addInvokeRule().
                     # if there is neither a doc string nor a comment argument, then use the function name
@@ -467,7 +468,7 @@
                         + ( [ hashlib.sha512( ''.join( map( FunctionString, usedFnsOld ) ) ).hexdigest() ] if \
                                 invokeFnOld else [] ),
                     name = ruleName,
-                    targets = map( functools.partial( SplitFN, i = i ), targets ),
+                    targets = list(map( functools.partial( SplitFN, i = i ), targets )),
                     **Dict( 'sources binaries mediumRuleName mediumRuleNameSfx attrs fileDescrs fileTypes '
                             'paramMap saveOutputTo' ))
 
@@ -517,7 +518,7 @@
                           # as well as the arguments we are passing to it, in readable form.
                           commandsReadable = \
                               func_name + '(' + ', '.join( [ arg + ' = ' + pprint.pformat( val ) \
-                                                                          for arg, val in invokeArgs.iteritems() ]  )+ ')',
+                                                                          for arg, val in invokeArgs.items() ]  )+ ')',
                           # the rule comment is normally taken from the Python doc string of invokeFn,
                           # but can be overridden with the comment argument to addInvokeRule().
                           # if there is neither a doc string nor a comment argument, then use the function name
@@ -613,14 +614,14 @@
     class FileTuple(tuple):
 
         def containing( self, s ):
-            r = filter( lambda v: s in v, self )
+            r = [v for v in self if s in v]
             if not r: dbg( 'self s' )
             assert r
             return r[0] if len(r)==1 else r
 
 
         def not_containing( self, s ):
-            r = filter( lambda v: s not in v, self )
+            r = [v for v in self if s not in v]
             if not r: dbg( 'self s' )
             assert r
             return r[0] if len(r)==1 else r
@@ -667,7 +668,7 @@
         if creates and not targets: targets = creates
         if depends_on and not sources: sources = depends_on
 
-        assert saveOutputTo is None or isinstance( saveOutputTo, types.StringTypes )
+        assert saveOutputTo is None or isinstance( saveOutputTo, (str,) )
 
         if not targets and saveOutputTo: targets = saveOutputTo
         assert targets
@@ -676,7 +677,7 @@
 
         if saveOutputTo and saveOutputTo not in targets: targets += ( saveOutputTo, )
 
-        def IsString( s ): return isinstance( s, types.StringTypes )
+        def IsString( s ): return isinstance( s, (str,) )
         def ChkStrings( strs ):
             if not all( map( IsString, MakeSeq( strs ) ) ):
                 dbg( 'MakeSeq(strs) map(IsString,MakeSeq(strs))' )
@@ -719,11 +720,10 @@
 
         targets, sources, commands, commandsOld, commandsOld2, commandsOld3, commandsReadable, comment, mediumRuleName, \
             name = \
-            map( lambda s: s if isinstance( s, types.NoneType ) else \
+            [s if isinstance( s, type(None) ) else \
                      applyStr( s ) if isinstance( s, ( StringType, numpy.chararray ) ) else \
-                     map( applyStr, s ),
-                 ( targets, sources, commands, commandsOld, commandsOld2, commandsOld3, commandsReadable,
-                   comment, mediumRuleName, name ) )
+                     list(map( applyStr, s )) for s in ( targets, sources, commands, commandsOld, commandsOld2, commandsOld3, commandsReadable,
+                   comment, mediumRuleName, name )]
 
         #del applyStr
         #del caller  # to avoid circular references
@@ -770,7 +770,7 @@
 
             if attrs:
                 attrsElt = self.__addElt( "attrs", rule )
-                for a, v in attrs.iteritems():
+                for a, v in attrs.items():
                     vals = MakeSeq( v )
                     for val in vals:
                         self.__addElt( a, attrsElt, val )
@@ -786,7 +786,7 @@
                                                MakeSeq( usesTests ), rule )
 
             if fileDescrs:
-                for fileName, fileDescr in fileDescrs.items():
+                for fileName, fileDescr in list(fileDescrs.items()):
                     fileDescrElt = self.__addElt( "fileDescr", self.fileDescrsElt );
                     self.__addElt( "file", fileDescrElt, MakeSeq( targets )[ fileName ] if isinstance( fileName, int ) else fileName )
 
@@ -804,7 +804,7 @@
                                 self.__addElt( 'sameAsCol', thisColDescrElt, colDescr[1] )
 
             if fileTypes:
-                for fname, fileType in fileTypes.items():
+                for fname, fileType in list(fileTypes.items()):
                     for fileName in MakeSeq( fname ):
 
                         fileTypeElt = self.__addElt( "fileType", self.fileTypesElt );
@@ -844,7 +844,7 @@
         will find the right file.
         
         """
-        for fromStr, toStr in fileMapping.iteritems():
+        for fromStr, toStr in fileMapping.items():
             thisMapping = self.__addElt( "fileMapping", self.fileNameMap )
             self.__addElt( "from", thisMapping, fromStr )
             self.__addElt( "to", thisMapping, toStr )
@@ -1244,11 +1244,11 @@
     """
 
     # allow dependencies on a class; if anything in 
-    if isinstance( fn, (types.ClassType, types.ModuleType) ): return inspect.getsource( fn )
-    if isinstance( fn, types.MethodType ): fn = fn.im_func
+    if isinstance( fn, (type, types.ModuleType) ): return inspect.getsource( fn )
+    if isinstance( fn, types.MethodType ): fn = fn.__func__
 
     assert isinstance( fn, types.FunctionType )
-    code = fn.func_code
+    code = fn.__code__
     result = ''.join( FunctionChecksum( code, fn ) )
 
     # Because identifiers in Python are resolved dynamically at runtime,
RefactoringTool: No changes to ./old/Operations/Ilya_Operations/PipeRun/python/prun_par.py
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/PipeRun/python/resultPusher.py
--- ./old/Operations/Ilya_Operations/PipeRun/python/resultPusher.py	(original)
+++ ./old/Operations/Ilya_Operations/PipeRun/python/resultPusher.py	(refactored)
@@ -54,7 +54,7 @@
             logging.info( 'Pushing results in ' + queue + '...' )
 
             # find an unclaimed task in this queue, and try to claim it
-            try: taskDirs = filter( lambda f: f.startswith('mq'), os.listdir( queue ) )
+            try: taskDirs = [f for f in os.listdir( queue ) if f.startswith('mq')]
             except EnvironmentError as e:
                 logging.info( 'Error getting list of tasks in queue ' + queue + ': ' + str( e ) )
                 # sleep a bit -- maybe it's some transient condition that will resolve itself
@@ -91,7 +91,7 @@
                             logging.info( 'Could not read attrs for task in ' + fullTaskDir + ': ' + str( e ) )
                         
                         try:
-                            infoFNs = [ os.path.join( fullTaskDir, f ) for f in 'command.dat', 'attrs.tsv', 'claimed.dat' ]
+                            infoFNs = [ os.path.join( fullTaskDir, f ) for f in ('command.dat', 'attrs.tsv', 'claimed.dat') ]
                             infoContents = '\n'.join([ SlurpFile( f ) if os.path.exists( f ) else 'missing file: ' + f
                                                        for f in infoFNs ])
 
@@ -168,7 +168,7 @@
 
         
 if __name__ == '__main__':
-    print 'STARTING PUSHERS FROM MAIN'
+    print('STARTING PUSHERS FROM MAIN')
     StartPushers()
                     
                     
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/PipeRun/python/runner.py
--- ./old/Operations/Ilya_Operations/PipeRun/python/runner.py	(original)
+++ ./old/Operations/Ilya_Operations/PipeRun/python/runner.py	(refactored)
@@ -10,7 +10,7 @@
 
 """
 
-from __future__ import division, with_statement
+
 from Operations.MiscUtil import GetHostName, SlurpFile, DumpFile, dbg, SystemSucceed, coerceVal, EnsureDirExists
 import os, errno, time, getpass, multiprocessing, atexit, optparse, sys, random, stat, types, logging, socket, signal, traceback, re, shutil
 
@@ -33,9 +33,8 @@
 except ImportError:
     haveParamiko = False
 
-class FileSystem(object):
+class FileSystem(object, metaclass=ABCMeta):
     """A unified interface to local or remote file systems"""
-    __metaclass__ = ABCMeta
 
     def __init__(self):
         self.path = self  # to allow the usage of os.path routines
@@ -150,7 +149,7 @@
         self.hostname, self.root = hostAndPath.split( ':' )
         self.pw = pw
         if not pw and not pkey: pkey = '~/.ssh/identity'
-        if isinstance( pkey, types.StringTypes ):
+        if isinstance( pkey, (str,) ):
             pkey = pm.RSAKey.from_private_key_file( filename = os.path.expanduser( pkey ) )
         self.pkey = pkey
         self.transport = None
@@ -412,7 +411,7 @@
 
                         try:
                             fs.write( fd, 'locked by process %d on host %s\n' % ( os.getpid(), GetHostName() ) )
-                            for v in os.environ.keys():
+                            for v in list(os.environ.keys()):
                                 fs.write( fd, '%s=%s\n' % ( v, os.environ[v] ) )
                         finally:
                             fs.close( fd )
@@ -466,7 +465,7 @@
                                 SystemSucceed( 'rm -rf ' + srcLockFile )
 
                             targets = fs.SlurpFile( os.path.join( fs.root, fullTaskDir, 'targets.lst' ) ).rstrip( '\n' ).split('\n')
-                            targetDirs = set( map( os.path.dirname, filter( None, map( str.strip, targets ) ) ) )
+                            targetDirs = set( map( os.path.dirname, [_f for _f in map( str.strip, targets ) if _f] ) )
                             dbg( '"DDDDDD" targetDirs' )
 
                             for targetDir in targetDirs:
RefactoringTool: No changes to ./old/Operations/Ilya_Operations/localize/gatherCausalRanks.py
RefactoringTool: Refactored ./old/Operations/Ilya_Operations/sim/sfs/working/pardis2/RunSimulations.py
--- ./old/Operations/Ilya_Operations/sim/sfs/working/pardis2/RunSimulations.py	(original)
+++ ./old/Operations/Ilya_Operations/sim/sfs/working/pardis2/RunSimulations.py	(refactored)
@@ -1,5 +1,5 @@
 
-from __future__ import division, with_statement
+
 
 __all__ = ( 'DefineRulesTo_CreateSimulationParams', 'DefineRulesTo_runSims' )
 
@@ -11,6 +11,7 @@
 from operator import concat
 from shutil import copyfile
 import itertools, os
+from functools import reduce
 
 def CreateSimsParams_neutral( Ddata, suffix, inputParamsFiles, getio = None ):
 	"""Write the neutral parameter file.
@@ -22,7 +23,7 @@
 
 	if getio: return dict( depends_on = inputParamsFiles, creates = neutralParamsFile )
 
-	neutralParams = reduce( concat, map( SlurpFile, inputParamsFiles ) )
+	neutralParams = reduce( concat, list(map( SlurpFile, inputParamsFiles )) )
 	
 	DumpFile( neutralParamsFile, neutralParams )
 	
@@ -38,8 +39,8 @@
 				       suffix + '/%dky/params_sel%d_%d' % ( mutAge, mutFreq, mutPop ) )
 				     for mutAge in mutAges for mutPop in mutPops for mutFreq in mutFreqs )
 
-	if getio: return dict( depends_on = [ neutralParamsFile ] + scen2alternateNeutralParams.values(),
-			       creates = selectionParamsFiles.values() )
+	if getio: return dict( depends_on = [ neutralParamsFile ] + list(scen2alternateNeutralParams.values()),
+			       creates = list(selectionParamsFiles.values()) )
 
 	neutralParams = SlurpFile( neutralParamsFile )
 
@@ -90,7 +91,7 @@
 
 	dbg( '"YYYYYYYYYYYYY" inputParamsFiles' )
 	if inputParamsFiles == None:
-		inputParamsFiles = [ Ddata + '/simParams' + befAft + suffix + '.txt' for befAft in 'Bef', 'Aft' ]
+		inputParamsFiles = [ Ddata + '/simParams' + befAft + suffix + '.txt' for befAft in ('Bef', 'Aft') ]
 	dbg( '"ZZZZZZZZZZZZZ" inputParamsFiles' )
 
 	pr.addInvokeRule( invokeFn = CreateSimsParams_neutral, invokeArgs = Dict( 'Ddata suffix inputParamsFiles' ) )
@@ -128,7 +129,7 @@
 	for scen in GetScenarios( **Dict( 'mutAges mutPops mutFreqs includeNeutral' ) ):
 		if DdataSeeds: seeds = IDotData( os.path.join( DdataSeeds, 'replicastats', scen.scenDir(), 'simSeeds.tsv' ) )
 
-		for replicaNum, seedsLine in zip( range( nreplicas ), seeds if DdataSeeds else itertools.repeat( None, nreplicas ) ):
+		for replicaNum, seedsLine in zip( list(range( nreplicas)), seeds if DdataSeeds else itertools.repeat( None, nreplicas ) ):
 
 			assert not DdataSeeds or seedsLine.replicaNum == replicaNum
 			
@@ -165,8 +166,8 @@
 					    ( [ useGenMapFile, useMutRateFile ] if DdataMimic else [] ),
 				    commands = ' '.join(('perl ../Operations/Ilya_Operations/sim/sfs/working/pardis2/' \
 								 'runOneSim.pl' + ( ' --coalSeed %ld --recombSeed %ld --useMutRate %s'
-										    % ( long( seedsLine.coalescentSeed ),
-											long( seedsLine.recombSeed ),
+										    % ( int( seedsLine.coalescentSeed ),
+											int( seedsLine.recombSeed ),
 											seedsLine.GetStrItem( 'mutRate' ) )
 										    if DdataSeeds else '' )
 							 + ( ' --useGenMap ' + useGenMap if useGenMap else '' )
@@ -197,24 +198,24 @@
 
 	cfgDir = Ddata + '/config' + suffix
 
-	if getio: return dict( depends_on = configFiles.keys() + list( inputParamsFiles ),
-			       creates = [ cfgDir + '/' + f + powerSfx + '.txt' for f in 'scenarios', 'sims', 'pops'  ]
-			       + configFiles.values() )
-
-	neutralParams = reduce( concat, map( SlurpFile, inputParamsFiles ), '' )
+	if getio: return dict( depends_on = list(configFiles.keys()) + list( inputParamsFiles ),
+			       creates = [ cfgDir + '/' + f + powerSfx + '.txt' for f in ('scenarios', 'sims', 'pops')  ]
+			       + list(configFiles.values()) )
+
+	neutralParams = reduce( concat, list(map( SlurpFile, inputParamsFiles )), '' )
 
 	# Check that the list of pops defined in the param file matches the list of pops we are analyzing
 	assert sorted( [ int( s.split()[1] ) for s in neutralParams.split( '\n' ) if s.startswith( 'pop_define' ) ] ) \
 	    == sorted( mutPops )
 
-	assert set( map( int, pop2name.keys() ) ) == set( mutPops )
+	assert set( map( int, list(pop2name.keys()) ) ) == set( mutPops )
 
 	DumpFile( cfgDir + '/scenarios%s.txt' % powerSfx, '\n'.join( scen.scenDir()
 							      for scen in GetScenarios( mutAges, mutPops, mutFreqs ) ) )
 	DumpFile( cfgDir + '/sims%s.txt' % powerSfx, '%d\n%d' % (0, nreplicas-1) )
 	DumpFile( cfgDir + '/pops%s.txt' % powerSfx , '\n'.join( '%s\t%d' % ( popName, popNum )
-						   for popNum, popName in pop2name.items() ) )
-	for fromFile, toFile in configFiles.items():
+						   for popNum, popName in list(pop2name.items()) ) )
+	for fromFile, toFile in list(configFiles.items()):
 		copyfile( fromFile, toFile )
         
 #if __name__ == '__main__':
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/CMS.py
--- ./old/Operations/Shari_Operations/localize/CMS.py	(original)
+++ ./old/Operations/Shari_Operations/localize/CMS.py	(refactored)
@@ -3,7 +3,7 @@
 """
 
 
-from __future__ import division, with_statement
+
 
 __all__ = ( 'DefineRulesTo_computeCMSstats', 'DefineRulesTo_computeCMSforSims', 'DefineRulesTo_normalizeCMS', 'DefineRulesTo_createLikesTables' ) 
 
@@ -41,6 +41,7 @@
 from operator import concat, attrgetter, itemgetter, add
 import logging, os, types, itertools, sys, math, operator
 import matplotlib.pyplot as pp
+from functools import reduce
 
 
 # Bin boundaries for CMS statistics
@@ -146,13 +147,13 @@
                          'and the average of derived allele frequencies in the non-selected populations.' ) ) ) }
     
     if getio: return dict( depends_on = ( mergedData, ) +
-                           ( ( pop2sampleSize, ) if isinstance( pop2sampleSize, types.StringTypes ) else () ),
+                           ( ( pop2sampleSize, ) if isinstance( pop2sampleSize, (str,) ) else () ),
                            creates = cmsStatsRawFN,
                            mediumRuleNameSfx = ( scenario.scenDir(), putativeMutPop ),
                            fileDescrs = fileDescrs )
 
 
-    if isinstance( pop2sampleSize, types.StringTypes ): pop2sampleSize = dict( IDotData( pop2sampleSize ) )
+    if isinstance( pop2sampleSize, (str,) ): pop2sampleSize = dict( IDotData( pop2sampleSize ) )
 
     dbg( 'pop2sampleSize' )
     
@@ -231,10 +232,10 @@
     # Make new pos column
     Pos = Data['CHROM_POS %d' % minPopNum]
 
-    Chrom = map( float, Data.replicaNum if 'replicaNum' in Data.dtype.names
+    Chrom = list(map( float, Data.replicaNum if 'replicaNum' in Data.dtype.names
                  else where( isnan( Data.Chrom ),
                              Data[ 'Chrom ' + aPopPair ],
-                             Data.Chrom ) )
+                             Data.Chrom ) ))
     
     cmsStatsRaw = DotData( Columns = [Chrom, Pos, gdPos, ihs, StdDiff, meanFst, derFreq,
                                       max_xpop, mean_anc, freqDiff,iHHDiff, Data['FREQ1 %d' % putativeMutPop] ],
@@ -328,14 +329,14 @@
                          'and the average of derived allele frequencies in the non-selected populations.' ) ) ) }
     
     if getio: return dict( depends_on = ( mergedData, ihhDiff_sumsByFreqFN, ihhDiff_sumsFN ) +
-                           ( ( pop2sampleSize, ) if isinstance( pop2sampleSize, types.StringTypes ) else () ),
+                           ( ( pop2sampleSize, ) if isinstance( pop2sampleSize, (str,) ) else () ),
                            creates = cmsStatsRawFN,
                            splitByCols = { mergedData: dict( keyCols = () ) },
                            mediumRuleNameSfx = ( scenario.scenDir(), putativeMutPop ),
                            fileDescrs = fileDescrs )
 
 
-    if isinstance( pop2sampleSize, types.StringTypes ): pop2sampleSize = dict( IDotData( pop2sampleSize ) )
+    if isinstance( pop2sampleSize, (str,) ): pop2sampleSize = dict( IDotData( pop2sampleSize ) )
 
     dbg( 'pop2sampleSize' )
     
@@ -718,7 +719,7 @@
     for causal and non-causal SNPs.
     """
 
-    print 'deprecated!'
+    print('deprecated!')
     sys.exit(1)
 
     snpStatsDir = os.path.join( Ddata, 'snpStats' + thinSfx, scenario.scenDir() )
@@ -781,7 +782,7 @@
 
     cols = dtype.names
     DotData( names = cols,
-             Columns = [ reduce( add, map( itemgetter( col ), hists ) ) for col in cols ] ).save( outFile )
+             Columns = [ reduce( add, list(map( itemgetter( col ), hists )) ) for col in cols ] ).save( outFile )
 
 
 def LoadBins( fname ):    
@@ -814,7 +815,7 @@
 def LoadLikesTable( likesTable, likesTableSfx = '', mutPop = None, Ddata = None  ):
     """Load a likes table, returning a dictionary that maps likes table
     attributes to values.  For example, gives the name of hitLikes and missLikes files."""
-    if isinstance( likesTable, types.StringTypes ):
+    if isinstance( likesTable, (str,) ):
         if Ddata and not os.path.dirname( likesTable ): likesTable = os.path.join( Ddata, likesTable )
         dbg( 'likesTable likesTableSfx' )
         likesTable = IDotData( AddFileSfx( likesTable, likesTableSfx, mutPop ) )
@@ -848,7 +849,7 @@
 
     #stats = ( 'max_xpop', )
     
-    print 'in computeCMS: likesBins is ', likesBins, 'likesSfx is ', likesSfx, ' likesTable is ', likesTable
+    print('in computeCMS: likesBins is ', likesBins, 'likesSfx is ', likesSfx, ' likesTable is ', likesTable)
 
     if likesTable:
         likesTable = LoadLikesTable( **Dict( 'Ddata likesTable likesTableSfx' ) )
@@ -1017,7 +1018,7 @@
 
     #stats = ( 'max_xpop', )
     
-    print 'in computeCMS: likesBins is ', likesBins, 'likesSfx is ', likesSfx, ' likesTable is ', likesTable
+    print('in computeCMS: likesBins is ', likesBins, 'likesSfx is ', likesSfx, ' likesTable is ', likesTable)
 
     if likesTable:
         likesTable = LoadLikesTable( **Dict( 'Ddata likesTable likesTableSfx' ) )
@@ -1121,7 +1122,7 @@
                                 tuple([ stat + 'likeRatio' for stat in stats ]) + tuple([ stat + 'likeBin' for stat in stats ]) ) \
                                 as complikeOut:
     
-        for rowNum, ( rRaw, rNorm ) in enumerate( itertools.izip( cmsStatsRaw, cmsStatsNorm ) ):
+        for rowNum, ( rRaw, rNorm ) in enumerate( zip( cmsStatsRaw, cmsStatsNorm ) ):
 
             if rRaw.Chrom >= nreplicas: break 
 
@@ -1183,7 +1184,7 @@
 
     replica2choice = {}
     
-    for r in findReplicasMatchingConds( allScens = ( scenario, ), showVals = map( itemgetter( 0 ), replicaCond2choice ),
+    for r in findReplicasMatchingConds( allScens = ( scenario, ), showVals = list(map( itemgetter( 0 ), replicaCond2choice )),
                                         **Dict( 'Ddata replicaTables' ) ):
         gotChoice = False
         for replicaNum, choice in enumerate( r[2:] ):
@@ -1194,10 +1195,10 @@
         assert gotChoice
 
     dbg( 'replica2choice' )
-    iDotDatas = map( IDotData, complikeFNs_in ) 
+    iDotDatas = list(map( IDotData, complikeFNs_in )) 
     def makeResult():
         yield iDotDatas[0].headings
-        for r in itertools.izip( *iDotDatas ):
+        for r in zip( *iDotDatas ):
             yield r[ replica2choice[ r[0].Chrom ] ]
 
     IDotData.fromFn( makeResult ).save( complikeFN )
@@ -1236,7 +1237,7 @@
 
     #stats = ( 'max_xpop', )
     
-    print 'in computeCMS: likesBins is ', likesBins, 'likesSfx is ', likesSfx, ' likesTable is ', likesTable
+    print('in computeCMS: likesBins is ', likesBins, 'likesSfx is ', likesSfx, ' likesTable is ', likesTable)
 
     if not likesTableNeut: likesTableNeut = likesTable
 
@@ -1434,7 +1435,7 @@
         return dict( depends_on = ( complikeFN, gdMapFN ) +
                      ( ( causalGdPosFN, ) if not scenario.isNeutral() else () ) +
                      ( ( intervalsListFN, ) if includeSpatialLoc else () ),
-                     creates = tuple( filter( None, plotFilesFNs ) ),
+                     creates = tuple( [_f for _f in plotFilesFNs if _f] ),
                      name = 'plotCMS' + Sfx( xAxis, whichSpatialLoc if includeSpatialLoc else None ),
                      mediumRuleNameSfx = ( scenario.scenDir(), putativeMutPop if scenario.isNeutral() else None,
                                            xAxis ) )
@@ -1443,7 +1444,7 @@
     
     usedReplicas = zeros( nreplicas, dtype = bool )
     for ( replicaNum, cmsForReplica ), plotFileFN, ( replicaNum2, intervals ) in \
-            itertools.izip( IDotData( complikeFN ).groupby( 'Chrom' ),
+            zip( IDotData( complikeFN ).groupby( 'Chrom' ),
                             plotFilesFNs,
                             IDotData( intervalsListFN ).groupby( 'replicaNum' ) if includeSpatialLoc \
                                 else itertools.repeat( ( None, None ) ) ):
@@ -1479,7 +1480,7 @@
             
             foundLike = 0
             margin = ( maxCMS - minCMS ) / 100
-            for pos, absLikeHere, likeHere in itertools.izip( cmsForReplica[ xCol ], X.complikeExp, like ):
+            for pos, absLikeHere, likeHere in zip( cmsForReplica[ xCol ], X.complikeExp, like ):
                 if likeHere > .2:
                     pp.vlines( [ pos ], absLikeHere - margin, absLikeHere + margin, color = 'g',
                                linewidth = 3.0 ).set_urls( [ 'like:_%.2f' % likeHere ] )
@@ -1573,7 +1574,7 @@
         usedReplicas[ replicaNum ] = True
 
     numMissingReplicas = 0
-    for replicaNum, plotFileFN in zip( range( nreplicas ), plotFilesFNs ):
+    for replicaNum, plotFileFN in zip( list(range( nreplicas)), plotFilesFNs ):
         if plotFileFN and not usedReplicas[ replicaNum ]:
             pp.figure()
             pp.xlabel( xLabel )
@@ -1612,7 +1613,7 @@
        xAxisGd - use genetic distance for the X axis if True, physical distance if False.
     """
 
-    if isinstance( scenario, types.StringTypes ): scenario = Scenario.fromString( scenario )
+    if isinstance( scenario, (str,) ): scenario = Scenario.fromString( scenario )
     snpStatsDir = os.path.join( Ddata, 'snpStats'+ thinSfx, scenario.scenDir() )
     replicaStatsDir = os.path.join( Ddata, 'replicastats'+ thinSfx, scenario.scenDir() )
     if putativeMutPop == None: putativeMutPop = scenario.mutPop
@@ -1661,7 +1662,7 @@
 
     pp.figure( figsize = figSize )
     for ( replicaNum, cmsForReplica ), ( replicaNum2, intervals ) in \
-            itertools.izip( IDotData( complikeFN ).groupby( 'Chrom' ),
+            zip( IDotData( complikeFN ).groupby( 'Chrom' ),
                             IDotData( intervalsListFN ).groupby( 'replicaNum' ) if includeSpatialLoc \
                                 else itertools.repeat( ( None, None ) ) ):
         replicaNum = int( replicaNum )
@@ -1698,7 +1699,7 @@
             
             foundLike = 0
             margin = ( maxCMS - minCMS ) / 100
-            for pos, absLikeHere, likeHere in itertools.izip( cmsForReplica[ xCol ], X[ snpStat ], like ):
+            for pos, absLikeHere, likeHere in zip( cmsForReplica[ xCol ], X[ snpStat ], like ):
                 if likeHere > .2:
                     pp.vlines( [ pos ], absLikeHere - margin, absLikeHere + margin, color = 'g',
                                linewidth = 3.0 ).set_urls( [ 'like:_%.2f' % likeHere ] )
@@ -1857,14 +1858,14 @@
     usedReplicas = zeros( nreplicas, dtype = bool )
     for ( replicaNum, cmsForReplica ), plotFileFN, ( replicaNum2, intervals ), \
             ( replicaNum3, posteriorSpline ), ( replicaNum4, binInfo ), intervalsStats in \
-            itertools.izip( IDotData( complikeFN ).groupby( 'Chrom' ),
+            zip( IDotData( complikeFN ).groupby( 'Chrom' ),
                             plotFilesFNs,
                             IDotData( intervalsListFN ).groupby( 'replicaNum' ),
                             IDotData( posteriorSplineFN ).groupby( 'replicaNum' ),
                             IDotData( binInfoFN ).groupby( 'replicaNum' ),
                             IDotData( intervalsStatsFN ) ):
         replicaNum, replicaNum2, replicaNum3, replicaNum4 = \
-            map( int, ( replicaNum, replicaNum2, replicaNum3, replicaNum4 ) )
+            list(map( int, ( replicaNum, replicaNum2, replicaNum3, replicaNum4 ) ))
 
         assert replicaNum2 == replicaNum == replicaNum3 == replicaNum4 == intervalsStats.replicaNum
             
@@ -2040,7 +2041,7 @@
             z = DotData( names = r.headings, Records = r )
             z = clean_hapmap2_region( z, cleanIHS = True, cleanDer = False, cleanAnc = False, keepCausal = True )
             
-            yield IDotData.fromIterable( headings = z.dtype.names, iterable = itertools.imap( tuple, z ) )
+            yield IDotData.fromIterable( headings = z.dtype.names, iterable = map( tuple, z ) )
 
     IDotData.vstackFromIterable( GetNormedBlocks() ).save( complikeWithLogCLRFN )
 
@@ -2321,7 +2322,7 @@
                 # now has CMS scores for all the replicas.
                 #
                 
-                condNames = map( itemgetter(0), replicaConds )
+                condNames = list(map( itemgetter(0), replicaConds ))
                 condsFileFN = AddFileSfx( 'replicaConds.tsv', complikeSfx, *condNames )
                 pr.addInvokeRule( invokeFn = identifyReplicasMeetingConds,
                                   invokeArgs = 
@@ -2784,7 +2785,7 @@
         DefineRulesTo_computeCMSstats( **Dict( 'pr Ddata simsOut mutPops mutAges mutFreqs nreplicas thinExt thinSfx '
                                                'sampleSize pop2sampleSize pop2name statsSfx' ) )
 
-    condNames = map( itemgetter(0), replicaConds )
+    condNames = list(map( itemgetter(0), replicaConds ))
     
     if normalizeWithinReplicas:
         stat_start = CMSBinsLocal.stat_start
@@ -2905,11 +2906,11 @@
 
         assert all([ len( rtv ) == nreplicas for rtv in replicaTableVals ])
 
-        replicasToUse = array( [ eval( replicaCondExpr, globals(), dict( zip( replicaTables, replicaTableRows ) ) )
-                                 for replicaTableRows in itertools.izip( *replicaTableVals ) ] ) if replicaTables \
+        replicasToUse = array( [ eval( replicaCondExpr, globals(), dict( list(zip( replicaTables, replicaTableRows )) ) )
+                                 for replicaTableRows in zip( *replicaTableVals ) ] ) if replicaTables \
                                  else repeat( eval( replicaCondExpr, globals(), {} ), nreplicas )
                                  
-        rowsToUse = replicasToUse[ array( map( int, cmsStatsNorm.Chrom ) ) ]
+        rowsToUse = replicasToUse[ array( list(map( int, cmsStatsNorm.Chrom )) ) ]
         cmsStatsNorm = cmsStatsNorm[ rowsToUse ]
         cmsStatsRaw = cmsStatsRaw[ rowsToUse ]
 
@@ -2932,7 +2933,7 @@
     with open(summaryFN, 'w') as  outfile:
         outfile.write('N causal: 0 N missed: 0\n')  # dummy line
         tabwrite( outfile, 'Stat', 'Start', 'End', 'N_bin')
-        for stat in stat_nbin.keys():
+        for stat in list(stat_nbin.keys()):
             tabwrite( outfile, stat, stat_start[stat], stat_end[stat], stat_nbin[stat] )
 
 
@@ -2998,8 +2999,8 @@
 
         assert all([ len( rtv ) == nreplicas for rtv in replicaTableVals ])
 
-        replicasToUse = array( [ eval( replicaCondExpr, globals(), dict( zip( replicaTables, replicaTableRows ) ) )
-                                 for replicaTableRows in itertools.izip( *replicaTableVals ) ] ) if replicaTables \
+        replicasToUse = array( [ eval( replicaCondExpr, globals(), dict( list(zip( replicaTables, replicaTableRows )) ) )
+                                 for replicaTableRows in zip( *replicaTableVals ) ] ) if replicaTables \
                                  else repeat( eval( replicaCondExpr, globals(), {} ), nreplicas )
 
     hitmissCols = {}
@@ -3023,7 +3024,7 @@
     hmColsHits = [ hitmissCols[ stat + 'Hits' ] for stat in stats ]
     hmColsMisses = [ hitmissCols[ stat + 'Misses' ] for stat in stats ]
             
-    for rowNum, r in enumerate( itertools.izip( cmsStatsRaw, cmsStatsNorm ) ):
+    for rowNum, r in enumerate( zip( cmsStatsRaw, cmsStatsNorm ) ):
         if replicasToUse is not None and not replicasToUse[ int( r[0].Chrom ) ]: continue
 
         for stat, start, bin_size, nbin, normedOrNot, hmColsHit, hmColsMiss \
@@ -3043,7 +3044,7 @@
     with open(summaryFN, 'w') as  outfile:
         outfile.write('N causal: 0 N missed: 0\n')  # dummy line
         tabwrite( outfile, 'Stat', 'Start', 'End', 'N_bin')
-        for stat in stat_nbin.keys():
+        for stat in list(stat_nbin.keys()):
             tabwrite( outfile, stat, stat_start[stat], stat_end[stat], stat_nbin[stat] )
 
 def likes( Ddata = '../Data/Shari_Data/sim/', mutAges = AllAges, mutPops = AllPops,
@@ -3072,7 +3073,7 @@
                                                                    putativeMutPop, statsSfx, condName ) ) )
                        for likes in ( 'Likes', '' ) for which in ( 'hits', 'miss', 'region' ) ])
 
-    if getio: return dict( depends_on = scen2ranksFile.values(), creates = likesFiles.values(),
+    if getio: return dict( depends_on = list(scen2ranksFile.values()), creates = list(likesFiles.values()),
                            attrs = dict( piperun_short = True ),
                            mediumRuleNameSfx = ( likesSfx, putativeMutPop, condName ) )
 
@@ -3085,7 +3086,7 @@
     misses = {}
     region = {}
 
-    print 'stats are ', stats
+    print('stats are ', stats)
     for stat in stats:
         nbin_here = CMSBins.stat_nbin[stat] + CMSBinsBase.maxSpecialBins
         hits[stat] = zeros( nbin_here, dtype = int )
@@ -3204,7 +3205,7 @@
                                                                    putativeMutPop, statsSfx, condName ) ) )
                        for likes in ( 'Likes', '' ) for which in ( 'hits', 'miss', 'region' ) ])
 
-    if getio: return dict( depends_on = scen2ranksFile.values(), creates = likesFiles.values() + [ likesRatiosFile ],
+    if getio: return dict( depends_on = list(scen2ranksFile.values()), creates = list(likesFiles.values()) + [ likesRatiosFile ],
                            attrs = dict( piperun_short = True ),
                            mediumRuleNameSfx = ( likesSfx, putativeMutPop, condName ) )
 
@@ -3217,7 +3218,7 @@
     misses = {}
     region = {}
 
-    print 'stats are ', stats
+    print('stats are ', stats)
     for stat in stats:
         nbin_here = CMSBins.stat_nbin[stat] + CMSBinsBase.maxSpecialBins
         hits[stat] = zeros( nbin_here, dtype = int )
@@ -3383,7 +3384,7 @@
     """Visually plot a likes table.
     """
 
-    if getio: return dict( depends_on = filter( None, ( hitsLikesFile, missLikesFile, regionLikesFile, likesTable, likesBinsFile ) ),
+    if getio: return dict( depends_on = [_f for _f in ( hitsLikesFile, missLikesFile, regionLikesFile, likesTable, likesBinsFile ) if _f],
                            creates = plotFile,
                            mediumRuleNameSfx = ( likesSfx, putativeMutPop, condName ),
                            attrs = Dict( 'includeSpecialBins', piperun_short = True ) )
@@ -3435,7 +3436,7 @@
         missLine, = pp.plot( binStarts , missLikes[ stat ][:len( binStarts )], 'g-' )
         if regionLikesFile: regionLine, = pp.plot( binStarts, regionLikes[ stat ][:len(binStarts)], 'b-' )
 
-    pp.figlegend( filter( None, ( hitsLine, missLine, regionLine) ),
+    pp.figlegend( [_f for _f in ( hitsLine, missLine, regionLine) if _f],
                   ( 'selected SNPs', 'neutral SNPs in neutral regions' ) +
                   ( ( 'neutral SNPs in selected regions', ) if regionLikesFile else () ),
                   'upper center' )
@@ -3454,7 +3455,7 @@
                            creates = plotFile,
                            attrs = dict( piperun_short = True ) )
 
-    likesTable = map( LoadLikesTable, likesTableFNs )
+    likesTable = list(map( LoadLikesTable, likesTableFNs ))
 
     hitsLikes = [ IDotData( likesTable[ i ].hitsLikes ) for i in range( 2 ) ]
     missLikes = [ IDotData( likesTable[ i ].missLikes ) for i in range( 2 ) ]
@@ -3509,8 +3510,8 @@
             missLine[i], = pp.plot( binStarts , missLikes[i][ stat ][:len( binStarts )], 'g' + style )
             regionLine[i], = pp.plot( binStarts, regionLikes[i][ stat ][:len(binStarts)], 'b' + style )
 
-    pp.figlegend( filter( None, ( hitsLine[0], missLine[0], regionLine[0],
-                                  hitsLine[1], missLine[1], regionLine[1] ) ),
+    pp.figlegend( [_f for _f in ( hitsLine[0], missLine[0], regionLine[0],
+                                  hitsLine[1], missLine[1], regionLine[1] ) if _f],
                   ( 'selected SNPs 1', 'neutral SNPs in neutral regions 1', 'region snps 1',
                     'selected SNPs 2', 'neutral SNPs in neutral regions 2', 'region snps 2',
                 ),
@@ -3594,7 +3595,7 @@
                                                                       putativeMutPop, condName ) ) )
                           for likesKind in ( 'Likes', '' ) for which in ( 'hits', 'miss', 'region' ) )
 
-    likesFiles = dict( [ ( k, IDotData( FN ) ) for k, FN in likesFilesFNs.items() ] )
+    likesFiles = dict( [ ( k, IDotData( FN ) ) for k, FN in list(likesFilesFNs.items()) ] )
     
     if normalizeWithinReplicas:
         stat_start, stat_end, stat_nbins = CMSBinsLocal.stat_start, CMSBinsLocal.stat_end, CMSBinsLocal.stat_nbin
@@ -3662,11 +3663,11 @@
                      for likesTableDef in likesTableDefs ]
 
     outFN = os.path.join( likesTableDefs[0]['Ddata'], 'graphs',
-                          AddFileSfx( 'likesCmp.svg', putativeMutPop, cmpOp, *map( operator.itemgetter( 'complikeSfx' ), likesTableDefs ) ) )
+                          AddFileSfx( 'likesCmp.svg', putativeMutPop, cmpOp, *list(map( operator.itemgetter( 'complikeSfx' ), likesTableDefs )) ) )
 
     if getio: return dict( depends_on = likesDataFNs, creates = outFN, attrs = Dict( 'putativeMutPop cmpOp', piperun_short = True ) )
 
-    likesDataFiles = map( IDotData, likesDataFNs )
+    likesDataFiles = list(map( IDotData, likesDataFNs ))
 
     pp.figure( figsize = ( 16, 18 ) )
 
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/PopConsts.py
--- ./old/Operations/Shari_Operations/localize/PopConsts.py	(original)
+++ ./old/Operations/Shari_Operations/localize/PopConsts.py	(refactored)
@@ -7,7 +7,7 @@
 pn_European, pn_EastAsian, pn_WestAfrican, pn_LWK, pn_MKK = 1, 4, 5, 6, 7
 popName = { pn_European:'European',pn_EastAsian:'EastAsian',pn_WestAfrican:'WestAfrican' }
 pop2name = copy.deepcopy( popName )
-popName.update( ( str(k), v ) for k, v in popName.items() )
+popName.update( ( str(k), v ) for k, v in list(popName.items()) )
 
 pop2hm2name = { pn_European: 'CEU', pn_EastAsian: 'JPT+CHB', pn_WestAfrican: 'YRI' }
 
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/RunAndAnalyzeSims.py
--- ./old/Operations/Shari_Operations/localize/RunAndAnalyzeSims.py	(original)
+++ ./old/Operations/Shari_Operations/localize/RunAndAnalyzeSims.py	(refactored)
@@ -8,6 +8,7 @@
 from Operations.Shari_Operations.localize.Scenario import GetScenarios
 from Operations.Shari_Operations.localize.PopConsts import AllAges, AllPops, AllFreqs, pop2name
 import os, operator
+from functools import reduce
 
 
 def DefineRulesTo_RunSimsOnly( pr, mutAges = AllAges, mutPops = AllPops, mutFreqs = AllFreqs, nreplicas = 100,
RefactoringTool: No changes to ./old/Operations/Shari_Operations/localize/RunningStat.py
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/Scenario.py
--- ./old/Operations/Shari_Operations/localize/Scenario.py	(original)
+++ ./old/Operations/Shari_Operations/localize/Scenario.py	(refactored)
@@ -33,7 +33,7 @@
         """Return a string representing a subdir in which data for this scenario is stored."""
         return 'neutral'
 
-    def __nonzero__(self): return True
+    def __bool__(self): return True
 
     def __str__( self ):
         """Return an informal string representation"""
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/clean_hapmap2_region.py
--- ./old/Operations/Shari_Operations/localize/clean_hapmap2_region.py	(original)
+++ ./old/Operations/Shari_Operations/localize/clean_hapmap2_region.py	(refactored)
@@ -9,29 +9,29 @@
     if f: f.write(str(len(Z)) + '\t')
     if cleanIHS:
         ind1 =  invert(isnan(Z.StdDiff))
-        print 'SNPs with iHS scores: ',sum(ind1)
+        print('SNPs with iHS scores: ',sum(ind1))
     else:
         ind1 = ones(len(Z))
-    print 'ind1: ', sum(ind1)
+    print('ind1: ', sum(ind1))
     if f: f.write(str(sum(ind1)) + '\t')
     if cleanDer:
         ind2 = Z.derFreq > .2
     else:
         ind2 = ones(len(Z))
-    print 'Derived > .2: ', sum(ind2)
+    print('Derived > .2: ', sum(ind2))
     if f: f.write(str(sum(ind2)) + '\t')
     if cleanAnc:
         ind3 = Z.meanAnc > .4
     else:
         ind3 = ones(len(Z))
-    print 'Ancestral > .4: ', sum(ind3)
+    print('Ancestral > .4: ', sum(ind3))
     if f: f.write(str(sum(ind3)) + '\t')
     ind4 = zeros(len(Z))
     if keepCausal:
         ind4 = Z.Pos == 500000
     ind = all([ind1,ind2,ind3],axis=0)
     ind = any([ind,ind4],axis=0)
-    print 'SNPs to keep: ', sum(ind)
+    print('SNPs to keep: ', sum(ind))
     if f: f.write(str(sum(ind)) + '\t')
 
     if returnInd: return ind
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/fstBySNP_Npops.py
--- ./old/Operations/Shari_Operations/localize/fstBySNP_Npops.py	(original)
+++ ./old/Operations/Shari_Operations/localize/fstBySNP_Npops.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import division
+
 from Operations.MiscUtil import dbg
 import numpy as np
 from numpy import *
@@ -20,7 +20,7 @@
 
        	n = {}
 	Fst = {}
-	f1 = dict( ( pop2name[ popNum ], ancFreqs ) for popNum, ancFreqs in pop2ancFreqs.items() )
+	f1 = dict( ( pop2name[ popNum ], ancFreqs ) for popNum, ancFreqs in list(pop2ancFreqs.items()) )
 	f2 = {}
 	
 	nanc = {}
@@ -37,7 +37,7 @@
 		
 	for ipop in pops:
 		for jpop in pops:
-			if jpop + '_' + ipop in Fst.keys() or ipop == jpop:
+			if jpop + '_' + ipop in list(Fst.keys()) or ipop == jpop:
 				continue
 			Fst[ipop + '_' + jpop] = zeros(len(f1[ipop]))
 			for i in range(len(f1[ipop])):			
@@ -138,7 +138,7 @@
 
        	n = {}
 	Fst = {}
-	f1 = dict( ( pop2name[ popNum ], ancFreq ) for popNum, ancFreq in pop2ancFreq.items() )
+	f1 = dict( ( pop2name[ popNum ], ancFreq ) for popNum, ancFreq in list(pop2ancFreq.items()) )
 	f2 = {}
 	
 	nanc = {}
@@ -153,7 +153,7 @@
 		
 	for ipop in pops:
 		for jpop in pops:
-			if jpop + '_' + ipop in Fst.keys() or ipop == jpop:
+			if jpop + '_' + ipop in list(Fst.keys()) or ipop == jpop:
 				continue
 			Fst[ipop + '_' + jpop] = 0
 			pmean = (nanc[ipop] + nanc[jpop]) / (n[ipop] + n[jpop])
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/localizeSpatially.py
--- ./old/Operations/Shari_Operations/localize/localizeSpatially.py	(original)
+++ ./old/Operations/Shari_Operations/localize/localizeSpatially.py	(refactored)
@@ -1,7 +1,7 @@
 """Code for spatially localizing the selected SNP: identifying sub-intervals of the selected region that likely contain
 the selected SNP."""
 
-from __future__ import division
+
 from Classes.DotData import DotData
 from Operations.IDotData import IDotData
 from Operations.MiscUtil import AddFileSfx, dbg, Dict
@@ -87,9 +87,9 @@
 
             for bin, valsInBin in \
                     complikeForReplica.addCol( 'bin',
-                                               map( functools.partial( min, nbins-1 ),
-                                                    map( int,
-                                                         ( complikeForReplica.gdPos - gdMin ) / binSize ))).groupby('bin'):
+                                               list(map( functools.partial( min, nbins-1 ),
+                                                    list(map( int,
+                                                         ( complikeForReplica.gdPos - gdMin ) / binSize ))))).groupby('bin'):
 
                 binNums[ bin ] = bin
                 if valsInBin:
@@ -139,7 +139,7 @@
 
             binInfo = IDotData( names = binInfoHeadings, 
                                 Columns = ( itertools.repeat( replicaNum, nbins ),
-                                            range( nbins ),
+                                            list(range( nbins)),
                                             binLefts, binRights, binsToUse,
                                             binCenters, binAvgCMS, binMaxCMS, binIntegral, binIntegralNormed, binRank
                                             ) )
@@ -231,12 +231,12 @@
                                 'distanceToIntervalBoundaryBp distanceToIntervalBoundaryGd' ) as spatialLocEvalFile:
 
         for ( replicaNum2, replicaIntervals ), ( replicaNum3, causalGdPos, replicaNum4 ) in \
-                itertools.izip( IDotData( intervalsListFN ).groupby( 'replicaNum' ),
+                zip( IDotData( intervalsListFN ).groupby( 'replicaNum' ),
                                 IDotData.merge( iDotDatas = ( IDotData( causalGdPosFN ),
                                                               IDotData( intervalsListFN ).replicaNum.removeDups() ),
                                                 cols = ( 'replicaNum', 'replicaNum' ) ) ):
 
-            replicaNum2, replicaNum3, replicaNum4  = map( int, ( replicaNum2, replicaNum3, replicaNum4 ) )
+            replicaNum2, replicaNum3, replicaNum4  = list(map( int, ( replicaNum2, replicaNum3, replicaNum4 ) ))
             if not replicaNum2 == replicaNum3 == replicaNum4:
                 dbg( 'replicaNum2 replicaNum3 replicaNum4 intervalsListFN complikeFN causalGdPosFN spatialLocEvalFN' )
             assert replicaNum2 == replicaNum3 == replicaNum4
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/mergeSims.py
--- ./old/Operations/Shari_Operations/localize/mergeSims.py	(original)
+++ ./old/Operations/Shari_Operations/localize/mergeSims.py	(refactored)
@@ -4,6 +4,7 @@
 from Operations.Shari_Operations.localize.xpopMerge import xpopMerge
 from Operations.Shari_Operations.localize.Scenario import GetSelectionScenarios, GetScenarios
 from Operations.MiscUtil import MakeAlphaNum, Dict, Sfx, progress, AddFileSfx
+from functools import reduce
 
 def mergeSims( scenario, Ddata = '../Data/Shari_Data/sim/', simsOut = 'simsOut3', nreplicas = 5,
 	       thinExt = '.thin', thinSfx = '',
@@ -177,16 +178,16 @@
 
 		logging.info( 'Done merging.' )
 		logging.info( 'type(posAll) is ' + str( type( posAll ) ) )
-		print len(posAll)
+		print(len(posAll))
 		chrom = numpy.ones(len(posAll))*ichrom
 		newChrom = DotData(Columns = [chrom,],names=['newChrom',])
-		print newChrom
+		print(newChrom)
 		posAll = posAll[['CHROM_POS 1','FREQ1 1','FREQ1 4','FREQ1 5']]
 		posAll.hstack(newChrom)
 
 		logging.info( 'added replica number column' )
 		
-		print posAll
+		print(posAll)
 		posAllBlank = (numpy.nan,)*posAll.numCols()
 		
 		# 10-16-08 ADDED CHROM TO MERGED OUTPT  ( not now used -- can be removed? )
@@ -244,7 +245,7 @@
     """
 
     for scenario in ( GetSelectionScenarios if noNeutral else GetScenarios)( mutAges, mutPops, mutFreqs ):
-        print 'generating rule for scenario ', scenario
+        print('generating rule for scenario ', scenario)
         pr.addInvokeRule( invokeFn = mergeSims,
                           invokeArgs = Dict( 'scenario nreplicas Ddata simsOut thinExt thinSfx' ) )
 
RefactoringTool: No changes to ./old/Operations/Shari_Operations/localize/mergeSimsP3.py
RefactoringTool: Refactored ./old/Operations/Shari_Operations/localize/subs.py
--- ./old/Operations/Shari_Operations/localize/subs.py	(original)
+++ ./old/Operations/Shari_Operations/localize/subs.py	(refactored)
@@ -1,4 +1,4 @@
-from __future__ import with_statement, division
+
 from Classes.DotData import DotData
 from Operations.MiscUtil import dbg, clamp
 from Operations.Shari_Operations.localize.RunningStat import RunningStat 
@@ -9,7 +9,7 @@
 ## Normalizes an array by mean and std dev
 def normalize(rawVals, ind = itertools.repeat( True ) ):
     notNan = []
-    for val, useVal in itertools.izip( rawVals, ind ):
+    for val, useVal in zip( rawVals, ind ):
         if useVal and not isnan(val) and not isneginf(val):
             notNan.append(val)
 #    print len(notNan)
@@ -18,7 +18,7 @@
     
     normVals = []
 
-    for val, useVal in itertools.izip( rawVals, ind ):
+    for val, useVal in zip( rawVals, ind ):
         normVals.append( (val - theMean)/theStd if useVal else nan )
 
     return normVals, theMean, theStd
@@ -302,7 +302,7 @@
         bin = int(float(i)/len(stat)/bin_scale)
         if bin > ntopbins:
             highScorers.append(pos[sortInd[i]])
-            print str(pos[sortInd[i]]) + '\t' + str( stat[sortInd[i]] )
+            print(str(pos[sortInd[i]]) + '\t' + str( stat[sortInd[i]] ))
             CL.append(stat[i])
     
     output = array([highScorers,CL])
@@ -311,9 +311,9 @@
 def calcCDF(ranksDotData, hitRanksName = 'hitsRanks'):
     hitRanks = array(ranksDotData[hitRanksName])
     totalHits = sum(hitRanks)
-    print totalHits
+    print(totalHits)
     fractions = hitRanks / float(totalHits)
-    print fractions
+    print(fractions)
 
     return cumsum(fractions)
 
@@ -322,9 +322,9 @@
     for freq in ['high','low']:
         hitRanks = array(ranksDotData['hitsRanks_' + freq])
         totalHits = sum(hitRanks)
-        print totalHits
+        print(totalHits)
         fractions = hitRanks / float(totalHits)
-        print fractions
+        print(fractions)
 
         cdfs[freq] =  cumsum(fractions)
     
@@ -378,7 +378,7 @@
     with open( tsvFileName ) as f:
         header = f.readline().strip().split()
         colNum = header.index( columnName )
-        print 'colnum=', colNum
+        print('colnum=', colNum)
 
         lineNum = 0
         for line in f:
@@ -429,7 +429,7 @@
     
     indNaN = hitsLikes != 1e-10
     missingVal = log(min(hitsLikes[indNaN]/missLikes[indNaN]))
-    print missingVal
+    print(missingVal)
 
     for i in range(len(stat)):
 
@@ -508,7 +508,7 @@
     
     indNaN = hitsLikes != 1e-10
     missingVal = log(min(hitsLikes[indNaN]/missLikes[indNaN]))
-    print missingVal
+    print(missingVal)
 
     for i in range(len(stat)):
 
RefactoringTool: No changes to ./old/Operations/Shari_Operations/localize/xpopMerge.py
RefactoringTool: No changes to ./old/System/Colors.py
RefactoringTool: Refactored ./old/System/Utils.py
--- ./old/System/Utils.py	(original)
+++ ./old/System/Utils.py	(refactored)
@@ -30,8 +30,8 @@
 	'''
 		Fast invert a numpy permutation.
 	'''
-	X = numpy.array(range(len(s)))
-	X[s] = range(len(s))
+	X = numpy.array(list(range(len(s))))
+	X[s] = list(range(len(s)))
 	return X
 
 
@@ -40,7 +40,7 @@
 		Error class used for raising I/O errors (maybe should be moved to system_io_override?)
 	'''
 	def __init__(self,iofunc,readfiles,writefiles, Dependencies,Creates):
-		print "\nCHECK_ERROR: An I/O exception occured in function", iofunc, ": either the files" , readfiles , "aren't in" , Dependencies, "or the files", writefiles, "aren't in ", Creates, '. \n'
+		print("\nCHECK_ERROR: An I/O exception occured in function", iofunc, ": either the files" , readfiles , "aren't in" , Dependencies, "or the files", writefiles, "aren't in ", Creates, '. \n')
 		
 
 def RecursiveFileList(ToList,Avoid=None):
@@ -250,10 +250,10 @@
 
 def GetDataEnvironmentDirectory():
 	x = os.environ
-	if 'DataEnvironmentDirectory' in x.keys():
+	if 'DataEnvironmentDirectory' in list(x.keys()):
 		return x['DataEnvironmentDirectory']
 	else:
-		print 'DataEnvironmentDirectory not an environment variable, assuming it is ' , os.getcwd()[:os.getcwd().find('/')] + '/'
+		print('DataEnvironmentDirectory not an environment variable, assuming it is ' , os.getcwd()[:os.getcwd().find('/')] + '/')
 		return os.getcwd()[:os.getcwd().find('/')] + '/' 
 	
 def PathAlong(a,b):
@@ -541,7 +541,7 @@
 		T = numpy.array(['/'.join(z.split('/')[:i]) + ('/' if len(z.split('/')) > i else '')  for z in Z ])    #get i-reduced slash list from Z, call it T
 		R = (T[1:] != T[:-1]).nonzero()[0] 
 		R = numpy.append(R,numpy.array([len(T)-1]))
-		M = R[R.searchsorted(range(len(T)))]
+		M = R[R.searchsorted(list(range(len(T))))]
 		#get set of guys in Y with i slashes, call it L
 		L = (SlashList == i)
 		H = Y[L]
@@ -563,7 +563,7 @@
 	s = Z.argsort()
 	Z = Z[s]
 	[A,B] = getpathalong(Y,Z)
-	L = ListUnion([range(A[i],B[i]) for i in range(len(A)) if A[i] < B[i]])
+	L = ListUnion([list(range(A[i],B[i])) for i in range(len(A)) if A[i] < B[i]])
 	return s[L]
 
 
@@ -597,7 +597,7 @@
 	T = Z.copy()
 	R = (T[1:] != T[:-1]).nonzero()[0] 
 	R = numpy.append(R,numpy.array([len(T)-1]))
-	M = R[R.searchsorted(range(len(T)))]
+	M = R[R.searchsorted(list(range(len(T))))]
 	D = T.searchsorted(Y)
 	T = numpy.append(T,numpy.array([0]))
 	M = numpy.append(M,numpy.array([0]))
@@ -872,7 +872,7 @@
 
 	if Dir[-1] != '/':
 		if Verbose:
-			print "Warning: the directory name, ", Dir, ", was provided. The character '/' was appended to the end of the name."
+			print("Warning: the directory name, ", Dir, ", was provided. The character '/' was appended to the end of the name.")
 		return Dir + '/'
 	else:
 		return Dir
@@ -917,13 +917,13 @@
 	ArchivedName = GetTimeStampedArchiveName(toarchive)
 
 	if not PathExists('../Archive/'):
-		print 'Creating Archive ....'
+		print('Creating Archive ....')
 		MakeDir('../Archive/')
 		
 	if PathExists(toarchive):
 		strongcopy(toarchive,'../Archive/' + ArchivedName)
 	else:
-		print 'ERROR: The path', toarchive, 'does not exist; nothing archived.'
+		print('ERROR: The path', toarchive, 'does not exist; nothing archived.')
 
 
 def move_to_archive(toarchive,depends_on=('../',),creates=('../Archive/',)):
@@ -935,13 +935,13 @@
 	ArchivedName = GetTimeStampedArchiveName(toarchive)
 	
 	if not PathExists('../Archive/'):
-		print 'Creating Archive ....'
+		print('Creating Archive ....')
 		MakeDir('../Archive/')
 		
 	if PathExists(toarchive):
 		Rename(toarchive,'../Archive/' + ArchivedName)
 	else:
-		print 'ERROR: The path', toarchive, 'does not exist; nothing archived.'
+		print('ERROR: The path', toarchive, 'does not exist; nothing archived.')
 		
 
 def CompilerChecked(ToCheck):
@@ -955,7 +955,7 @@
 		try: 
 			re.compile(LL)
 		except:
-			print "Error: the string, ", LL, " was found calling", funcname(), ". This string could not be compiled as a regular expression and will not be loaded."
+			print("Error: the string, ", LL, " was found calling", funcname(), ". This string could not be compiled as a regular expression and will not be loaded.")
 		else:
 			X += [L if L != '' else '^$']
 	return X		
@@ -1058,7 +1058,7 @@
 		--dictionary whose keys are unique elements of values of D, and 
 		whose values on key 'K' are lists of keys 'k' in D such that D[k] = K
 	'''
-	return dict([(v,set([j for j in D.keys() if D[j] == v])) for v in set(D.values())])
+	return dict([(v,set([j for j in list(D.keys()) if D[j] == v])) for v in set(D.values())])
 
 
 def PathExists(ToCheck):
RefactoringTool: Refactored ./sim_generation/dispatch_collate_remodel.py
--- ./sim_generation/dispatch_collate_remodel.py	(original)
+++ ./sim_generation/dispatch_collate_remodel.py	(refactored)
@@ -90,7 +90,7 @@
 	for subdir in subdirs:
 		if True:
 			for pop in [1]:
-				reps = range(1,1001)
+				reps = list(range(1,1001))
 				this_rundir = this_basedir + subdir
 				for irep in reps:
 					tpedfilename = this_rundir + "tpeds/rep" + str(irep) + "_0_" + str(pop) + ".tped"
@@ -129,7 +129,7 @@
 								#subprocess.check_output(fullcmd.split())
 
 
-	print('loaded ' + str(len(allargs)) + " cmds")
+	print(('loaded ' + str(len(allargs)) + " cmds"))
 	####################
 	## DISPATCH JOBS ###
 	####################
RefactoringTool: Refactored ./sim_generation/dispatch_remodel_components.py
--- ./sim_generation/dispatch_remodel_components.py	(original)
+++ ./sim_generation/dispatch_remodel_components.py	(refactored)
@@ -15,7 +15,7 @@
 regimes = ['lct', 'slc24a5']
 #'edar']#'lct', 'edar', 'slc24a5']#['neut']
 pops = [1,2,3,4]
-reps = range(0,1001)
+reps = list(range(0,1001))
 #models, regimes, pops, reps = ['def15', 'defdef15', 'gradient15', 'nd', 'ndcs', 'gravel'], ['neut'], [1,2,3, 4], range(1,1001)
 #models, regimes, pops, reps = ['gradient15',], ['neut'], [1,2,3, 4], range(0,1001)
 basewritedir = "/idi/sabeti-scratch/jvitti/remodel/run4/" #'nd', 'ndcs', 
@@ -128,7 +128,7 @@
 	## DISPATCH ##
 	##############
 	shuffle(arguments)
-	print('loaded a total of ' + str(len(arguments)) + " commands.")
+	print(('loaded a total of ' + str(len(arguments)) + " commands."))
 	iscript = 0
 	for argchunk in chunks(arguments,ncmds_script):
 		iscript +=1
RefactoringTool: Refactored ./sim_generation/dispatch_remodel_components_neut.py
--- ./sim_generation/dispatch_remodel_components_neut.py	(original)
+++ ./sim_generation/dispatch_remodel_components_neut.py	(refactored)
@@ -12,7 +12,7 @@
 import subprocess
 
 #'defdef15', 'gradient15'
-models, regimes, pops, reps = ['nd'], ['neut'], [1,2,3,4], range(1,1000)#range(1001, 5000)#
+models, regimes, pops, reps = ['nd'], ['neut'], [1,2,3,4], list(range(1,1000))#range(1001, 5000)#
 #models, regimes, pops, reps = ['nd', 'ndcs', 'def15', 'defdef15', 'gradient15', 'gravel' ], ['neut'], [1,2,3,4], range(0,1001)#range(1001, 5000)#
 #models, regimes, pops, reps = [ 'gradient15'], ['neut'], [1,2,3,4], range(0,1001)#range(1001, 5000)#
 
@@ -127,7 +127,7 @@
 	## DISPATCH ##
 	##############
 	shuffle(arguments)
-	print('loaded a total of ' + str(len(arguments)) + " commands.")
+	print(('loaded a total of ' + str(len(arguments)) + " commands."))
 	iscript = 0
 	for argchunk in chunks(arguments,ncmds_script):
 		iscript +=1
RefactoringTool: Refactored ./sim_generation/dispatch_remodel_components_normalize.py
--- ./sim_generation/dispatch_remodel_components_normalize.py	(original)
+++ ./sim_generation/dispatch_remodel_components_normalize.py	(refactored)
@@ -12,7 +12,7 @@
 
 models = [ 'gradient15', 'defdef15', 'def15']
 regimes = ['lct', 'slc24a5', 'edar']
-reps = range(0,1001)
+reps = list(range(0,1001))
 basewritedir = "/idi/sabeti-scratch/jvitti/remodel/run4/"  
 #models, regimes, pops, reps = ['nd'], ['neut'], [1,2,3, 4], range(0,1001) #'gravel', 
 #models, regimes, pops, reps = ['gradient15', 'defdef15', 'def15', 'gravel'], ['neut'], [1,2,3, 4], range(1,1001) #'gravel', 
@@ -143,7 +143,7 @@
 	## DISPATCH ##
 	##############
 	#shuffle(arguments)
-	print('loaded a total of ' + str(len(arguments)) + " commands.")
+	print(('loaded a total of ' + str(len(arguments)) + " commands."))
 	iscript = 0
 	for argchunk in chunks(arguments,ncmds_script):
 		iscript +=1
RefactoringTool: Refactored ./sim_generation/dispatch_remodel_freqs.py
--- ./sim_generation/dispatch_remodel_freqs.py	(original)
+++ ./sim_generation/dispatch_remodel_freqs.py	(refactored)
@@ -12,7 +12,7 @@
 models = [ 'gradient15', 'defdef15', 'def15']#'nd', 'ndcs', 'gravel']#'gravel'] #--drop-singletons " + str(singrate) <--- absent from gravel as run! presnt in gradient15!
 regimes = ['lct', 'edar', 'slc24a5']#['neut']
 pops = [1,2,3,4]
-reps = range(0,1001)
+reps = list(range(0,1001))
 #models, regimes, pops, reps = ['gravel'], ['neut'], [1,2,3], range(0,1001)
 #models, regimes, pops, reps = ['defdef15', 'def15', 'nd', 'ndcs', 'gradient15'], ['neut'], [1,2,3, 4], range(0,1001)
 basewritedir = "/idi/sabeti-scratch/jvitti/remodel/run4/"
@@ -124,7 +124,7 @@
 	## DISPATCH ##
 	##############
 	shuffle(arguments)
-	print('loaded a total of ' + str(len(arguments)) + " commands.")
+	print(('loaded a total of ' + str(len(arguments)) + " commands."))
 	iscript = 0
 	for argchunk in chunks(arguments,ncmds_script):
 		iscript +=1
RefactoringTool: Refactored ./sim_generation/dispatch_remodel_isafe.py
--- ./sim_generation/dispatch_remodel_isafe.py	(original)
+++ ./sim_generation/dispatch_remodel_isafe.py	(refactored)
@@ -13,7 +13,7 @@
 models = [ 'gradient15', 'defdef15', 'def15']#'nd', 'ndcs', 'gravel']#'gravel'] #--drop-singletons " + str(singrate) <--- absent from gravel as run! presnt in gradient15!
 regimes = ['lct', 'edar', 'slc24a5']#['neut']
 pops = [1,2,3,4]
-reps = range(0,1001)
+reps = list(range(0,1001))
 #models, regimes, pops, reps = ['def15',], ['neut'], [1,2,3,4], range(0,1001)#range(1001, 5000)#
 #models, regimes, pops, reps = ['defdef15', 'gradient15', 'def15', 'gravel', 'nd', 'ndcs'], ['neut'], [1,2,3,4], range(0,1001)#range(1001, 5000)#
 #models, regimes, pops, reps = ['gravel'], ['neut'], [1,2,3,], range(0,1001)#range(1001, 5000)#
@@ -71,7 +71,7 @@
 								arguments.append(argstring)
 					
 
-	print('loaded a total of ' + str(len(arguments)) + " commands.")
+	print(('loaded a total of ' + str(len(arguments)) + " commands."))
 	iscript = 0
 	shuffle(arguments)
 	for argchunk in chunks(arguments,ncmds_script):
RefactoringTool: Refactored ./sim_generation/dispatch_remodel_negative_sims.py
--- ./sim_generation/dispatch_remodel_negative_sims.py	(original)
+++ ./sim_generation/dispatch_remodel_negative_sims.py	(refactored)
@@ -11,7 +11,7 @@
 import time
 
 basedir = "/n/scratchlfs/sabeti_lab/vitti/remodel_negsel/"
-reps = range(1, 1001)
+reps = list(range(1, 1001))
 basecmd = "python /n/home08/jvitti/remodel_rc/rerun_cosi.py"
 
 def chunks(l, n):
@@ -39,7 +39,7 @@
 	this_rundir = basedir
 	paramfilename = "/n/home08/jvitti/remodel_samplesize/params/test_neg.par"
 	if not os.path.isfile(paramfilename) or os.path.getsize(paramfilename) == 0:
-		print('missing ' + paramfilename)
+		print(('missing ' + paramfilename))
 	for irep in reps:
 		trajfilename = this_rundir + "sampled_vars/rep" + str(irep) + ".traj"
 		save_sample_filename = this_rundir + "sampled_vars/rep" + str(irep) + ".sampled"
@@ -51,7 +51,7 @@
 			commands.append(cmdstring)
 
 
-	print('loaded a total of ' + str(len(commands)) + " commands.")
+	print(('loaded a total of ' + str(len(commands)) + " commands."))
 	iscript = 0
 	shuffle(commands)
 	for commandchunk in chunks(commands,ncmds_script):
RefactoringTool: Refactored ./sim_generation/dispatch_remodel_samplesize_sims.py
--- ./sim_generation/dispatch_remodel_samplesize_sims.py	(original)
+++ ./sim_generation/dispatch_remodel_samplesize_sims.py	(refactored)
@@ -11,7 +11,7 @@
 import time
 
 basedir = "/n/scratchlfs/sabeti_lab/vitti/remodel_samplesize/"
-reps, ns = range(1, 1001), [25, 50, 100, 150, 200, 250, 500]#25, 50, 100, 150] #ADD 200, 250, 500 #,,,____ SOFT AND HARD
+reps, ns = list(range(1, 1001)), [25, 50, 100, 150, 200, 250, 500]#25, 50, 100, 150] #ADD 200, 250, 500 #,,,____ SOFT AND HARD
 basecmd = "python /n/home08/jvitti/remodel_rc/rerun_cosi.py"
 
 def chunks(l, n):
@@ -41,7 +41,7 @@
 			this_rundir = "/n/scratchlfs/sabeti_lab/vitti/remodel_samplesize" + batch + "/n" + str(n) + "/"#basedir + model +"_" + regime + "_sel" + str(pop) + "/"
 			paramfilename = "/n/home08/jvitti/remodel_samplesize/params/default_n" + str(n) + batch + ".par"
 			if not os.path.isfile(paramfilename) or os.path.getsize(paramfilename) == 0:
-				print('missing ' + paramfilename)
+				print(('missing ' + paramfilename))
 			for irep in reps:
 				trajfilename = this_rundir + "sampled_vars/rep" + str(irep) + ".traj"
 				save_sample_filename = this_rundir + "sampled_vars/rep" + str(irep) + ".sampled"
@@ -54,7 +54,7 @@
 					print(cmdstring)
 
 
-	print('loaded a total of ' + str(len(commands)) + " commands.")
+	print(('loaded a total of ' + str(len(commands)) + " commands."))
 	iscript = 0
 	shuffle(commands)
 	for commandchunk in chunks(commands,ncmds_script):
RefactoringTool: Refactored ./sim_generation/dispatch_remodel_sims.py
--- ./sim_generation/dispatch_remodel_sims.py	(original)
+++ ./sim_generation/dispatch_remodel_sims.py	(refactored)
@@ -14,7 +14,7 @@
 models = [ 'gradient15', 'defdef15', 'def15']#'nd', 'ndcs', 'gravel']#'gravel'] #--drop-singletons " + str(singrate) <--- absent from gravel as run! presnt in gradient15!
 regimes = ['lct', 'edar', 'slc24a5']#['neut']
 basedir = "/idi/sabeti-scratch/jvitti/remodel/run4/" #NEUT #run/"
-reps = range(0, 1001)
+reps = list(range(0, 1001))
 basecmd = "python /home/unix/vitti/rerun_cosi.py"
 
 def chunks(l, n):
@@ -53,7 +53,7 @@
 					subprocess.check_output( mkdir_cmd.split() )
 				paramfilename = basedir + "params/" + model + "_" + regime + ".par" 
 				if not os.path.isfile(paramfilename) or os.path.getsize(paramfilename) == 0:
-					print('missing ' + paramfilename)
+					print(('missing ' + paramfilename))
 				for irep in reps:
 					trajfilename = this_rundir + "sampled_vars/rep" + str(irep) + ".traj"
 					save_sample_filename = this_rundir + "sampled_vars/rep" + str(irep) + ".sampled"
@@ -64,7 +64,7 @@
 						arguments.append(argstring)
 
 	
-	print('loaded a total of ' + str(len(arguments)) + " commands.")
+	print(('loaded a total of ' + str(len(arguments)) + " commands."))
 	iscript = 0
 	shuffle(arguments)
 	for argchunk in chunks(arguments,ncmds_script):
RefactoringTool: Refactored ./sim_generation/quick_stitch_remodel_xpehh.py
--- ./sim_generation/quick_stitch_remodel_xpehh.py	(original)
+++ ./sim_generation/quick_stitch_remodel_xpehh.py	(refactored)
@@ -3,7 +3,7 @@
 
 sourcedir = "/idi/sabeti-scratch/jvitti/remodel/run/"
 targetdir = "/idi/sabeti-scratch/jvitti/remodel/run/normalize_params/"
-models, regimes, pops, reps = ['defdef15', 'gradient15', 'from_RC/nd', 'from_RC/ndcs'], ['soft', 'hard'], [1,2,3,4], range(1,5001)
+models, regimes, pops, reps = ['defdef15', 'gradient15', 'from_RC/nd', 'from_RC/ndcs'], ['soft', 'hard'], [1,2,3,4], list(range(1,5001))
 
 import os
 import subprocess
@@ -16,7 +16,7 @@
 			for pop in pops:
 				basedir = sourcedir + model + "_" + regime + "_sel" + str(pop) + "/xpehh/"
 				all_sources.extend([basedir + item for item in os.listdir(basedir) if ".out" in item and "OUTGROUP" in item])
-		print(model, str(len(all_sources)))
+		print((model, str(len(all_sources))))
 		
 		for pop in pops:
 			altpops = pops[:]
@@ -33,7 +33,7 @@
 						writefile.write(line)
 					readfile.close()
 				writefile.close()
-				print('wrote to ' + writefilename)
+				print(('wrote to ' + writefilename))
 
 main()
 
RefactoringTool: Refactored ./sim_generation/remodel_components.py
--- ./sim_generation/remodel_components.py	(original)
+++ ./sim_generation/remodel_components.py	(refactored)
@@ -36,7 +36,7 @@
 	if not os.path.isfile(tped_filename):
 		tped_filename += ".gz"					
 	if not os.path.isfile(tped_filename):
-		print('missing: ', tped_filename)	
+		print(('missing: ', tped_filename))	
 		sys.exit(0)				
 
 	ihh12_commandstring = "/idi/sabeti-scratch/jvitti/selscan/bin/linux/selscan"
RefactoringTool: Refactored ./sim_generation/remodel_components_4pop.py
--- ./sim_generation/remodel_components_4pop.py	(original)
+++ ./sim_generation/remodel_components_4pop.py	(refactored)
@@ -45,7 +45,7 @@
 	if not os.path.isfile(tped_filename):
 		tped_filename += ".gz"					
 	if not os.path.isfile(tped_filename):
-		print('missing: ', tped_filename)	
+		print(('missing: ', tped_filename))	
 		sys.exit(0)				
 	
 	ihs_commandstring = "python3 " + cmsdir + "scans.py selscan_ihs"
RefactoringTool: Refactored ./sim_generation/remodel_components_neut.py
--- ./sim_generation/remodel_components_neut.py	(original)
+++ ./sim_generation/remodel_components_neut.py	(refactored)
@@ -34,7 +34,7 @@
 	if not os.path.isfile(tped_filename):
 		tped_filename += ".gz"					
 	if not os.path.isfile(tped_filename):
-		print('missing: ', tped_filename)	
+		print(('missing: ', tped_filename))	
 		sys.exit(0)				
 
 	"""
RefactoringTool: Refactored ./sim_generation/remodel_components_normalize.py
--- ./sim_generation/remodel_components_normalize.py	(original)
+++ ./sim_generation/remodel_components_normalize.py	(refactored)
@@ -51,7 +51,7 @@
 
 def norm_sel_ihs(inputScoreFile, neutNormfilename):
 	''' normalize iHS component score for selection scenarios to neutral parameters ''' 
-	print("normalizing selection simulates to neutral: \n" + inputScoreFile + "\n" + neutNormfilename)
+	print(("normalizing selection simulates to neutral: \n" + inputScoreFile + "\n" + neutNormfilename))
 	bins, nums, means, variances = read_neut_normfile(neutNormfilename, 'ihs')
 	#print(str(means))
 	#print(str(variances))
@@ -77,7 +77,7 @@
 		normfile.write(writeline)		
 	openfile.close()
 	normfile.close()
-	print("wrote to: " + normfilename)
+	print(("wrote to: " + normfilename))
 	return
 def norm_neut_xpehh(inputScoreFile, outfileName, runProgram = "scans.py"):##JV: I found the way to properly write to file; should implement here
 	'''wraps call to scans.py'''
@@ -101,7 +101,7 @@
 		normfile.write(writeline)
 	openfile.close()
 	normfile.close()
-	print("wrote to: " + normfilename)
+	print(("wrote to: " + normfilename))
 	return
 
 
RefactoringTool: Refactored ./sim_generation/remodel_freqs.py
--- ./sim_generation/remodel_freqs.py	(original)
+++ ./sim_generation/remodel_freqs.py	(refactored)
@@ -33,7 +33,7 @@
 	if not os.path.isfile(tped_filename):
 		tped_filename += ".gz"					
 	if not os.path.isfile(tped_filename):
-		print('missing: ', tped_filename)	
+		print(('missing: ', tped_filename))	
 		sys.exit(0)				
 	"""
 	ihs_commandstring = "python " + cmsdir + "scans.py selscan_ihs"
RefactoringTool: Refactored ./sim_generation/remodel_freqs_4pop.py
--- ./sim_generation/remodel_freqs_4pop.py	(original)
+++ ./sim_generation/remodel_freqs_4pop.py	(refactored)
@@ -36,7 +36,7 @@
 	if not os.path.isfile(tped_filename):
 		tped_filename += ".gz"					
 	if not os.path.isfile(tped_filename):
-		print('missing: ', tped_filename)	
+		print(('missing: ', tped_filename))	
 		sys.exit(0)				
 	"""
 	ihs_commandstring = "python3 " + cmsdir + "scans.py selscan_ihs"
RefactoringTool: Refactored ./sim_generation/run_remodel_freqs_4pops.py
--- ./sim_generation/run_remodel_freqs_4pops.py	(original)
+++ ./sim_generation/run_remodel_freqs_4pops.py	(refactored)
@@ -10,7 +10,7 @@
 from random import shuffle
 import subprocess
 
-models, regimes, pops, reps = ['gradient15'], ['hard', 'soft'], [1,2,3,4], range(1,5001)#range(1001, 5000)#
+models, regimes, pops, reps = ['gradient15'], ['hard', 'soft'], [1,2,3,4], list(range(1,5001))#range(1001, 5000)#
 basewritedir = "/idi/sabeti-scratch/jvitti/remodel/run/"
 
 def chunks(l, n):
@@ -94,7 +94,7 @@
 	## DISPATCH ##
 	##############
 	shuffle(arguments)
-	print('loaded a total of ' + str(len(arguments)) + " commands.")
+	print(('loaded a total of ' + str(len(arguments)) + " commands."))
 	iscript = 0
 	for argchunk in chunks(arguments,ncmds_script):
 		iscript +=1
RefactoringTool: Refactored ./sim_generation/run_remodel_h12.py
--- ./sim_generation/run_remodel_h12.py	(original)
+++ ./sim_generation/run_remodel_h12.py	(refactored)
@@ -103,7 +103,7 @@
 	## DISPATCH ##
 	##############
 	shuffle(arguments)
-	print('loaded a total of ' + str(len(arguments)) + " commands.")
+	print(('loaded a total of ' + str(len(arguments)) + " commands."))
 	iscript = 0
 	for argchunk in chunks(arguments,ncmds_script):
 		iscript +=1
RefactoringTool: Refactored ./sim_generation/run_remodel_isafe.py
--- ./sim_generation/run_remodel_isafe.py	(original)
+++ ./sim_generation/run_remodel_isafe.py	(refactored)
@@ -63,7 +63,7 @@
 				physpos = int(entries[3])
 				genotypes = entries[4:]
 				these_reformatted_genotypes, nIndivs = reformat_genotypes_as_diploid_string(genotypes)
-				if physpos in writelines.keys():
+				if physpos in list(writelines.keys()):
 					writelines[physpos] +=  "\t" + these_reformatted_genotypes
 				else:
 					writelines[physpos] = these_reformatted_genotypes
@@ -74,7 +74,7 @@
 			full_writeline = "1\t" + str(item) + "\t" + str(item) + "\t0\t1\t100\tPASS\t.\tGT\t" + str(writelines[item]) + "\n"
 			vcf_file.write(full_writeline)
 		vcf_file.close()
-		print('wrote to ' + vcf_filename)
+		print(('wrote to ' + vcf_filename))
 		bgzip_cmd = "bgzip " + vcf_filename
 		print(bgzip_cmd)
 		subprocess.check_output(bgzip_cmd.split())
RefactoringTool: Files that were modified:
RefactoringTool: ./cms/FileConverter.py
RefactoringTool: ./cms/cms_exceptions.py
RefactoringTool: ./cms/cms_modeller.py
RefactoringTool: ./cms/composite.py
RefactoringTool: ./cms/likes_from_model.py
RefactoringTool: ./cms/main.py
RefactoringTool: ./cms/power.py
RefactoringTool: ./cms/run_traj.py
RefactoringTool: ./cms/scans.py
RefactoringTool: ./cms/selection.py
RefactoringTool: ./cms/combine/input_func.py
RefactoringTool: ./cms/combine/viz_func.py
RefactoringTool: ./cms/dists/freqbins_func.py
RefactoringTool: ./cms/dists/likes_func.py
RefactoringTool: ./cms/dists/scores_func.py
RefactoringTool: ./cms/model/bootstrap_func.py
RefactoringTool: ./cms/model/error_func.py
RefactoringTool: ./cms/model/params_func.py
RefactoringTool: ./cms/model/plot_func.py
RefactoringTool: ./cms/model/search_func.py
RefactoringTool: ./cms/power/parse_func.py
RefactoringTool: ./cms/power/power_func.py
RefactoringTool: ./cms/test/__init__.py
RefactoringTool: ./cms/test/integration/test_main.py
RefactoringTool: ./cms/test/unit/test_scans.py
RefactoringTool: ./cms/test/unit/test_selection.py
RefactoringTool: ./cms/test/unit/test_tools.py
RefactoringTool: ./cms/tools/__init__.py
RefactoringTool: ./cms/tools/selscan.py
RefactoringTool: ./cms/util/__init__.py
RefactoringTool: ./cms/util/call_sample_reader.py
RefactoringTool: ./cms/util/cmd.py
RefactoringTool: ./cms/util/db_helper.py
RefactoringTool: ./cms/util/file.py
RefactoringTool: ./cms/util/json_helpers.py
RefactoringTool: ./cms/util/misc.py
RefactoringTool: ./cms/util/parallel.py
RefactoringTool: ./cms/util/recom_map.py
RefactoringTool: ./cms/util/stats.py
RefactoringTool: ./cms/util/vcf_reader.py
RefactoringTool: ./cms/util/version.py
RefactoringTool: ./cms/util/old/annot.py
RefactoringTool: ./cms/util/old/vcf.py
RefactoringTool: ./docs/conf.py
RefactoringTool: ./old/fastcms.py
RefactoringTool: ./old/Classes/DotData.py
RefactoringTool: ./old/Operations/IDotData.py
RefactoringTool: ./old/Operations/MiscUtil.py
RefactoringTool: ./old/Operations/MiscUtil2.py
RefactoringTool: ./old/Operations/TypeInfer.py
RefactoringTool: ./old/Operations/bioutil.py
RefactoringTool: ./old/Operations/tsvutils.py
RefactoringTool: ./old/Operations/DotDataFunctions/DictionaryOps.py
RefactoringTool: ./old/Operations/DotDataFunctions/DotDataListFromDirectory.py
RefactoringTool: ./old/Operations/DotDataFunctions/DotDataListFromSV.py
RefactoringTool: ./old/Operations/DotDataFunctions/SaveColumnsInDotData.py
RefactoringTool: ./old/Operations/DotDataFunctions/SaveDotData.py
RefactoringTool: ./old/Operations/DotDataFunctions/SaveDotDataAsSV.py
RefactoringTool: ./old/Operations/DotDataFunctions/datahstack.py
RefactoringTool: ./old/Operations/DotDataFunctions/datavstack.py
RefactoringTool: ./old/Operations/DotDataFunctions/ondiskops.py
RefactoringTool: ./old/Operations/Ilya_Operations/RBTree.py
RefactoringTool: ./old/Operations/Ilya_Operations/SnpStats.py
RefactoringTool: ./old/Operations/Ilya_Operations/sweep.py
RefactoringTool: ./old/Operations/Ilya_Operations/CMS/fastcms.py
RefactoringTool: ./old/Operations/Ilya_Operations/PipeRun/python/FunctionChecksums.py
RefactoringTool: ./old/Operations/Ilya_Operations/PipeRun/python/PipeRun.py
RefactoringTool: ./old/Operations/Ilya_Operations/PipeRun/python/prun_par.py
RefactoringTool: ./old/Operations/Ilya_Operations/PipeRun/python/resultPusher.py
RefactoringTool: ./old/Operations/Ilya_Operations/PipeRun/python/runner.py
RefactoringTool: ./old/Operations/Ilya_Operations/localize/gatherCausalRanks.py
RefactoringTool: ./old/Operations/Ilya_Operations/sim/sfs/working/pardis2/RunSimulations.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/CMS.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/PopConsts.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/RunAndAnalyzeSims.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/RunningStat.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/Scenario.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/clean_hapmap2_region.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/fstBySNP_Npops.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/localizeSpatially.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/mergeSims.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/mergeSimsP3.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/subs.py
RefactoringTool: ./old/Operations/Shari_Operations/localize/xpopMerge.py
RefactoringTool: ./old/System/Colors.py
RefactoringTool: ./old/System/Utils.py
RefactoringTool: ./sim_generation/dispatch_collate_remodel.py
RefactoringTool: ./sim_generation/dispatch_remodel_components.py
RefactoringTool: ./sim_generation/dispatch_remodel_components_neut.py
RefactoringTool: ./sim_generation/dispatch_remodel_components_normalize.py
RefactoringTool: ./sim_generation/dispatch_remodel_freqs.py
RefactoringTool: ./sim_generation/dispatch_remodel_isafe.py
RefactoringTool: ./sim_generation/dispatch_remodel_negative_sims.py
RefactoringTool: ./sim_generation/dispatch_remodel_samplesize_sims.py
RefactoringTool: ./sim_generation/dispatch_remodel_sims.py
RefactoringTool: ./sim_generation/quick_stitch_remodel_xpehh.py
RefactoringTool: ./sim_generation/remodel_components.py
RefactoringTool: ./sim_generation/remodel_components_4pop.py
RefactoringTool: ./sim_generation/remodel_components_neut.py
RefactoringTool: ./sim_generation/remodel_components_normalize.py
RefactoringTool: ./sim_generation/remodel_freqs.py
RefactoringTool: ./sim_generation/remodel_freqs_4pop.py
RefactoringTool: ./sim_generation/run_remodel_freqs_4pops.py
RefactoringTool: ./sim_generation/run_remodel_h12.py
RefactoringTool: ./sim_generation/run_remodel_isafe.py
RefactoringTool: Warnings/messages while refactoring:
RefactoringTool: ### In file ./old/Operations/MiscUtil.py ###
RefactoringTool: Line 224: You should use a for loop here
RefactoringTool: Line 237: You should use a for loop here
RefactoringTool: Line 245: You should use a for loop here
RefactoringTool: ### In file ./old/Operations/Shari_Operations/localize/CMS.py ###
RefactoringTool: Line 2910: You should use 'operator.mul( eval( replicaCondExpr, globals(), {} ), nreplicas)' here.
RefactoringTool: Line 3003: You should use 'operator.mul( eval( replicaCondExpr, globals(), {} ), nreplicas)' here.
(master_env_v178_py36) 21:44  [cms] $ echo $?
0
(master_env_v178_py36) 21:44  [cms] $ 